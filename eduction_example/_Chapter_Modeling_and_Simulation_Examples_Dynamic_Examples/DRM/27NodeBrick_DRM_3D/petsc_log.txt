************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:06:30 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           4.806e+00      1.00002   4.806e+00
Objects:              2.900e+01      1.00000   2.900e+01
Flops:                5.977e+08      1.10645   5.634e+08  1.690e+09
Flops/sec:            1.244e+08      1.10645   1.172e+08  3.517e+08
MPI Messages:         1.000e+01      1.17647   9.167e+00  2.750e+01
MPI Message Lengths:  7.121e+06      1.37984   6.428e+05  1.768e+07
MPI Reductions:       1.800e+01      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 4.8061e+00 100.0%  1.6903e+09 100.0%  2.750e+01 100.0%  6.428e+05      100.0%  1.700e+01  94.4% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatSolve               2 1.0 4.4539e-03 1.1 7.29e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  4568
MatLUFactorNum         2 1.0 2.4289e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  5 99  0  0  0   5 99  0  0  0  6875
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin       2 1.0 6.1331e-03 1.7 0.00e+00 0.0 1.5e+01 1.2e+06 2.0e+00  0  0 55100 11   0  0 55100 12     0
MatAssemblyEnd         2 1.0 6.3354e-02 1.0 0.00e+00 0.0 1.2e+01 5.5e+03 9.0e+00  1  0 44  0 50   1  0 44  0 53     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries         2 1.0 1.7771e-02 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm                2 1.0 1.8229e-02246.6 4.37e+04 1.1 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  6   0  0  0  0  6     7
VecCopy                2 1.0 6.9141e-05 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                 6 1.0 9.7275e-05 2.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize           2 1.0 1.8232e-02243.5 4.37e+04 1.1 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  6   0  0  0  0  6     7
KSPSetUp               4 1.0 1.1659e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve               2 1.0 2.6322e-01 1.0 5.98e+08 1.1 0.0e+00 0.0e+00 1.0e+00  5100  0  0  6   5100  0  0  6  6422
PCSetUp                4 1.0 2.5847e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  5 99  0  0  0   5 99  0  0  0  6461
PCSetUpOnBlocks        2 1.0 2.5842e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  5 99  0  0  0   5 99  0  0  0  6462
PCApply                2 1.0 4.4858e-03 1.1 7.29e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  4536
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    11              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 3.00407e-06
Average time for zero size MPI_Send(): 1.03315e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:06:31 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           5.930e+00      1.00003   5.930e+00
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                1.418e+09      1.11114   1.336e+09  4.008e+09
Flops/sec:            2.392e+08      1.11117   2.253e+08  6.758e+08
MPI Messages:         4.500e+01      1.07143   4.333e+01  1.300e+02
MPI Message Lengths:  1.493e+07      1.37542   2.858e+05  3.715e+07
MPI Reductions:       2.077e+03      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 5.9250e+00  99.9%  4.0078e+09 100.0%  1.270e+02  97.7%  2.858e+05      100.0%  5.000e+01   2.4% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult               28 1.0 7.9446e-02 1.0 1.10e+08 1.1 8.4e+01 2.2e+04 0.0e+00  1  8 65  5  0   1  8 66  5  0  3918
MatSolve              32 1.0 7.7865e-02 1.2 1.17e+08 1.1 0.0e+00 0.0e+00 0.0e+00  1  8  0  0  0   1  8  0  0  0  4181
MatLUFactorNum         4 1.0 5.0134e-01 1.1 1.18e+09 1.1 0.0e+00 0.0e+00 0.0e+00  8 83  0  0  0   8 83  0  0  0  6662
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin       4 1.0 1.0942e-02 1.6 0.00e+00 0.0 3.0e+01 1.2e+06 4.0e+00  0  0 23 95  0   0  0 24 95  8     0
MatAssemblyEnd         4 1.0 1.0786e-01 1.0 0.00e+00 0.0 1.2e+01 5.5e+03 1.1e+01  2  0  9  0  1   2  0  9  0 22     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries         4 1.0 2.1369e-02 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot               28 1.0 1.7309e-02 8.0 4.59e+06 1.1 0.0e+00 0.0e+00 1.4e+01  0  0  0  0  1   0  0  0  0 28   765
VecNorm               32 1.0 6.3775e-0276.0 6.99e+05 1.1 0.0e+00 0.0e+00 1.6e+01  1  0  0  0  1   1  0  0  0 32    32
VecScale              30 1.0 1.3733e-04 1.3 3.28e+05 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6889
VecCopy                4 1.0 8.9169e-05 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                40 1.0 4.7708e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY                2 1.0 5.0068e-05 2.5 4.37e+04 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2520
VecMAXPY              30 1.0 1.0617e-03 1.2 5.20e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 14140
VecScatterBegin       28 1.0 2.7990e-04 1.3 0.00e+00 0.0 8.4e+01 2.2e+04 0.0e+00  0  0 65  5  0   0  0 66  5  0     0
VecScatterEnd         28 1.0 1.4538e-0221.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          32 1.0 6.3915e-0264.5 1.03e+06 1.1 0.0e+00 0.0e+00 1.6e+01  1  0  0  0  1   1  0  0  0 32    46
KSPGMRESOrthog        28 1.0 1.8138e-02 5.7 9.17e+06 1.1 0.0e+00 0.0e+00 1.4e+01  0  1  0  0  1   0  1  0  0 28  1461
KSPSetUp               6 1.0 1.1659e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve               4 1.0 6.7883e-01 1.0 1.42e+09 1.1 8.4e+01 2.2e+04 3.0e+01 11100 65  5  1  11100 66  5 60  5904
PCSetUp                8 1.0 5.1692e-01 1.1 1.18e+09 1.1 0.0e+00 0.0e+00 0.0e+00  8 83  0  0  0   8 83  0  0  0  6461
PCSetUpOnBlocks        4 1.0 2.5842e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  4 42  0  0  0   4 42  0  0  0  6462
PCApply               32 1.0 3.3691e-01 1.3 7.07e+08 1.1 0.0e+00 0.0e+00 0.0e+00  5 50  0  0  0   5 50  0  0  0  5923
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 2.19345e-06
Average time for zero size MPI_Send(): 1.03315e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:06:32 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           7.062e+00      1.00002   7.062e+00
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                2.239e+09      1.11239   2.108e+09  6.325e+09
Flops/sec:            3.171e+08      1.11242   2.986e+08  8.957e+08
MPI Messages:         8.000e+01      1.05960   7.750e+01  2.325e+02
MPI Message Lengths:  2.274e+07      1.37404   2.435e+05  5.662e+07
MPI Reductions:       4.136e+03      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 7.0510e+00  99.8%  6.3253e+09 100.0%  2.265e+02  97.4%  2.435e+05      100.0%  8.300e+01   2.0% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult               56 1.0 1.5805e-01 1.0 2.21e+08 1.1 1.7e+02 2.2e+04 0.0e+00  2 10 72  7  0   2 10 74  7  0  3939
MatSolve              62 1.0 1.5056e-01 1.2 2.26e+08 1.1 0.0e+00 0.0e+00 0.0e+00  2 10  0  0  0   2 10  0  0  0  4189
MatLUFactorNum         6 1.0 7.5840e-01 1.2 1.77e+09 1.1 0.0e+00 0.0e+00 0.0e+00 10 79  0  0  0  10 79  0  0  0  6605
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin       6 1.0 1.6587e-02 1.6 0.00e+00 0.0 4.5e+01 1.2e+06 6.0e+00  0  0 19 93  0   0  0 20 93  7     0
MatAssemblyEnd         6 1.0 1.5037e-01 1.0 0.00e+00 0.0 1.2e+01 5.5e+03 1.3e+01  2  0  5  0  0   2  0  5  0 16     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries         6 1.0 2.4946e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot               56 1.0 2.9709e-02 7.6 9.17e+06 1.1 0.0e+00 0.0e+00 2.8e+01  0  0  0  0  1   0  0  0  0 34   892
VecNorm               62 1.0 1.1237e-0172.5 1.35e+06 1.1 0.0e+00 0.0e+00 3.1e+01  1  0  0  0  1   1  0  0  0 37    35
VecScale              60 1.0 2.9349e-04 1.3 6.55e+05 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6447
VecCopy                6 1.0 1.3018e-04 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                74 1.0 1.2436e-03 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY                4 1.0 7.2002e-05 1.8 8.73e+04 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  3504
VecMAXPY              60 1.0 2.1036e-03 1.2 1.04e+07 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 14273
VecScatterBegin       56 1.0 5.0354e-04 1.3 0.00e+00 0.0 1.7e+02 2.2e+04 0.0e+00  0  0 72  7  0   0  0 74  7  0     0
VecScatterEnd         56 1.0 2.4318e-0221.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          62 1.0 1.1265e-0160.9 2.01e+06 1.1 0.0e+00 0.0e+00 3.1e+01  1  0  0  0  1   1  0  0  0 37    52
KSPGMRESOrthog        56 1.0 3.1480e-02 5.3 1.83e+07 1.1 0.0e+00 0.0e+00 2.8e+01  0  1  0  0  1   0  1  0  0 34  1683
KSPSetUp               8 1.0 1.1754e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve               6 1.0 1.0900e+00 1.0 2.24e+09 1.1 1.7e+02 2.2e+04 5.9e+01 15100 72  7  1  15100 74  7 71  5803
PCSetUp               12 1.0 7.7399e-01 1.2 1.77e+09 1.1 0.0e+00 0.0e+00 0.0e+00 10 79  0  0  0  10 79  0  0  0  6472
PCSetUpOnBlocks        6 1.0 2.5843e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  4 26  0  0  0   4 26  0  0  0  6462
PCApply               62 1.0 6.6714e-01 1.2 1.41e+09 1.1 0.0e+00 0.0e+00 0.0e+00  9 63  0  0  0   9 63  0  0  0  5951
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 3.19481e-06
Average time for zero size MPI_Send(): 1.03315e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:06:33 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           8.211e+00      1.00002   8.211e+00
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                3.060e+09      1.11298   2.881e+09  8.643e+09
Flops/sec:            3.727e+08      1.11300   3.509e+08  1.053e+09
MPI Messages:         1.150e+02      1.05505   1.117e+02  3.350e+02
MPI Message Lengths:  3.056e+07      1.37337   2.272e+05  7.610e+07
MPI Reductions:       6.195e+03      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 8.1951e+00  99.8%  8.6428e+09 100.0%  3.260e+02  97.3%  2.272e+05      100.0%  1.160e+02   1.9% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult               84 1.0 2.3543e-01 1.0 3.31e+08 1.1 2.5e+02 2.2e+04 0.0e+00  3 11 75  7  0   3 11 77  7  0  3966
MatSolve              92 1.0 2.2234e-01 1.2 3.35e+08 1.1 0.0e+00 0.0e+00 0.0e+00  2 11  0  0  0   2 11  0  0  0  4209
MatLUFactorNum         8 1.0 1.0118e+00 1.2 2.36e+09 1.1 0.0e+00 0.0e+00 0.0e+00 11 77  0  0  0  11 77  0  0  0  6601
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin       8 1.0 2.1908e-02 1.5 0.00e+00 0.0 6.0e+01 1.2e+06 8.0e+00  0  0 18 93  0   0  0 18 93  7     0
MatAssemblyEnd         8 1.0 1.9479e-01 1.0 0.00e+00 0.0 1.2e+01 5.5e+03 1.5e+01  2  0  4  0  0   2  0  4  0 13     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries         8 1.0 2.8933e-02 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot               84 1.0 4.5484e-02 8.1 1.38e+07 1.1 0.0e+00 0.0e+00 4.2e+01  0  0  0  0  1   0  0  0  0 36   874
VecNorm               92 1.0 1.6287e-0171.2 2.01e+06 1.1 0.0e+00 0.0e+00 4.6e+01  1  0  0  0  1   1  0  0  0 40    36
VecScale              90 1.0 4.2915e-04 1.3 9.83e+05 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6614
VecCopy                8 1.0 1.5020e-04 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               108 1.0 1.5612e-03 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY                6 1.0 9.4891e-05 1.6 1.31e+05 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  3988
VecMAXPY              90 1.0 3.1474e-03 1.2 1.56e+07 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 14309
VecScatterBegin       84 1.0 7.1931e-04 1.3 0.00e+00 0.0 2.5e+02 2.2e+04 0.0e+00  0  0 75  7  0   0  0 77  7  0     0
VecScatterEnd         84 1.0 3.6472e-0222.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          92 1.0 1.6327e-0160.2 2.99e+06 1.1 0.0e+00 0.0e+00 4.6e+01  1  0  0  0  1   1  0  0  0 40    53
KSPGMRESOrthog        84 1.0 4.8095e-02 5.6 2.75e+07 1.1 0.0e+00 0.0e+00 4.2e+01  0  1  0  0  1   0  1  0  0 36  1652
KSPSetUp              10 1.0 1.1754e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve               8 1.0 1.4954e+00 1.0 3.06e+09 1.1 2.5e+02 2.2e+04 8.8e+01 18100 75  7  1  18100 77  7 76  5780
PCSetUp               16 1.0 1.0274e+00 1.2 2.36e+09 1.1 0.0e+00 0.0e+00 0.0e+00 12 77  0  0  0  12 77  0  0  0  6501
PCSetUpOnBlocks        8 1.0 2.5843e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  3 19  0  0  0   3 19  0  0  0  6462
PCApply               92 1.0 9.9283e-01 1.2 2.11e+09 1.1 0.0e+00 0.0e+00 0.0e+00 11 69  0  0  0  11 69  0  0  0  5988
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 2.24113e-06
Average time for zero size MPI_Send(): 9.53674e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:06:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           9.405e+00      1.00003   9.405e+00
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                3.881e+09      1.11331   3.653e+09  1.096e+10
Flops/sec:            4.126e+08      1.11334   3.885e+08  1.165e+09
MPI Messages:         1.500e+02      1.05263   1.458e+02  4.375e+02
MPI Message Lengths:  3.837e+07      1.37297   2.184e+05  9.557e+07
MPI Reductions:       8.254e+03      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 9.3842e+00  99.8%  1.0960e+10 100.0%  4.255e+02  97.3%  2.184e+05      100.0%  1.490e+02   1.8% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult              112 1.0 3.1105e-01 1.0 4.41e+08 1.1 3.4e+02 2.2e+04 0.0e+00  3 11 77  8  0   3 11 79  8  0  4002
MatSolve             122 1.0 2.9198e-01 1.2 4.45e+08 1.1 0.0e+00 0.0e+00 0.0e+00  3 11  0  0  0   3 11  0  0  0  4251
MatLUFactorNum        10 1.0 1.2615e+00 1.2 2.95e+09 1.1 0.0e+00 0.0e+00 0.0e+00 12 76  0  0  0  12 76  0  0  0  6619
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      10 1.0 2.7513e-02 1.5 0.00e+00 0.0 7.5e+01 1.2e+06 1.0e+01  0  0 17 92  0   0  0 18 92  7     0
MatAssemblyEnd        10 1.0 2.3939e-01 1.0 0.00e+00 0.0 1.2e+01 5.5e+03 1.7e+01  2  0  3  0  0   2  0  3  0 11     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        10 1.0 3.2538e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot              112 1.0 5.9191e-02 8.1 1.83e+07 1.1 0.0e+00 0.0e+00 5.6e+01  0  0  0  0  1   0  0  0  0 38   895
VecNorm              122 1.0 1.9548e-0166.7 2.66e+06 1.1 0.0e+00 0.0e+00 6.1e+01  1  0  0  0  1   1  0  0  0 41    39
VecScale             120 1.0 5.5385e-04 1.3 1.31e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6833
VecCopy               10 1.0 1.9479e-04 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               142 1.0 2.2886e-03 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY                8 1.0 1.1706e-04 1.4 1.75e+05 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  4310
VecMAXPY             120 1.0 4.1392e-03 1.2 2.08e+07 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 14507
VecScatterBegin      112 1.0 9.5749e-04 1.3 0.00e+00 0.0 3.4e+02 2.2e+04 0.0e+00  0  0 77  8  0   0  0 79  8  0     0
VecScatterEnd        112 1.0 4.7576e-0223.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize         122 1.0 1.9600e-0156.4 3.97e+06 1.1 0.0e+00 0.0e+00 6.1e+01  1  0  0  0  1   1  0  0  0 41    59
KSPGMRESOrthog       112 1.0 6.2634e-02 5.6 3.67e+07 1.1 0.0e+00 0.0e+00 5.6e+01  0  1  0  0  1   0  1  0  0 38  1692
KSPSetUp              12 1.0 1.1849e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              10 1.0 1.8931e+00 1.0 3.88e+09 1.1 3.4e+02 2.2e+04 1.2e+02 20100 77  8  1  20100 79  8 79  5790
PCSetUp               20 1.0 1.2771e+00 1.2 2.95e+09 1.1 0.0e+00 0.0e+00 0.0e+00 13 76  0  0  0  13 76  0  0  0  6538
PCSetUpOnBlocks       10 1.0 2.5843e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  3 15  0  0  0   3 15  0  0  0  6462
PCApply              122 1.0 1.3126e+00 1.2 2.81e+09 1.1 0.0e+00 0.0e+00 0.0e+00 13 72  0  0  0  13 72  0  0  0  6034
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 2.38419e-06
Average time for zero size MPI_Send(): 1.03315e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:06:36 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           1.092e+01      1.00002   1.092e+01
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                4.701e+09      1.11353   4.426e+09  1.328e+10
Flops/sec:            4.304e+08      1.11356   4.052e+08  1.215e+09
MPI Messages:         1.850e+02      1.05114   1.800e+02  5.400e+02
MPI Message Lengths:  4.618e+07      1.37271   2.130e+05  1.150e+08
MPI Reductions:       1.031e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 1.0899e+01  99.8%  1.3278e+10 100.0%  5.250e+02  97.2%  2.130e+05      100.0%  1.820e+02   1.8% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult              140 1.0 4.0948e-01 1.0 5.51e+08 1.1 4.2e+02 2.2e+04 0.0e+00  4 12 78  8  0   4 12 80  8  0  3800
MatSolve             152 1.0 3.6084e-01 1.1 5.54e+08 1.1 0.0e+00 0.0e+00 0.0e+00  3 12  0  0  0   3 12  0  0  0  4285
MatLUFactorNum        12 1.0 1.4986e+00 1.1 3.54e+09 1.1 0.0e+00 0.0e+00 0.0e+00 13 75  0  0  0  13 75  0  0  0  6686
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      12 1.0 3.6670e-02 1.7 0.00e+00 0.0 9.0e+01 1.2e+06 1.2e+01  0  0 17 92  0   0  0 17 92  7     0
MatAssemblyEnd        12 1.0 2.8004e-01 1.1 0.00e+00 0.0 1.2e+01 5.5e+03 1.9e+01  2  0  2  0  0   2  0  2  0 10     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        12 1.0 3.6154e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot              140 1.0 7.7837e-02 3.0 2.29e+07 1.1 0.0e+00 0.0e+00 7.0e+01  0  0  0  0  1   0  0  0  0 38   851
VecNorm              152 1.0 2.4192e-01 2.9 3.32e+06 1.1 0.0e+00 0.0e+00 7.6e+01  1  0  0  0  1   1  0  0  0 42    40
VecScale             150 1.0 7.1096e-04 1.2 1.64e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6654
VecCopy               12 1.0 2.2030e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               176 1.0 2.6779e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               10 1.0 1.3900e-04 1.3 2.18e+05 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  4538
VecMAXPY             150 1.0 5.3692e-03 1.2 2.60e+07 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 13980
VecScatterBegin      140 1.0 1.2922e-03 1.4 0.00e+00 0.0 4.2e+02 2.2e+04 0.0e+00  0  0 78  8  0   0  0 80  8  0     0
VecScatterEnd        140 1.0 5.9281e-02 2.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize         152 1.0 2.4259e-01 2.8 4.96e+06 1.1 0.0e+00 0.0e+00 7.6e+01  1  0  0  0  1   1  0  0  0 42    59
KSPGMRESOrthog       140 1.0 8.2338e-02 2.7 4.59e+07 1.1 0.0e+00 0.0e+00 7.0e+01  0  1  0  0  1   0  1  0  0 38  1609
KSPSetUp              14 1.0 1.1849e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              12 1.0 2.3997e+00 1.0 4.70e+09 1.1 4.2e+02 2.2e+04 1.5e+02 22100 78  8  1  22100 80  8 80  5533
PCSetUp               24 1.0 1.5142e+00 1.1 3.54e+09 1.1 0.0e+00 0.0e+00 0.0e+00 13 75  0  0  0  13 75  0  0  0  6617
PCSetUpOnBlocks       12 1.0 2.5843e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  2 13  0  0  0   2 13  0  0  0  6461
PCApply              152 1.0 1.6197e+00 1.1 3.51e+09 1.1 0.0e+00 0.0e+00 0.0e+00 14 75  0  0  0  14 75  0  0  0  6110
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 2.81334e-06
Average time for zero size MPI_Send(): 1.03315e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:06:37 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           1.219e+01      1.00001   1.219e+01
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                5.522e+09      1.11369   5.198e+09  1.560e+10
Flops/sec:            4.531e+08      1.11370   4.265e+08  1.280e+09
MPI Messages:         2.200e+02      1.05012   2.142e+02  6.425e+02
MPI Message Lengths:  5.399e+07      1.37252   2.094e+05  1.345e+08
MPI Reductions:       1.237e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 1.2155e+01  99.7%  1.5595e+10 100.0%  6.245e+02  97.2%  2.094e+05      100.0%  2.150e+02   1.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult              168 1.0 4.9039e-01 1.0 6.62e+08 1.1 5.0e+02 2.2e+04 0.0e+00  4 12 78  8  0   4 12 81  8  0  3808
MatSolve             182 1.0 4.3402e-01 1.2 6.63e+08 1.1 0.0e+00 0.0e+00 0.0e+00  3 12  0  0  0   3 12  0  0  0  4266
MatLUFactorNum        14 1.0 1.7444e+00 1.1 4.13e+09 1.1 0.0e+00 0.0e+00 0.0e+00 14 75  0  0  0  14 75  0  0  0  6701
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      14 1.0 4.2336e-02 1.6 0.00e+00 0.0 1.0e+02 1.2e+06 1.4e+01  0  0 16 92  0   0  0 17 92  7     0
MatAssemblyEnd        14 1.0 3.2813e-01 1.1 0.00e+00 0.0 1.2e+01 5.5e+03 2.1e+01  3  0  2  0  0   3  0  2  0 10     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        14 1.0 3.9840e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot              168 1.0 9.3967e-02 3.3 2.75e+07 1.1 0.0e+00 0.0e+00 8.4e+01  0  1  0  0  1   0  1  0  0 39   846
VecNorm              182 1.0 2.7157e-01 2.9 3.97e+06 1.1 0.0e+00 0.0e+00 9.1e+01  1  0  0  0  1   1  0  0  0 42    42
VecScale             180 1.0 8.3184e-04 1.2 1.97e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6824
VecCopy               14 1.0 2.4271e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               210 1.0 3.4397e-03 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               12 1.0 1.8668e-04 1.2 2.62e+05 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  4054
VecMAXPY             180 1.0 6.4547e-03 1.2 3.12e+07 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 13954
VecScatterBegin      168 1.0 1.5476e-03 1.4 0.00e+00 0.0 5.0e+02 2.2e+04 0.0e+00  0  0 78  8  0   0  0 81  8  0     0
VecScatterEnd        168 1.0 7.2159e-02 3.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize         182 1.0 2.7237e-01 2.9 5.94e+06 1.1 0.0e+00 0.0e+00 9.1e+01  1  0  0  0  1   1  0  0  0 42    63
KSPGMRESOrthog       168 1.0 9.9350e-02 2.9 5.50e+07 1.1 0.0e+00 0.0e+00 8.4e+01  0  1  0  0  1   0  1  0  0 39  1600
KSPSetUp              16 1.0 1.1945e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              14 1.0 2.8087e+00 1.0 5.52e+09 1.1 5.0e+02 2.2e+04 1.8e+02 23100 78  8  1  23100 81  8 81  5553
PCSetUp               28 1.0 1.7600e+00 1.1 4.13e+09 1.1 0.0e+00 0.0e+00 0.0e+00 14 75  0  0  0  14 75  0  0  0  6641
PCSetUpOnBlocks       14 1.0 2.5843e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  2 11  0  0  0   2 11  0  0  0  6461
PCApply              182 1.0 1.9428e+00 1.1 4.21e+09 1.1 0.0e+00 0.0e+00 0.0e+00 15 76  0  0  0  15 76  0  0  0  6110
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 2.00272e-06
Average time for zero size MPI_Send(): 9.53674e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:06:39 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           1.376e+01      1.00002   1.376e+01
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                6.360e+09      1.11386   5.986e+09  1.796e+10
Flops/sec:            4.623e+08      1.11387   4.352e+08  1.306e+09
MPI Messages:         2.570e+02      1.04898   2.503e+02  7.510e+02
MPI Message Lengths:  6.185e+07      1.37231   2.052e+05  1.541e+08
MPI Reductions:       1.443e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 1.3718e+01  99.7%  1.7959e+10 100.0%  7.300e+02  97.2%  2.052e+05      100.0%  2.500e+02   1.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult              198 1.0 5.9939e-01 1.0 7.80e+08 1.1 5.9e+02 2.2e+04 0.0e+00  4 12 79  9  0   4 12 81  9  0  3672
MatSolve             214 1.0 5.1649e-01 1.1 7.80e+08 1.1 0.0e+00 0.0e+00 0.0e+00  4 12  0  0  0   4 12  0  0  0  4215
MatLUFactorNum        16 1.0 2.1025e+00 1.1 4.72e+09 1.1 0.0e+00 0.0e+00 0.0e+00 15 74  0  0  0  15 74  0  0  0  6354
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      16 1.0 4.8550e-02 1.6 0.00e+00 0.0 1.2e+02 1.2e+06 1.6e+01  0  0 16 91  0   0  0 16 91  6     0
MatAssemblyEnd        16 1.0 3.7774e-01 1.1 0.00e+00 0.0 1.2e+01 5.5e+03 2.3e+01  3  0  2  0  0   3  0  2  0  9     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        16 1.0 4.3624e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot              198 1.0 9.6941e-02 1.8 3.28e+07 1.1 0.0e+00 0.0e+00 9.9e+01  0  1  0  0  1   0  1  0  0 40   976
VecNorm              214 1.0 2.9948e-01 3.2 4.67e+06 1.1 0.0e+00 0.0e+00 1.1e+02  1  0  0  0  1   1  0  0  0 43    45
VecScale             212 1.0 1.0083e-03 1.1 2.31e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6631
VecCopy               16 1.0 2.6870e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               246 1.0 4.4732e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               14 1.0 2.5558e-04 1.4 3.06e+05 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  3455
VecMAXPY             212 1.0 8.2631e-03 1.1 3.71e+07 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 12961
VecScatterBegin      198 1.0 1.9221e-03 1.4 0.00e+00 0.0 5.9e+02 2.2e+04 0.0e+00  0  0 79  9  0   0  0 81  9  0     0
VecScatterEnd        198 1.0 7.3319e-02 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize         214 1.0 3.0058e-01 3.2 6.99e+06 1.1 0.0e+00 0.0e+00 1.1e+02  1  0  0  0  1   1  0  0  0 43    67
KSPGMRESOrthog       198 1.0 1.0390e-01 1.7 6.55e+07 1.1 0.0e+00 0.0e+00 9.9e+01  1  1  0  0  1   1  1  0  0 40  1821
KSPSetUp              18 1.0 1.1945e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              16 1.0 3.3825e+00 1.0 6.36e+09 1.1 5.9e+02 2.2e+04 2.1e+02 25100 79  9  1  25100 81  9 82  5310
PCSetUp               32 1.0 2.1182e+00 1.1 4.72e+09 1.1 0.0e+00 0.0e+00 0.0e+00 15 74  0  0  0  15 74  0  0  0  6307
PCSetUpOnBlocks       16 1.0 2.5843e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  2  9  0  0  0   2  9  0  0  0  6461
PCApply              214 1.0 2.3806e+00 1.1 4.91e+09 1.1 0.0e+00 0.0e+00 0.0e+00 17 77  0  0  0  17 77  0  0  0  5824
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 1.62125e-06
Average time for zero size MPI_Send(): 1.03315e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:06:41 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           1.554e+01      1.00003   1.554e+01
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                7.197e+09      1.11398   6.775e+09  2.032e+10
Flops/sec:            4.632e+08      1.11402   4.360e+08  1.308e+09
MPI Messages:         2.940e+02      1.04813   2.865e+02  8.595e+02
MPI Message Lengths:  6.972e+07      1.37214   2.021e+05  1.737e+08
MPI Reductions:       1.649e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 1.5489e+01  99.7%  2.0324e+10 100.0%  8.355e+02  97.2%  2.021e+05      100.0%  2.850e+02   1.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult              228 1.0 6.9275e-01 1.0 8.98e+08 1.1 6.8e+02 2.2e+04 0.0e+00  4 12 80  9  0   4 12 82  9  0  3658
MatSolve             246 1.0 6.0109e-01 1.1 8.97e+08 1.1 0.0e+00 0.0e+00 0.0e+00  4 12  0  0  0   4 12  0  0  0  4163
MatLUFactorNum        18 1.0 2.4578e+00 1.1 5.31e+09 1.1 0.0e+00 0.0e+00 0.0e+00 15 74  0  0  0  15 74  0  0  0  6115
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      18 1.0 5.6053e-02 1.6 0.00e+00 0.0 1.4e+02 1.2e+06 1.8e+01  0  0 16 91  0   0  0 16 91  6     0
MatAssemblyEnd        18 1.0 4.1474e-01 1.1 0.00e+00 0.0 1.2e+01 5.5e+03 2.5e+01  3  0  1  0  0   3  0  1  0  9     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        18 1.0 4.7532e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot              228 1.0 1.0838e-01 1.9 3.80e+07 1.1 0.0e+00 0.0e+00 1.1e+02  0  1  0  0  1   0  1  0  0 40  1013
VecNorm              246 1.0 3.2519e-01 3.4 5.37e+06 1.1 0.0e+00 0.0e+00 1.2e+02  2  0  0  0  1   2  0  0  0 43    48
VecScale             244 1.0 1.1542e-03 1.1 2.66e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6667
VecCopy               18 1.0 4.1533e-04 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               282 1.0 5.4712e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               16 1.0 3.1066e-04 1.5 3.49e+05 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  3249
VecMAXPY             244 1.0 9.6352e-03 1.1 4.30e+07 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 12883
VecScatterBegin      228 1.0 2.2411e-03 1.4 0.00e+00 0.0 6.8e+02 2.2e+04 0.0e+00  0  0 80  9  0   0  0 82  9  0     0
VecScatterEnd        228 1.0 8.4126e-02 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize         246 1.0 3.2646e-01 3.4 8.04e+06 1.1 0.0e+00 0.0e+00 1.2e+02  2  0  0  0  1   2  0  0  0 43    71
KSPGMRESOrthog       228 1.0 1.1652e-01 1.7 7.60e+07 1.1 0.0e+00 0.0e+00 1.1e+02  1  1  0  0  1   1  1  0  0 40  1884
KSPSetUp              20 1.0 1.2040e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              18 1.0 3.9216e+00 1.0 7.20e+09 1.1 6.8e+02 2.2e+04 2.4e+02 25100 80  9  1  25100 82  9 83  5183
PCSetUp               36 1.0 2.4734e+00 1.1 5.31e+09 1.1 0.0e+00 0.0e+00 0.0e+00 15 74  0  0  0  15 74  0  0  0  6076
PCSetUpOnBlocks       18 1.0 2.5843e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  2  8  0  0  0   2  8  0  0  0  6461
PCApply              246 1.0 2.8215e+00 1.1 5.62e+09 1.1 0.0e+00 0.0e+00 0.0e+00 17 78  0  0  0  17 78  0  0  0  5622
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 3.00407e-06
Average time for zero size MPI_Send(): 9.53674e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:06:42 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           1.698e+01      1.00001   1.698e+01
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                8.034e+09      1.11409   7.563e+09  2.269e+10
Flops/sec:            4.732e+08      1.11410   4.454e+08  1.336e+09
MPI Messages:         3.310e+02      1.04747   3.227e+02  9.680e+02
MPI Message Lengths:  7.758e+07      1.37201   1.997e+05  1.933e+08
MPI Reductions:       1.856e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 1.6925e+01  99.7%  2.2688e+10 100.0%  9.410e+02  97.2%  1.997e+05      100.0%  3.200e+02   1.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult              258 1.0 7.7434e-01 1.0 1.02e+09 1.1 7.7e+02 2.2e+04 0.0e+00  4 13 80  9  0   4 13 82  9  0  3704
MatSolve             278 1.0 6.7725e-01 1.1 1.01e+09 1.1 0.0e+00 0.0e+00 0.0e+00  4 12  0  0  0   4 12  0  0  0  4176
MatLUFactorNum        20 1.0 2.6907e+00 1.1 5.90e+09 1.1 0.0e+00 0.0e+00 0.0e+00 15 74  0  0  0  15 74  0  0  0  6206
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      20 1.0 6.2191e-02 1.6 0.00e+00 0.0 1.5e+02 1.2e+06 2.0e+01  0  0 15 91  0   0  0 16 91  6     0
MatAssemblyEnd        20 1.0 4.5895e-01 1.1 0.00e+00 0.0 1.2e+01 5.5e+03 2.7e+01  3  0  1  0  0   3  0  1  0  8     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        20 1.0 5.1495e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot              258 1.0 1.1738e-01 2.0 4.32e+07 1.1 0.0e+00 0.0e+00 1.3e+02  0  1  0  0  1   0  1  0  0 40  1064
VecNorm              278 1.0 3.3916e-01 3.6 6.07e+06 1.1 0.0e+00 0.0e+00 1.4e+02  1  0  0  0  1   1  0  0  0 43    52
VecScale             276 1.0 1.2825e-03 1.1 3.01e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6787
VecCopy               20 1.0 4.6515e-04 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               318 1.0 6.2807e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               18 1.0 3.3164e-04 1.4 3.93e+05 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  3423
VecMAXPY             276 1.0 1.0799e-02 1.1 4.89e+07 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 13071
VecScatterBegin      258 1.0 2.5103e-03 1.4 0.00e+00 0.0 7.7e+02 2.2e+04 0.0e+00  0  0 80  9  0   0  0 82  9  0     0
VecScatterEnd        258 1.0 9.3151e-02 2.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize         278 1.0 3.4058e-01 3.5 9.08e+06 1.1 0.0e+00 0.0e+00 1.4e+02  1  0  0  0  1   1  0  0  0 43    77
KSPGMRESOrthog       258 1.0 1.2651e-01 1.8 8.65e+07 1.1 0.0e+00 0.0e+00 1.3e+02  1  1  0  0  1   1  1  0  0 40  1974
KSPSetUp              22 1.0 1.2040e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              20 1.0 4.3160e+00 1.0 8.03e+09 1.1 7.7e+02 2.2e+04 2.7e+02 25100 80  9  1  25100 82  9 84  5257
PCSetUp               40 1.0 2.7064e+00 1.1 5.90e+09 1.1 0.0e+00 0.0e+00 0.0e+00 15 74  0  0  0  15 74  0  0  0  6170
PCSetUpOnBlocks       20 1.0 2.5843e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  1  7  0  0  0   1  7  0  0  0  6461
PCApply              278 1.0 3.1316e+00 1.1 6.33e+09 1.1 0.0e+00 0.0e+00 0.0e+00 17 79  0  0  0  17 79  0  0  0  5702
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 2.57492e-06
Average time for zero size MPI_Send(): 1.03315e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:06:43 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           1.828e+01      1.00001   1.828e+01
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                8.872e+09      1.11417   8.351e+09  2.505e+10
Flops/sec:            4.853e+08      1.11418   4.568e+08  1.370e+09
MPI Messages:         3.680e+02      1.04694   3.588e+02  1.076e+03
MPI Message Lengths:  8.544e+07      1.37190   1.978e+05  2.129e+08
MPI Reductions:       2.062e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 1.8222e+01  99.7%  2.5052e+10 100.0%  1.046e+03  97.2%  1.978e+05      100.0%  3.550e+02   1.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult              288 1.0 8.7371e-01 1.0 1.13e+09 1.1 8.6e+02 2.2e+04 0.0e+00  5 13 80  9  0   5 13 83  9  0  3664
MatSolve             310 1.0 7.7013e-01 1.1 1.13e+09 1.1 0.0e+00 0.0e+00 0.0e+00  4 13  0  0  0   4 13  0  0  0  4095
MatLUFactorNum        22 1.0 2.9446e+00 1.1 6.49e+09 1.1 0.0e+00 0.0e+00 0.0e+00 15 73  0  0  0  15 73  0  0  0  6238
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      22 1.0 6.9500e-02 1.6 0.00e+00 0.0 1.6e+02 1.2e+06 2.2e+01  0  0 15 91  0   0  0 16 91  6     0
MatAssemblyEnd        22 1.0 4.9645e-01 1.1 0.00e+00 0.0 1.2e+01 5.5e+03 2.9e+01  3  0  1  0  0   3  0  1  0  8     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        22 1.0 5.5233e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot              288 1.0 1.3995e-01 2.2 4.85e+07 1.1 0.0e+00 0.0e+00 1.4e+02  1  1  0  0  1   1  1  0  0 41  1001
VecNorm              310 1.0 3.5345e-01 3.1 6.77e+06 1.1 0.0e+00 0.0e+00 1.6e+02  1  0  0  0  1   1  0  0  0 44    55
VecScale             308 1.0 1.4281e-03 1.1 3.36e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6802
VecCopy               22 1.0 4.9305e-04 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               354 1.0 6.6488e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               20 1.0 3.6669e-04 1.4 4.37e+05 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  3440
VecMAXPY             308 1.0 1.2152e-02 1.1 5.48e+07 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 13018
VecScatterBegin      288 1.0 2.7497e-03 1.3 0.00e+00 0.0 8.6e+02 2.2e+04 0.0e+00  0  0 80  9  0   0  0 83  9  0     0
VecScatterEnd        288 1.0 1.1313e-01 2.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize         310 1.0 3.5507e-01 3.0 1.01e+07 1.1 0.0e+00 0.0e+00 1.6e+02  1  0  0  0  1   1  0  0  0 44    82
KSPGMRESOrthog       288 1.0 1.5070e-01 2.0 9.70e+07 1.1 0.0e+00 0.0e+00 1.4e+02  1  1  0  0  1   1  1  0  0 41  1858
KSPSetUp              24 1.0 1.2040e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              22 1.0 4.7875e+00 1.0 8.87e+09 1.1 8.6e+02 2.2e+04 3.0e+02 26100 80  9  1  26100 83  9 84  5233
PCSetUp               44 1.0 2.9603e+00 1.1 6.49e+09 1.1 0.0e+00 0.0e+00 0.0e+00 15 73  0  0  0  15 73  0  0  0  6205
PCSetUpOnBlocks       22 1.0 2.5844e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  1  7  0  0  0   1  7  0  0  0  6461
PCApply              310 1.0 3.4795e+00 1.1 7.03e+09 1.1 0.0e+00 0.0e+00 0.0e+00 18 79  0  0  0  18 79  0  0  0  5705
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 2.38419e-06
Average time for zero size MPI_Send(): 1.35104e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:06:45 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           1.957e+01      1.00001   1.957e+01
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                9.709e+09      1.11424   9.139e+09  2.742e+10
Flops/sec:            4.961e+08      1.11425   4.670e+08  1.401e+09
MPI Messages:         4.050e+02      1.04651   3.950e+02  1.185e+03
MPI Message Lengths:  9.330e+07      1.37181   1.962e+05  2.325e+08
MPI Reductions:       2.268e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 1.9505e+01  99.7%  2.7416e+10 100.0%  1.152e+03  97.2%  1.962e+05      100.0%  3.900e+02   1.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult              318 1.0 9.7839e-01 1.0 1.25e+09 1.1 9.5e+02 2.2e+04 0.0e+00  5 13 81  9  0   5 13 83  9  0  3613
MatSolve             342 1.0 8.4873e-01 1.1 1.25e+09 1.1 0.0e+00 0.0e+00 0.0e+00  4 13  0  0  0   4 13  0  0  0  4099
MatLUFactorNum        24 1.0 3.2220e+00 1.1 7.08e+09 1.1 0.0e+00 0.0e+00 0.0e+00 15 73  0  0  0  16 73  0  0  0  6219
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      24 1.0 7.5137e-02 1.6 0.00e+00 0.0 1.8e+02 1.2e+06 2.4e+01  0  0 15 91  0   0  0 16 91  6     0
MatAssemblyEnd        24 1.0 5.4208e-01 1.1 0.00e+00 0.0 1.2e+01 5.5e+03 3.1e+01  3  0  1  0  0   3  0  1  0  8     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        24 1.0 5.8747e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot              318 1.0 1.4419e-01 1.8 5.37e+07 1.1 0.0e+00 0.0e+00 1.6e+02  1  1  0  0  1   1  1  0  0 41  1076
VecNorm              342 1.0 4.0905e-01 3.5 7.47e+06 1.1 0.0e+00 0.0e+00 1.7e+02  2  0  0  0  1   2  0  0  0 44    53
VecScale             340 1.0 1.5554e-03 1.1 3.71e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6894
VecCopy               24 1.0 5.1904e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               390 1.0 7.6032e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               22 1.0 3.8862e-04 1.4 4.80e+05 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  3571
VecMAXPY             340 1.0 1.3858e-02 1.1 6.07e+07 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 12644
VecScatterBegin      318 1.0 3.0687e-03 1.4 0.00e+00 0.0 9.5e+02 2.2e+04 0.0e+00  0  0 81  9  0   0  0 83  9  0     0
VecScatterEnd        318 1.0 1.1599e-01 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize         342 1.0 4.1082e-01 3.5 1.12e+07 1.1 0.0e+00 0.0e+00 1.7e+02  2  0  0  0  1   2  0  0  0 44    79
KSPGMRESOrthog       318 1.0 1.5632e-01 1.7 1.07e+08 1.1 0.0e+00 0.0e+00 1.6e+02  1  1  0  0  1   1  1  0  0 41  1985
KSPSetUp              26 1.0 1.2040e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              24 1.0 5.2702e+00 1.0 9.71e+09 1.1 9.5e+02 2.2e+04 3.3e+02 27100 81  9  1  27100 83  9 85  5202
PCSetUp               48 1.0 3.2377e+00 1.1 7.08e+09 1.1 0.0e+00 0.0e+00 0.0e+00 16 73  0  0  0  16 73  0  0  0  6189
PCSetUpOnBlocks       24 1.0 2.5844e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  1  6  0  0  0   1  6  0  0  0  6461
PCApply              342 1.0 3.8361e+00 1.1 7.74e+09 1.1 0.0e+00 0.0e+00 0.0e+00 18 80  0  0  0  19 80  0  0  0  5695
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 1.38283e-06
Average time for zero size MPI_Send(): 6.35783e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:06:46 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           2.106e+01      1.00001   2.106e+01
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                1.055e+10      1.11429   9.927e+09  2.978e+10
Flops/sec:            5.008e+08      1.11431   4.714e+08  1.414e+09
MPI Messages:         4.420e+02      1.04615   4.312e+02  1.294e+03
MPI Message Lengths:  1.012e+08      1.37174   1.949e+05  2.522e+08
MPI Reductions:       2.474e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 2.0989e+01  99.7%  2.9780e+10 100.0%  1.258e+03  97.2%  1.949e+05      100.0%  4.250e+02   1.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult              348 1.0 1.0750e+00 1.0 1.37e+09 1.1 1.0e+03 2.2e+04 0.0e+00  5 13 81  9  0   5 13 83  9  0  3599
MatSolve             374 1.0 9.3717e-01 1.1 1.36e+09 1.1 0.0e+00 0.0e+00 0.0e+00  4 13  0  0  0   4 13  0  0  0  4060
MatLUFactorNum        26 1.0 3.5435e+00 1.1 7.67e+09 1.1 0.0e+00 0.0e+00 0.0e+00 16 73  0  0  0  16 73  0  0  0  6126
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      26 1.0 8.1347e-02 1.6 0.00e+00 0.0 2.0e+02 1.2e+06 2.6e+01  0  0 15 91  0   0  0 16 91  6     0
MatAssemblyEnd        26 1.0 5.9064e-01 1.1 0.00e+00 0.0 1.2e+01 5.5e+03 3.3e+01  3  0  1  0  0   3  0  1  0  8     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        26 1.0 6.2592e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot              348 1.0 1.6539e-01 2.0 5.90e+07 1.1 0.0e+00 0.0e+00 1.7e+02  1  1  0  0  1   1  1  0  0 41  1030
VecNorm              374 1.0 4.8777e-01 4.1 8.17e+06 1.1 0.0e+00 0.0e+00 1.9e+02  2  0  0  0  1   2  0  0  0 44    48
VecScale             372 1.0 1.7090e-03 1.1 4.06e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6865
VecCopy               26 1.0 5.4693e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               426 1.0 8.3230e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               24 1.0 4.0960e-04 1.3 5.24e+05 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  3696
VecMAXPY             372 1.0 1.5206e-02 1.1 6.66e+07 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 12644
VecScatterBegin      348 1.0 3.3073e-03 1.3 0.00e+00 0.0 1.0e+03 2.2e+04 0.0e+00  0  0 81  9  0   0  0 83  9  0     0
VecScatterEnd        348 1.0 1.3482e-01 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
VecNormalize         374 1.0 4.8971e-01 4.1 1.22e+07 1.1 0.0e+00 0.0e+00 1.9e+02  2  0  0  0  1   2  0  0  0 44    72
KSPGMRESOrthog       348 1.0 1.7857e-01 1.8 1.18e+08 1.1 0.0e+00 0.0e+00 1.7e+02  1  1  0  0  1   1  1  0  0 41  1907
KSPSetUp              28 1.0 1.2136e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              26 1.0 5.7831e+00 1.0 1.05e+10 1.1 1.0e+03 2.2e+04 3.6e+02 27100 81  9  1  28100 83  9 85  5150
PCSetUp               52 1.0 3.5592e+00 1.1 7.67e+09 1.1 0.0e+00 0.0e+00 0.0e+00 16 73  0  0  0  16 73  0  0  0  6099
PCSetUpOnBlocks       26 1.0 2.5844e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  1  6  0  0  0   1  6  0  0  0  6461
PCApply              374 1.0 4.2472e+00 1.1 8.45e+09 1.1 0.0e+00 0.0e+00 0.0e+00 19 80  0  0  0  19 80  0  0  0  5614
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 2.19345e-06
Average time for zero size MPI_Send(): 9.53674e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:06:48 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           2.246e+01      1.00001   2.246e+01
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                1.137e+10      1.11431   1.070e+10  3.210e+10
Flops/sec:            5.060e+08      1.11432   4.763e+08  1.429e+09
MPI Messages:         4.770e+02      1.04605   4.653e+02  1.396e+03
MPI Message Lengths:  1.090e+08      1.37172   1.946e+05  2.716e+08
MPI Reductions:       2.680e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 2.2389e+01  99.7%  3.2098e+10 100.0%  1.357e+03  97.2%  1.946e+05      100.0%  4.580e+02   1.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult              376 1.0 1.1538e+00 1.0 1.48e+09 1.1 1.1e+03 2.2e+04 0.0e+00  5 13 81  9  0   5 13 83  9  0  3623
MatSolve             404 1.0 1.0094e+00 1.1 1.47e+09 1.1 0.0e+00 0.0e+00 0.0e+00  4 13  0  0  0   4 13  0  0  0  4072
MatLUFactorNum        28 1.0 3.8104e+00 1.1 8.27e+09 1.1 0.0e+00 0.0e+00 0.0e+00 16 73  0  0  0  16 73  0  0  0  6135
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      28 1.0 8.7575e-02 1.6 0.00e+00 0.0 2.1e+02 1.2e+06 2.8e+01  0  0 15 91  0   0  0 15 91  6     0
MatAssemblyEnd        28 1.0 6.3660e-01 1.1 0.00e+00 0.0 1.2e+01 5.5e+03 3.5e+01  3  0  1  0  0   3  0  1  0  8     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        28 1.0 6.6487e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot              376 1.0 1.7417e-01 1.8 6.35e+07 1.1 0.0e+00 0.0e+00 1.9e+02  1  1  0  0  1   1  1  0  0 41  1054
VecNorm              404 1.0 5.0726e-01 4.3 8.82e+06 1.1 0.0e+00 0.0e+00 2.0e+02  2  0  0  0  1   2  0  0  0 44    50
VecScale             402 1.0 1.8358e-03 1.1 4.39e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6906
VecCopy               28 1.0 5.7602e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               460 1.0 8.7490e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               26 1.0 4.7421e-04 1.4 5.68e+05 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  3458
VecMAXPY             402 1.0 1.6414e-02 1.1 7.18e+07 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 12628
VecScatterBegin      376 1.0 3.5620e-03 1.3 0.00e+00 0.0 1.1e+03 2.2e+04 0.0e+00  0  0 81  9  0   0  0 83  9  0     0
VecScatterEnd        376 1.0 1.4248e-01 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
VecNormalize         404 1.0 5.0934e-01 4.2 1.32e+07 1.1 0.0e+00 0.0e+00 2.0e+02  2  0  0  0  1   2  0  0  0 44    75
KSPGMRESOrthog       376 1.0 1.8846e-01 1.7 1.27e+08 1.1 0.0e+00 0.0e+00 1.9e+02  1  1  0  0  1   1  1  0  0 41  1948
KSPSetUp              30 1.0 1.2136e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              28 1.0 6.2145e+00 1.0 1.14e+10 1.1 1.1e+03 2.2e+04 3.9e+02 28100 81  9  1  28100 83  9 85  5165
PCSetUp               56 1.0 3.8262e+00 1.1 8.27e+09 1.1 0.0e+00 0.0e+00 0.0e+00 16 73  0  0  0  16 73  0  0  0  6110
PCSetUpOnBlocks       28 1.0 2.5844e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  1  5  0  0  0   1  5  0  0  0  6461
PCApply              404 1.0 4.5873e+00 1.1 9.15e+09 1.1 0.0e+00 0.0e+00 0.0e+00 19 80  0  0  0  19 80  0  0  0  5628
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 1.81198e-06
Average time for zero size MPI_Send(): 6.35783e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:06:49 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           2.413e+01      1.00001   2.413e+01
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                1.219e+10      1.11433   1.147e+10  3.442e+10
Flops/sec:            5.051e+08      1.11434   4.754e+08  1.426e+09
MPI Messages:         5.120e+02      1.04597   4.995e+02  1.498e+03
MPI Message Lengths:  1.168e+08      1.37170   1.943e+05  2.911e+08
MPI Reductions:       2.886e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 2.4052e+01  99.7%  3.4415e+10 100.0%  1.456e+03  97.2%  1.943e+05      100.0%  4.910e+02   1.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult              404 1.0 1.2490e+00 1.0 1.59e+09 1.1 1.2e+03 2.2e+04 0.0e+00  5 13 81  9  0   5 13 83  9  0  3596
MatSolve             434 1.0 1.0827e+00 1.1 1.58e+09 1.1 0.0e+00 0.0e+00 0.0e+00  4 13  0  0  0   4 13  0  0  0  4078
MatLUFactorNum        30 1.0 4.0619e+00 1.1 8.86e+09 1.1 0.0e+00 0.0e+00 0.0e+00 16 73  0  0  0  16 73  0  0  0  6167
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      30 1.0 9.4564e-02 1.6 0.00e+00 0.0 2.2e+02 1.2e+06 3.0e+01  0  0 15 91  0   0  0 15 91  6     0
MatAssemblyEnd        30 1.0 6.9340e-01 1.1 0.00e+00 0.0 1.2e+01 5.5e+03 3.7e+01  3  0  1  0  0   3  0  1  0  8     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        30 1.0 7.0529e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot              404 1.0 1.8793e-01 1.6 6.81e+07 1.1 0.0e+00 0.0e+00 2.0e+02  1  1  0  0  1   1  1  0  0 41  1047
VecNorm              434 1.0 5.2235e-01 2.4 9.48e+06 1.1 0.0e+00 0.0e+00 2.2e+02  2  0  0  0  1   2  0  0  0 44    52
VecScale             432 1.0 2.0039e-03 1.1 4.72e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6799
VecCopy               30 1.0 6.1011e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               494 1.0 9.1240e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               28 1.0 5.0235e-04 1.4 6.11e+05 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  3516
VecMAXPY             432 1.0 1.7700e-02 1.1 7.70e+07 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 12558
VecScatterBegin      404 1.0 3.8369e-03 1.3 0.00e+00 0.0 1.2e+03 2.2e+04 0.0e+00  0  0 81  9  0   0  0 83  9  0     0
VecScatterEnd        404 1.0 1.5173e-01 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
VecNormalize         434 1.0 5.2461e-01 2.4 1.42e+07 1.1 0.0e+00 0.0e+00 2.2e+02  2  0  0  0  1   2  0  0  0 44    78
KSPGMRESOrthog       404 1.0 2.0338e-01 1.6 1.36e+08 1.1 0.0e+00 0.0e+00 2.0e+02  1  1  0  0  1   1  1  0  0 41  1935
KSPSetUp              32 1.0 1.2231e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              30 1.0 6.7481e+00 1.0 1.22e+10 1.1 1.2e+03 2.2e+04 4.2e+02 28100 81  9  1  28100 83  9 85  5100
PCSetUp               60 1.0 4.0776e+00 1.1 8.86e+09 1.1 0.0e+00 0.0e+00 0.0e+00 16 73  0  0  0  16 73  0  0  0  6143
PCSetUpOnBlocks       30 1.0 2.5844e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  1  5  0  0  0   1  5  0  0  0  6461
PCApply              434 1.0 4.9131e+00 1.1 9.85e+09 1.1 0.0e+00 0.0e+00 0.0e+00 19 81  0  0  0  19 81  0  0  0  5657
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 2.57492e-06
Average time for zero size MPI_Send(): 1.03315e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:06:52 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           2.670e+01      1.00001   2.670e+01
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                1.303e+10      1.11437   1.226e+10  3.678e+10
Flops/sec:            4.879e+08      1.11438   4.592e+08  1.378e+09
MPI Messages:         5.490e+02      1.04571   5.357e+02  1.607e+03
MPI Message Lengths:  1.247e+08      1.37164   1.933e+05  3.107e+08
MPI Reductions:       3.092e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 2.6612e+01  99.7%  3.6780e+10 100.0%  1.562e+03  97.2%  1.933e+05      100.0%  5.260e+02   1.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult              434 1.0 2.2295e+00 1.0 1.71e+09 1.1 1.3e+03 2.2e+04 0.0e+00  8 13 81  9  0   8 13 83  9  0  2164
MatSolve             466 1.0 1.1642e+00 1.1 1.70e+09 1.1 0.0e+00 0.0e+00 0.0e+00  4 13  0  0  0   4 13  0  0  0  4072
MatLUFactorNum        32 1.0 4.3736e+00 1.1 9.45e+09 1.1 0.0e+00 0.0e+00 0.0e+00 16 73  0  0  0  16 73  0  0  0  6109
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      32 1.0 1.0093e-01 1.6 0.00e+00 0.0 2.4e+02 1.2e+06 3.2e+01  0  0 15 91  0   0  0 15 91  6     0
MatAssemblyEnd        32 1.0 7.6348e-01 1.1 0.00e+00 0.0 1.2e+01 5.5e+03 3.9e+01  3  0  1  0  0   3  0  1  0  7     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        32 1.0 7.4197e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot              434 1.0 2.0611e-01 1.5 7.34e+07 1.1 0.0e+00 0.0e+00 2.2e+02  1  1  0  0  1   1  1  0  0 41  1028
VecNorm              466 1.0 6.2070e-01 2.3 1.02e+07 1.1 0.0e+00 0.0e+00 2.3e+02  2  0  0  0  1   2  0  0  0 44    47
VecScale             464 1.0 2.1927e-03 1.1 5.07e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6674
VecCopy               32 1.0 6.6113e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               530 1.0 1.1368e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               30 1.0 5.2834e-04 1.4 6.55e+05 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  3582
VecMAXPY             464 1.0 1.9388e-02 1.1 8.28e+07 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 12343
VecScatterBegin      434 1.0 8.6931e-01279.3 0.00e+00 0.0 1.3e+03 2.2e+04 0.0e+00  2  0 81  9  0   2  0 83  9  0     0
VecScatterEnd        434 1.0 9.6212e-01 6.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0
VecNormalize         466 1.0 6.2316e-01 2.3 1.52e+07 1.1 0.0e+00 0.0e+00 2.3e+02  2  0  0  0  1   2  0  0  0 44    71
KSPGMRESOrthog       434 1.0 2.2286e-01 1.4 1.47e+08 1.1 0.0e+00 0.0e+00 2.2e+02  1  1  0  0  1   1  1  0  0 41  1902
KSPSetUp              34 1.0 1.2350e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              32 1.0 8.1985e+00 1.0 1.30e+10 1.1 1.3e+03 2.2e+04 4.5e+02 31100 81  9  1  31100 83  9 86  4486
PCSetUp               64 1.0 4.3893e+00 1.1 9.45e+09 1.1 0.0e+00 0.0e+00 0.0e+00 16 73  0  0  0  16 73  0  0  0  6087
PCSetUpOnBlocks       32 1.0 2.5844e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  1  5  0  0  0   1  5  0  0  0  6461
PCApply              466 1.0 5.3073e+00 1.1 1.06e+10 1.1 0.0e+00 0.0e+00 0.0e+00 19 81  0  0  0  19 81  0  0  0  5613
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 2.38419e-06
Average time for zero size MPI_Send(): 1.27157e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:06:54 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           2.852e+01      1.00001   2.852e+01
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                1.386e+10      1.11441   1.305e+10  3.914e+10
Flops/sec:            4.860e+08      1.11442   4.575e+08  1.372e+09
MPI Messages:         5.860e+02      1.04550   5.718e+02  1.716e+03
MPI Message Lengths:  1.325e+08      1.37160   1.925e+05  3.303e+08
MPI Reductions:       3.298e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 2.8433e+01  99.7%  3.9144e+10 100.0%  1.668e+03  97.2%  1.925e+05      100.0%  5.610e+02   1.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult              464 1.0 2.3480e+00 1.0 1.83e+09 1.1 1.4e+03 2.2e+04 0.0e+00  8 13 81  9  0   8 13 83  9  0  2197
MatSolve             498 1.0 1.2682e+00 1.1 1.81e+09 1.1 0.0e+00 0.0e+00 0.0e+00  4 13  0  0  0   4 13  0  0  0  3995
MatLUFactorNum        34 1.0 4.6427e+00 1.1 1.00e+10 1.1 0.0e+00 0.0e+00 0.0e+00 16 73  0  0  0  16 73  0  0  0  6114
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      34 1.0 1.0741e-01 1.6 0.00e+00 0.0 2.6e+02 1.2e+06 3.4e+01  0  0 15 91  0   0  0 15 91  6     0
MatAssemblyEnd        34 1.0 8.1424e-01 1.1 0.00e+00 0.0 1.2e+01 5.5e+03 4.1e+01  3  0  1  0  0   3  0  1  0  7     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        34 1.0 7.9117e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot              464 1.0 2.2636e-01 1.6 7.86e+07 1.1 0.0e+00 0.0e+00 2.3e+02  1  1  0  0  1   1  1  0  0 41  1003
VecNorm              498 1.0 6.2178e-01 1.7 1.09e+07 1.1 0.0e+00 0.0e+00 2.5e+02  2  0  0  0  1   2  0  0  0 44    51
VecScale             496 1.0 2.3594e-03 1.1 5.42e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6630
VecCopy               34 1.0 8.7643e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               566 1.0 1.2438e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               32 1.0 6.2037e-04 1.5 6.99e+05 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  3254
VecMAXPY             496 1.0 2.0781e-02 1.1 8.87e+07 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 12335
VecScatterBegin      464 1.0 8.6966e-01257.8 0.00e+00 0.0 1.4e+03 2.2e+04 0.0e+00  2  0 81  9  0   2  0 83  9  0     0
VecScatterEnd        464 1.0 9.6784e-01 5.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0
VecNormalize         498 1.0 6.2443e-01 1.7 1.63e+07 1.1 0.0e+00 0.0e+00 2.5e+02  2  0  0  0  1   2  0  0  0 44    75
KSPGMRESOrthog       464 1.0 2.4452e-01 1.5 1.57e+08 1.1 0.0e+00 0.0e+00 2.3e+02  1  1  0  0  1   1  1  0  0 41  1857
KSPSetUp              36 1.0 1.2350e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              34 1.0 8.7822e+00 1.0 1.39e+10 1.1 1.4e+03 2.2e+04 4.8e+02 31100 81  9  1  31100 83  9 86  4457
PCSetUp               68 1.0 4.6585e+00 1.1 1.00e+10 1.1 0.0e+00 0.0e+00 0.0e+00 16 73  0  0  0  16 73  0  0  0  6094
PCSetUpOnBlocks       34 1.0 2.5844e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  1  4  0  0  0   1  4  0  0  0  6461
PCApply              498 1.0 5.6818e+00 1.1 1.13e+10 1.1 0.0e+00 0.0e+00 0.0e+00 19 81  0  0  0  19 81  0  0  0  5594
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 3.62396e-06
Average time for zero size MPI_Send(): 1.35104e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:06:55 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           3.026e+01      1.00001   3.026e+01
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                1.470e+10      1.11444   1.384e+10  4.151e+10
Flops/sec:            4.857e+08      1.11445   4.572e+08  1.372e+09
MPI Messages:         6.230e+02      1.04530   6.080e+02  1.824e+03
MPI Message Lengths:  1.404e+08      1.37155   1.918e+05  3.499e+08
MPI Reductions:       3.504e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 3.0166e+01  99.7%  4.1508e+10 100.0%  1.773e+03  97.2%  1.918e+05      100.0%  5.960e+02   1.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult              494 1.0 2.4495e+00 1.0 1.95e+09 1.1 1.5e+03 2.2e+04 0.0e+00  8 13 81  9  0   8 13 84  9  0  2242
MatSolve             530 1.0 1.3486e+00 1.1 1.93e+09 1.1 0.0e+00 0.0e+00 0.0e+00  4 13  0  0  0   4 13  0  0  0  3998
MatLUFactorNum        36 1.0 4.9393e+00 1.1 1.06e+10 1.1 0.0e+00 0.0e+00 0.0e+00 16 72  0  0  0  16 72  0  0  0  6085
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      36 1.0 1.1399e-01 1.6 0.00e+00 0.0 2.7e+02 1.2e+06 3.6e+01  0  0 15 91  0   0  0 15 91  6     0
MatAssemblyEnd        36 1.0 8.6637e-01 1.1 0.00e+00 0.0 1.2e+01 5.5e+03 4.3e+01  3  0  1  0  0   3  0  1  0  7     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        36 1.0 8.3227e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot              494 1.0 2.5216e-01 1.6 8.39e+07 1.1 0.0e+00 0.0e+00 2.5e+02  1  1  0  0  1   1  1  0  0 41   960
VecNorm              530 1.0 6.7045e-01 1.8 1.16e+07 1.1 0.0e+00 0.0e+00 2.6e+02  2  0  0  0  1   2  0  0  0 44    50
VecScale             528 1.0 2.5282e-03 1.1 5.76e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6586
VecCopy               36 1.0 9.3150e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               602 1.0 1.3279e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               34 1.0 7.8726e-04 1.8 7.42e+05 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2724
VecMAXPY             528 1.0 2.2088e-02 1.1 9.46e+07 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 12376
VecScatterBegin      494 1.0 8.6995e-01240.2 0.00e+00 0.0 1.5e+03 2.2e+04 0.0e+00  2  0 81  9  0   2  0 84  9  0     0
VecScatterEnd        494 1.0 9.7700e-01 5.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
VecNormalize         530 1.0 6.7326e-01 1.8 1.73e+07 1.1 0.0e+00 0.0e+00 2.6e+02  2  0  0  0  1   2  0  0  0 44    74
KSPGMRESOrthog       494 1.0 2.7130e-01 1.5 1.68e+08 1.1 0.0e+00 0.0e+00 2.5e+02  1  1  0  0  1   1  1  0  0 41  1786
KSPSetUp              38 1.0 1.2445e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              36 1.0 9.2918e+00 1.0 1.47e+10 1.1 1.5e+03 2.2e+04 5.1e+02 31100 81  9  1  31100 84  9 86  4467
PCSetUp               72 1.0 4.9551e+00 1.1 1.06e+10 1.1 0.0e+00 0.0e+00 0.0e+00 16 72  0  0  0  16 72  0  0  0  6066
PCSetUpOnBlocks       36 1.0 2.5844e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  1  4  0  0  0   1  4  0  0  0  6461
PCApply              530 1.0 6.0597e+00 1.1 1.20e+10 1.1 0.0e+00 0.0e+00 0.0e+00 19 81  0  0  0  19 81  0  0  0  5574
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 2.00272e-06
Average time for zero size MPI_Send(): 9.53674e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:06:57 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           3.185e+01      1.00001   3.185e+01
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                1.554e+10      1.11447   1.462e+10  4.387e+10
Flops/sec:            4.878e+08      1.11447   4.591e+08  1.377e+09
MPI Messages:         6.600e+02      1.04513   6.442e+02  1.932e+03
MPI Message Lengths:  1.482e+08      1.37152   1.912e+05  3.695e+08
MPI Reductions:       3.710e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 3.1749e+01  99.7%  4.3872e+10 100.0%  1.878e+03  97.2%  1.912e+05      100.0%  6.310e+02   1.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult              524 1.0 2.5357e+00 1.0 2.06e+09 1.1 1.6e+03 2.2e+04 0.0e+00  8 13 81  9  0   8 13 84  9  0  2297
MatSolve             562 1.0 1.4248e+00 1.1 2.05e+09 1.1 0.0e+00 0.0e+00 0.0e+00  4 13  0  0  0   4 13  0  0  0  4013
MatLUFactorNum        38 1.0 5.1804e+00 1.1 1.12e+10 1.1 0.0e+00 0.0e+00 0.0e+00 16 72  0  0  0  16 72  0  0  0  6124
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      38 1.0 1.2208e-01 1.6 0.00e+00 0.0 2.8e+02 1.2e+06 3.8e+01  0  0 15 91  0   0  0 15 91  6     0
MatAssemblyEnd        38 1.0 9.2029e-01 1.1 0.00e+00 0.0 1.2e+01 5.5e+03 4.5e+01  3  0  1  0  0   3  0  1  0  7     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        38 1.0 8.7182e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot              524 1.0 2.6380e-01 1.7 8.91e+07 1.1 0.0e+00 0.0e+00 2.6e+02  1  1  0  0  1   1  1  0  0 42   976
VecNorm              562 1.0 6.8495e-01 1.8 1.23e+07 1.1 0.0e+00 0.0e+00 2.8e+02  2  0  0  0  1   2  0  0  0 45    52
VecScale             560 1.0 2.6493e-03 1.1 6.11e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6666
VecCopy               38 1.0 1.0896e-03 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               638 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               36 1.0 8.0824e-04 1.7 7.86e+05 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2809
VecMAXPY             560 1.0 2.3185e-02 1.1 1.01e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 12525
VecScatterBegin      524 1.0 8.7025e-01229.3 0.00e+00 0.0 1.6e+03 2.2e+04 0.0e+00  2  0 81  9  0   2  0 84  9  0     0
VecScatterEnd        524 1.0 9.7870e-01 5.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
VecNormalize         562 1.0 6.8792e-01 1.8 1.84e+07 1.1 0.0e+00 0.0e+00 2.8e+02  2  0  0  0  1   2  0  0  0 45    77
KSPGMRESOrthog       524 1.0 2.8398e-01 1.6 1.78e+08 1.1 0.0e+00 0.0e+00 2.6e+02  1  1  0  0  1   1  1  0  0 42  1812
KSPSetUp              40 1.0 1.2445e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              38 1.0 9.7017e+00 1.0 1.55e+10 1.1 1.6e+03 2.2e+04 5.4e+02 30100 81  9  1  31100 84  9 86  4522
PCSetUp               76 1.0 5.1963e+00 1.1 1.12e+10 1.1 0.0e+00 0.0e+00 0.0e+00 16 72  0  0  0  16 72  0  0  0  6106
PCSetUpOnBlocks       38 1.0 2.5845e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  1  4  0  0  0   1  4  0  0  0  6461
PCApply              562 1.0 6.3781e+00 1.1 1.27e+10 1.1 0.0e+00 0.0e+00 0.0e+00 19 82  0  0  0  19 82  0  0  0  5609
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 2.19345e-06
Average time for zero size MPI_Send(): 9.53674e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:06:59 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           3.359e+01      1.00001   3.359e+01
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                1.637e+10      1.11449   1.541e+10  4.624e+10
Flops/sec:            4.875e+08      1.11450   4.588e+08  1.377e+09
MPI Messages:         6.970e+02      1.04498   6.803e+02  2.041e+03
MPI Message Lengths:  1.561e+08      1.37148   1.907e+05  3.891e+08
MPI Reductions:       3.916e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 3.3479e+01  99.7%  4.6236e+10 100.0%  1.984e+03  97.2%  1.907e+05      100.0%  6.660e+02   1.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult              554 1.0 2.6370e+00 1.0 2.18e+09 1.1 1.7e+03 2.2e+04 0.0e+00  8 13 81  9  0   8 13 84  9  0  2335
MatSolve             594 1.0 1.5044e+00 1.1 2.16e+09 1.1 0.0e+00 0.0e+00 0.0e+00  4 13  0  0  0   4 13  0  0  0  4017
MatLUFactorNum        40 1.0 5.4535e+00 1.0 1.18e+10 1.1 0.0e+00 0.0e+00 0.0e+00 16 72  0  0  0  16 72  0  0  0  6124
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      40 1.0 1.3063e-01 1.5 0.00e+00 0.0 3.0e+02 1.2e+06 4.0e+01  0  0 15 91  0   0  0 15 91  6     0
MatAssemblyEnd        40 1.0 9.6986e-01 1.1 0.00e+00 0.0 1.2e+01 5.5e+03 4.7e+01  3  0  1  0  0   3  0  1  0  7     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        40 1.0 9.2186e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot              554 1.0 2.7936e-01 1.5 9.43e+07 1.1 0.0e+00 0.0e+00 2.8e+02  1  1  0  0  1   1  1  0  0 42   975
VecNorm              594 1.0 6.8599e-01 1.6 1.30e+07 1.1 0.0e+00 0.0e+00 3.0e+02  2  0  0  0  1   2  0  0  0 45    55
VecScale             592 1.0 2.8307e-03 1.1 6.46e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6596
VecCopy               40 1.0 1.1415e-03 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               674 1.0 1.6412e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               38 1.0 8.4114e-04 1.6 8.30e+05 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2850
VecMAXPY             592 1.0 2.4713e-02 1.1 1.06e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 12440
VecScatterBegin      554 1.0 8.7058e-01218.6 0.00e+00 0.0 1.7e+03 2.2e+04 0.0e+00  2  0 81  9  0   2  0 84  9  0     0
VecScatterEnd        554 1.0 9.9245e-01 5.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
VecNormalize         594 1.0 6.8914e-01 1.6 1.94e+07 1.1 0.0e+00 0.0e+00 3.0e+02  2  0  0  0  1   2  0  0  0 45    81
KSPGMRESOrthog       554 1.0 3.0106e-01 1.5 1.89e+08 1.1 0.0e+00 0.0e+00 2.8e+02  1  1  0  0  1   1  1  0  0 42  1810
KSPSetUp              42 1.0 1.2445e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              40 1.0 1.0232e+01 1.0 1.64e+10 1.1 1.7e+03 2.2e+04 5.7e+02 30100 81  9  1  31100 84  9 86  4519
PCSetUp               80 1.0 5.4693e+00 1.0 1.18e+10 1.1 0.0e+00 0.0e+00 0.0e+00 16 72  0  0  0  16 72  0  0  0  6106
PCSetUpOnBlocks       40 1.0 2.5845e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  1  4  0  0  0   1  4  0  0  0  6461
PCApply              594 1.0 6.7317e+00 1.1 1.34e+10 1.1 0.0e+00 0.0e+00 0.0e+00 19 82  0  0  0  20 82  0  0  0  5611
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 2.19345e-06
Average time for zero size MPI_Send(): 1.03315e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:07:00 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           3.512e+01      1.00001   3.512e+01
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                1.721e+10      1.11452   1.620e+10  4.860e+10
Flops/sec:            4.901e+08      1.11453   4.613e+08  1.384e+09
MPI Messages:         7.340e+02      1.04484   7.165e+02  2.150e+03
MPI Message Lengths:  1.640e+08      1.37145   1.902e+05  4.087e+08
MPI Reductions:       4.122e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 3.5005e+01  99.7%  4.8600e+10 100.0%  2.090e+03  97.2%  1.902e+05      100.0%  7.010e+02   1.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult              584 1.0 2.7371e+00 1.0 2.30e+09 1.1 1.8e+03 2.2e+04 0.0e+00  8 13 82 10  0   8 13 84 10  0  2372
MatSolve             626 1.0 1.5830e+00 1.1 2.28e+09 1.1 0.0e+00 0.0e+00 0.0e+00  4 13  0  0  0   4 13  0  0  0  4023
MatLUFactorNum        42 1.0 5.7126e+00 1.0 1.24e+10 1.1 0.0e+00 0.0e+00 0.0e+00 16 72  0  0  0  16 72  0  0  0  6138
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      42 1.0 1.4060e-01 1.6 0.00e+00 0.0 3.2e+02 1.2e+06 4.2e+01  0  0 15 90  0   0  0 15 90  6     0
MatAssemblyEnd        42 1.0 1.0136e+00 1.1 0.00e+00 0.0 1.2e+01 5.5e+03 4.9e+01  3  0  1  0  0   3  0  1  0  7     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        42 1.0 9.6739e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot              584 1.0 2.8220e-01 1.4 9.96e+07 1.1 0.0e+00 0.0e+00 2.9e+02  1  1  0  0  1   1  1  0  0 42  1019
VecNorm              626 1.0 6.8702e-01 1.4 1.37e+07 1.1 0.0e+00 0.0e+00 3.1e+02  2  0  0  0  1   2  0  0  0 45    57
VecScale             624 1.0 2.9678e-03 1.1 6.81e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6631
VecCopy               42 1.0 1.1904e-03 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               710 1.0 1.8222e-02 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               40 1.0 8.7714e-04 1.5 8.73e+05 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2876
VecMAXPY             624 1.0 2.6319e-02 1.1 1.12e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 12328
VecScatterBegin      584 1.0 8.7093e-01209.1 0.00e+00 0.0 1.8e+03 2.2e+04 0.0e+00  2  0 82 10  0   2  0 84 10  0     0
VecScatterEnd        584 1.0 1.0085e+00 4.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
VecNormalize         626 1.0 6.9041e-01 1.4 2.05e+07 1.1 0.0e+00 0.0e+00 3.1e+02  2  0  0  0  1   2  0  0  0 45    86
KSPGMRESOrthog       584 1.0 3.0555e-01 1.3 1.99e+08 1.1 0.0e+00 0.0e+00 2.9e+02  1  1  0  0  1   1  1  0  0 42  1883
KSPSetUp              44 1.0 1.2541e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              42 1.0 1.0758e+01 1.0 1.72e+10 1.1 1.8e+03 2.2e+04 6.0e+02 31100 82 10  1  31100 84 10 86  4518
PCSetUp               84 1.0 5.7285e+00 1.0 1.24e+10 1.1 0.0e+00 0.0e+00 0.0e+00 16 72  0  0  0  16 72  0  0  0  6121
PCSetUpOnBlocks       42 1.0 2.5845e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  1  3  0  0  0   1  3  0  0  0  6461
PCApply              626 1.0 7.0703e+00 1.0 1.41e+10 1.1 0.0e+00 0.0e+00 0.0e+00 20 82  0  0  0  20 82  0  0  0  5624
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 1.95503e-06
Average time for zero size MPI_Send(): 7.15256e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:07:02 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           3.661e+01      1.00001   3.661e+01
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                1.805e+10      1.11454   1.699e+10  5.096e+10
Flops/sec:            4.930e+08      1.11454   4.641e+08  1.392e+09
MPI Messages:         7.710e+02      1.04472   7.527e+02  2.258e+03
MPI Message Lengths:  1.718e+08      1.37142   1.897e+05  4.283e+08
MPI Reductions:       4.328e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 3.6490e+01  99.7%  5.0965e+10 100.0%  2.195e+03  97.2%  1.897e+05      100.0%  7.360e+02   1.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult              614 1.0 2.8289e+00 1.0 2.42e+09 1.1 1.8e+03 2.2e+04 0.0e+00  8 13 82 10  0   8 13 84 10  0  2413
MatSolve             658 1.0 1.6615e+00 1.1 2.40e+09 1.1 0.0e+00 0.0e+00 0.0e+00  4 13  0  0  0   4 13  0  0  0  4029
MatLUFactorNum        44 1.0 5.9806e+00 1.0 1.30e+10 1.1 0.0e+00 0.0e+00 0.0e+00 16 72  0  0  0  16 72  0  0  0  6143
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      44 1.0 1.4782e-01 1.6 0.00e+00 0.0 3.3e+02 1.2e+06 4.4e+01  0  0 15 90  0   0  0 15 90  6     0
MatAssemblyEnd        44 1.0 1.0696e+00 1.1 0.00e+00 0.0 1.2e+01 5.5e+03 5.1e+01  3  0  1  0  0   3  0  1  0  7     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        44 1.0 1.0096e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot              614 1.0 2.8786e-01 1.4 1.05e+08 1.1 0.0e+00 0.0e+00 3.1e+02  1  1  0  0  1   1  1  0  0 42  1052
VecNorm              658 1.0 7.5737e-01 1.3 1.44e+07 1.1 0.0e+00 0.0e+00 3.3e+02  2  0  0  0  1   2  0  0  0 45    55
VecScale             656 1.0 3.1040e-03 1.1 7.16e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6665
VecCopy               44 1.0 1.2844e-03 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               746 1.0 1.9639e-02 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               42 1.0 9.0027e-04 1.5 9.17e+05 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2943
VecMAXPY             656 1.0 2.7448e-02 1.1 1.18e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 12441
VecScatterBegin      614 1.0 8.7125e-01200.7 0.00e+00 0.0 1.8e+03 2.2e+04 0.0e+00  2  0 82 10  0   2  0 84 10  0     0
VecScatterEnd        614 1.0 1.0134e+00 4.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
VecNormalize         658 1.0 7.6094e-01 1.3 2.15e+07 1.1 0.0e+00 0.0e+00 3.3e+02  2  0  0  0  1   2  0  0  0 45    82
KSPGMRESOrthog       614 1.0 3.1235e-01 1.4 2.10e+08 1.1 0.0e+00 0.0e+00 3.1e+02  1  1  0  0  1   1  1  0  0 42  1939
KSPSetUp              46 1.0 1.2636e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              44 1.0 1.1284e+01 1.0 1.80e+10 1.1 1.8e+03 2.2e+04 6.4e+02 31100 82 10  1  31100 84 10 86  4517
PCSetUp               88 1.0 5.9964e+00 1.0 1.30e+10 1.1 0.0e+00 0.0e+00 0.0e+00 16 72  0  0  0  16 72  0  0  0  6126
PCSetUpOnBlocks       44 1.0 2.5845e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  1  3  0  0  0   1  3  0  0  0  6461
PCApply              658 1.0 7.4173e+00 1.0 1.48e+10 1.1 0.0e+00 0.0e+00 0.0e+00 20 82  0  0  0  20 82  0  0  0  5630
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 2.57492e-06
Average time for zero size MPI_Send(): 1.03315e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:07:03 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           3.807e+01      1.00001   3.807e+01
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                1.889e+10      1.11455   1.778e+10  5.333e+10
Flops/sec:            4.961e+08      1.11456   4.669e+08  1.401e+09
MPI Messages:         8.080e+02      1.04460   7.888e+02  2.366e+03
MPI Message Lengths:  1.797e+08      1.37140   1.893e+05  4.479e+08
MPI Reductions:       4.534e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 3.7948e+01  99.7%  5.3329e+10 100.0%  2.300e+03  97.2%  1.893e+05      100.0%  7.710e+02   1.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult              644 1.0 2.9315e+00 1.0 2.54e+09 1.1 1.9e+03 2.2e+04 0.0e+00  8 13 82 10  0   8 13 84 10  0  2442
MatSolve             690 1.0 1.7503e+00 1.1 2.51e+09 1.1 0.0e+00 0.0e+00 0.0e+00  4 13  0  0  0   4 13  0  0  0  4010
MatLUFactorNum        46 1.0 6.2880e+00 1.0 1.36e+10 1.1 0.0e+00 0.0e+00 0.0e+00 16 72  0  0  0  16 72  0  0  0  6108
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      46 1.0 1.5449e-01 1.5 0.00e+00 0.0 3.4e+02 1.2e+06 4.6e+01  0  0 15 90  0   0  0 15 90  6     0
MatAssemblyEnd        46 1.0 1.1218e+00 1.1 0.00e+00 0.0 1.2e+01 5.5e+03 5.3e+01  3  0  1  0  0   3  0  1  0  7     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        46 1.0 1.0514e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot              644 1.0 3.0283e-01 1.4 1.10e+08 1.1 0.0e+00 0.0e+00 3.2e+02  1  1  0  0  1   1  1  0  0 42  1050
VecNorm              690 1.0 7.9735e-01 1.4 1.51e+07 1.1 0.0e+00 0.0e+00 3.4e+02  2  0  0  0  1   2  0  0  0 45    55
VecScale             688 1.0 3.2647e-03 1.1 7.51e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6646
VecCopy               46 1.0 1.3325e-03 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               782 1.0 2.0996e-02 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               44 1.0 9.2721e-04 1.4 9.61e+05 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2993
VecMAXPY             688 1.0 2.8702e-02 1.1 1.24e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 12491
VecScatterBegin      644 1.0 8.7155e-01191.6 0.00e+00 0.0 1.9e+03 2.2e+04 0.0e+00  2  0 82 10  0   2  0 84 10  0     0
VecScatterEnd        644 1.0 1.0190e+00 4.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
VecNormalize         690 1.0 8.0109e-01 1.4 2.26e+07 1.1 0.0e+00 0.0e+00 3.4e+02  2  0  0  0  1   2  0  0  0 45    81
KSPGMRESOrthog       644 1.0 3.2848e-01 1.4 2.20e+08 1.1 0.0e+00 0.0e+00 3.2e+02  1  1  0  0  1   1  1  0  0 42  1936
KSPSetUp              48 1.0 1.2732e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              46 1.0 1.1788e+01 1.0 1.89e+10 1.1 1.9e+03 2.2e+04 6.7e+02 31100 82 10  1  31100 84 10 87  4524
PCSetUp               92 1.0 6.3039e+00 1.0 1.36e+10 1.1 0.0e+00 0.0e+00 0.0e+00 16 72  0  0  0  16 72  0  0  0  6093
PCSetUpOnBlocks       46 1.0 2.5845e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  1  3  0  0  0   1  3  0  0  0  6461
PCApply              690 1.0 7.8147e+00 1.0 1.55e+10 1.1 0.0e+00 0.0e+00 0.0e+00 20 82  0  0  0  20 82  0  0  0  5599
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 2.38419e-06
Average time for zero size MPI_Send(): 9.53674e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:07:04 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           3.943e+01      1.00001   3.943e+01
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                1.972e+10      1.11457   1.856e+10  5.569e+10
Flops/sec:            5.002e+08      1.11458   4.708e+08  1.413e+09
MPI Messages:         8.450e+02      1.04450   8.250e+02  2.475e+03
MPI Message Lengths:  1.876e+08      1.37138   1.889e+05  4.675e+08
MPI Reductions:       4.740e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 3.9299e+01  99.7%  5.5693e+10 100.0%  2.406e+03  97.2%  1.889e+05      100.0%  8.060e+02   1.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult              674 1.0 3.0315e+00 1.0 2.65e+09 1.1 2.0e+03 2.2e+04 0.0e+00  8 13 82 10  0   8 13 84 10  0  2471
MatSolve             722 1.0 1.8268e+00 1.1 2.63e+09 1.1 0.0e+00 0.0e+00 0.0e+00  4 13  0  0  0   4 13  0  0  0  4021
MatLUFactorNum        48 1.0 6.5447e+00 1.0 1.42e+10 1.1 0.0e+00 0.0e+00 0.0e+00 16 72  0  0  0  16 72  0  0  0  6123
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      48 1.0 1.5966e-01 1.5 0.00e+00 0.0 3.6e+02 1.2e+06 4.8e+01  0  0 15 90  0   0  0 15 90  6     0
MatAssemblyEnd        48 1.0 1.1693e+00 1.1 0.00e+00 0.0 1.2e+01 5.5e+03 5.5e+01  3  0  0  0  0   3  0  0  0  7     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        48 1.0 1.0895e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot              674 1.0 3.0682e-01 1.3 1.15e+08 1.1 0.0e+00 0.0e+00 3.4e+02  1  1  0  0  1   1  1  0  0 42  1085
VecNorm              722 1.0 8.1728e-01 1.4 1.58e+07 1.1 0.0e+00 0.0e+00 3.6e+02  2  0  0  0  1   2  0  0  0 45    56
VecScale             720 1.0 3.3991e-03 1.1 7.86e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6680
VecCopy               48 1.0 1.3804e-03 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               818 1.0 2.2562e-02 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               46 1.0 9.5224e-04 1.4 1.00e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  3047
VecMAXPY             720 1.0 3.0249e-02 1.1 1.30e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 12415
VecScatterBegin      674 1.0 8.7194e-01184.7 0.00e+00 0.0 2.0e+03 2.2e+04 0.0e+00  1  0 82 10  0   1  0 84 10  0     0
VecScatterEnd        674 1.0 1.0361e+00 4.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
VecNormalize         722 1.0 8.2124e-01 1.4 2.36e+07 1.1 0.0e+00 0.0e+00 3.6e+02  2  0  0  0  1   2  0  0  0 45    83
KSPGMRESOrthog       674 1.0 3.3389e-01 1.3 2.31e+08 1.1 0.0e+00 0.0e+00 3.4e+02  1  1  0  0  1   1  1  0  0 42  1995
KSPSetUp              50 1.0 1.2732e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              48 1.0 1.2244e+01 1.0 1.97e+10 1.1 2.0e+03 2.2e+04 7.0e+02 31100 82 10  1  31100 84 10 87  4549
PCSetUp               96 1.0 6.5606e+00 1.0 1.42e+10 1.1 0.0e+00 0.0e+00 0.0e+00 16 72  0  0  0  16 72  0  0  0  6109
PCSetUpOnBlocks       48 1.0 2.5845e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  1  3  0  0  0   1  3  0  0  0  6461
PCApply              722 1.0 8.1483e+00 1.0 1.62e+10 1.1 0.0e+00 0.0e+00 0.0e+00 20 82  0  0  0  20 82  0  0  0  5615
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 1.62125e-06
Average time for zero size MPI_Send(): 7.15256e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:07:06 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           4.099e+01      1.00000   4.099e+01
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                2.056e+10      1.11459   1.935e+10  5.806e+10
Flops/sec:            5.016e+08      1.11459   4.721e+08  1.416e+09
MPI Messages:         8.820e+02      1.04440   8.612e+02  2.584e+03
MPI Message Lengths:  1.954e+08      1.37136   1.886e+05  4.872e+08
MPI Reductions:       4.947e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 4.0856e+01  99.7%  5.8057e+10 100.0%  2.512e+03  97.2%  1.886e+05      100.0%  8.410e+02   1.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult              704 1.0 3.1293e+00 1.0 2.77e+09 1.1 2.1e+03 2.2e+04 0.0e+00  8 13 82 10  0   8 13 84 10  0  2501
MatSolve             754 1.0 1.9103e+00 1.1 2.75e+09 1.1 0.0e+00 0.0e+00 0.0e+00  4 13  0  0  0   5 13  0  0  0  4015
MatLUFactorNum        50 1.0 6.8435e+00 1.0 1.48e+10 1.1 0.0e+00 0.0e+00 0.0e+00 16 72  0  0  0  16 72  0  0  0  6100
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      50 1.0 1.6676e-01 1.5 0.00e+00 0.0 3.8e+02 1.2e+06 5.0e+01  0  0 15 90  0   0  0 15 90  6     0
MatAssemblyEnd        50 1.0 1.2180e+00 1.1 0.00e+00 0.0 1.2e+01 5.5e+03 5.7e+01  3  0  0  0  0   3  0  0  0  7     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        50 1.0 1.1306e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot              704 1.0 3.1516e-01 1.3 1.21e+08 1.1 0.0e+00 0.0e+00 3.5e+02  1  1  0  0  1   1  1  0  0 42  1105
VecNorm              754 1.0 8.4919e-01 1.4 1.65e+07 1.1 0.0e+00 0.0e+00 3.8e+02  2  0  0  0  1   2  0  0  0 45    56
VecScale             752 1.0 3.5377e-03 1.1 8.21e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6704
VecCopy               50 1.0 1.4305e-03 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               854 1.0 2.3140e-02 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               48 1.0 1.0073e-03 1.4 1.05e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  3006
VecMAXPY             752 1.0 3.1953e-02 1.1 1.36e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 12286
VecScatterBegin      704 1.0 8.7222e-01177.4 0.00e+00 0.0 2.1e+03 2.2e+04 0.0e+00  1  0 82 10  0   1  0 84 10  0     0
VecScatterEnd        704 1.0 1.0414e+00 4.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
VecNormalize         754 1.0 8.5334e-01 1.4 2.47e+07 1.1 0.0e+00 0.0e+00 3.8e+02  2  0  0  0  1   2  0  0  0 45    84
KSPGMRESOrthog       704 1.0 3.4363e-01 1.3 2.41e+08 1.1 0.0e+00 0.0e+00 3.5e+02  1  1  0  0  1   1  1  0  0 42  2026
KSPSetUp              52 1.0 1.2827e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              50 1.0 1.2741e+01 1.0 2.06e+10 1.1 2.1e+03 2.2e+04 7.3e+02 31100 82 10  1  31100 84 10 87  4557
PCSetUp              100 1.0 6.8594e+00 1.0 1.48e+10 1.1 0.0e+00 0.0e+00 0.0e+00 16 72  0  0  0  16 72  0  0  0  6086
PCSetUpOnBlocks       50 1.0 2.5845e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  1  3  0  0  0   1  3  0  0  0  6461
PCApply              754 1.0 8.5316e+00 1.0 1.69e+10 1.1 0.0e+00 0.0e+00 0.0e+00 20 82  0  0  0  20 82  0  0  0  5596
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 1.19209e-06
Average time for zero size MPI_Send(): 1.03315e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:07:08 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           4.264e+01      1.00000   4.264e+01
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                2.140e+10      1.11460   2.014e+10  6.042e+10
Flops/sec:            5.018e+08      1.11461   4.723e+08  1.417e+09
MPI Messages:         9.190e+02      1.04432   8.973e+02  2.692e+03
MPI Message Lengths:  2.033e+08      1.37134   1.882e+05  5.068e+08
MPI Reductions:       5.153e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 4.2507e+01  99.7%  6.0421e+10 100.0%  2.617e+03  97.2%  1.882e+05      100.0%  8.760e+02   1.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult              734 1.0 3.2229e+00 1.0 2.89e+09 1.1 2.2e+03 2.2e+04 0.0e+00  7 14 82 10  0   7 14 84 10  0  2532
MatSolve             786 1.0 1.9951e+00 1.1 2.86e+09 1.1 0.0e+00 0.0e+00 0.0e+00  5 13  0  0  0   5 13  0  0  0  4008
MatLUFactorNum        52 1.0 7.2014e+00 1.0 1.53e+10 1.1 0.0e+00 0.0e+00 0.0e+00 16 72  0  0  0  16 72  0  0  0  6029
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      52 1.0 1.7264e-01 1.5 0.00e+00 0.0 3.9e+02 1.2e+06 5.2e+01  0  0 14 90  0   0  0 15 90  6     0
MatAssemblyEnd        52 1.0 1.2660e+00 1.1 0.00e+00 0.0 1.2e+01 5.5e+03 5.9e+01  3  0  0  0  0   3  0  0  0  7     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        52 1.0 1.1706e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot              734 1.0 3.3051e-01 1.3 1.26e+08 1.1 0.0e+00 0.0e+00 3.7e+02  1  1  0  0  1   1  1  0  0 42  1099
VecNorm              786 1.0 9.3051e-01 1.6 1.72e+07 1.1 0.0e+00 0.0e+00 3.9e+02  2  0  0  0  1   2  0  0  0 45    53
VecScale             784 1.0 3.6795e-03 1.1 8.56e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6720
VecCopy               52 1.0 1.4546e-03 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               890 1.0 2.4038e-02 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               50 1.0 1.0674e-03 1.4 1.09e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2955
VecMAXPY             784 1.0 3.3328e-02 1.1 1.42e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 12290
VecScatterBegin      734 1.0 8.7251e-01168.6 0.00e+00 0.0 2.2e+03 2.2e+04 0.0e+00  1  0 82 10  0   1  0 84 10  0     0
VecScatterEnd        734 1.0 1.0420e+00 4.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
VecNormalize         786 1.0 9.3472e-01 1.6 2.57e+07 1.1 0.0e+00 0.0e+00 3.9e+02  2  0  0  0  1   2  0  0  0 45    79
KSPGMRESOrthog       734 1.0 3.6017e-01 1.3 2.52e+08 1.1 0.0e+00 0.0e+00 3.7e+02  1  1  0  0  1   1  1  0  0 42  2017
KSPSetUp              54 1.0 1.2827e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              52 1.0 1.3287e+01 1.0 2.14e+10 1.1 2.2e+03 2.2e+04 7.6e+02 31100 82 10  1  31100 84 10 87  4547
PCSetUp              104 1.0 7.2173e+00 1.0 1.53e+10 1.1 0.0e+00 0.0e+00 0.0e+00 16 72  0  0  0  16 72  0  0  0  6016
PCSetUpOnBlocks       52 1.0 2.5845e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  1  3  0  0  0   1  3  0  0  0  6461
PCApply              786 1.0 8.9753e+00 1.1 1.76e+10 1.1 0.0e+00 0.0e+00 0.0e+00 20 82  0  0  0  20 82  0  0  0  5542
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 3.19481e-06
Average time for zero size MPI_Send(): 1.03315e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:07:09 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           4.428e+01      1.00000   4.428e+01
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                2.224e+10      1.11462   2.093e+10  6.279e+10
Flops/sec:            5.022e+08      1.11462   4.727e+08  1.418e+09
MPI Messages:         9.560e+02      1.04424   9.335e+02  2.800e+03
MPI Message Lengths:  2.111e+08      1.37132   1.880e+05  5.264e+08
MPI Reductions:       5.359e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 4.4136e+01  99.7%  6.2785e+10 100.0%  2.722e+03  97.2%  1.880e+05      100.0%  9.110e+02   1.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult              764 1.0 3.3276e+00 1.0 3.01e+09 1.1 2.3e+03 2.2e+04 0.0e+00  7 14 82 10  0   7 14 84 10  0  2552
MatSolve             818 1.0 2.0915e+00 1.1 2.98e+09 1.1 0.0e+00 0.0e+00 0.0e+00  5 13  0  0  0   5 13  0  0  0  3979
MatLUFactorNum        54 1.0 7.4778e+00 1.1 1.59e+10 1.1 0.0e+00 0.0e+00 0.0e+00 16 72  0  0  0  16 72  0  0  0  6029
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      54 1.0 1.7948e-01 1.5 0.00e+00 0.0 4.0e+02 1.2e+06 5.4e+01  0  0 14 90  0   0  0 15 90  6     0
MatAssemblyEnd        54 1.0 1.3138e+00 1.1 0.00e+00 0.0 1.2e+01 5.5e+03 6.1e+01  3  0  0  0  0   3  0  0  0  7     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        54 1.0 1.2125e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot              764 1.0 3.5906e-01 1.4 1.31e+08 1.1 0.0e+00 0.0e+00 3.8e+02  1  1  0  0  1   1  1  0  0 42  1054
VecNorm              818 1.0 1.0040e+00 1.6 1.79e+07 1.1 0.0e+00 0.0e+00 4.1e+02  2  0  0  0  1   2  0  0  0 45    51
VecScale             816 1.0 3.8223e-03 1.1 8.91e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6733
VecCopy               54 1.0 1.5006e-03 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               926 1.0 2.6549e-02 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               52 1.0 1.2283e-03 1.6 1.14e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2670
VecMAXPY             816 1.0 3.4613e-02 1.1 1.48e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 12326
VecScatterBegin      764 1.0 8.7278e-01161.7 0.00e+00 0.0 2.3e+03 2.2e+04 0.0e+00  1  0 82 10  0   1  0 84 10  0     0
VecScatterEnd        764 1.0 1.0427e+00 3.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
VecNormalize         818 1.0 1.0084e+00 1.6 2.68e+07 1.1 0.0e+00 0.0e+00 4.1e+02  2  0  0  0  1   2  0  0  0 45    77
KSPGMRESOrthog       764 1.0 3.8999e-01 1.4 2.62e+08 1.1 0.0e+00 0.0e+00 3.8e+02  1  1  0  0  1   1  1  0  0 42  1941
KSPSetUp              56 1.0 1.2827e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              54 1.0 1.3793e+01 1.0 2.22e+10 1.1 2.3e+03 2.2e+04 7.9e+02 31100 82 10  1  31100 84 10 87  4552
PCSetUp              108 1.0 7.4937e+00 1.1 1.59e+10 1.1 0.0e+00 0.0e+00 0.0e+00 16 72  0  0  0  16 72  0  0  0  6016
PCSetUpOnBlocks       54 1.0 2.5845e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  1  3  0  0  0   1  3  0  0  0  6461
PCApply              818 1.0 9.3495e+00 1.1 1.83e+10 1.1 0.0e+00 0.0e+00 0.0e+00 20 82  0  0  0  20 82  0  0  0  5534
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 2.38419e-06
Average time for zero size MPI_Send(): 1.35104e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:07:11 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           4.603e+01      1.00000   4.603e+01
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                2.307e+10      1.11463   2.172e+10  6.515e+10
Flops/sec:            5.012e+08      1.11463   4.718e+08  1.415e+09
MPI Messages:         9.930e+02      1.04416   9.697e+02  2.909e+03
MPI Message Lengths:  2.190e+08      1.37130   1.877e+05  5.460e+08
MPI Reductions:       5.565e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 4.5885e+01  99.7%  6.5150e+10 100.0%  2.828e+03  97.2%  1.877e+05      100.0%  9.460e+02   1.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult              794 1.0 3.4219e+00 1.0 3.13e+09 1.1 2.4e+03 2.2e+04 0.0e+00  7 14 82 10  0   7 14 84 10  0  2579
MatSolve             850 1.0 2.1708e+00 1.1 3.10e+09 1.1 0.0e+00 0.0e+00 0.0e+00  5 13  0  0  0   5 13  0  0  0  3983
MatLUFactorNum        56 1.0 7.7467e+00 1.0 1.65e+10 1.1 0.0e+00 0.0e+00 0.0e+00 16 72  0  0  0  16 72  0  0  0  6036
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      56 1.0 1.8726e-01 1.5 0.00e+00 0.0 4.2e+02 1.2e+06 5.6e+01  0  0 14 90  0   0  0 15 90  6     0
MatAssemblyEnd        56 1.0 1.3622e+00 1.1 0.00e+00 0.0 1.2e+01 5.5e+03 6.3e+01  3  0  0  0  0   3  0  0  0  7     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        56 1.0 1.2580e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot              794 1.0 3.6150e-01 1.3 1.36e+08 1.1 0.0e+00 0.0e+00 4.0e+02  1  1  0  0  1   1  1  0  0 42  1089
VecNorm              850 1.0 1.0052e+00 1.5 1.86e+07 1.1 0.0e+00 0.0e+00 4.2e+02  2  0  0  0  1   2  0  0  0 45    53
VecScale             848 1.0 3.9666e-03 1.1 9.26e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6742
VecCopy               56 1.0 1.5295e-03 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               962 1.0 2.6982e-02 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               54 1.0 1.2553e-03 1.6 1.18e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2713
VecMAXPY             848 1.0 3.5930e-02 1.1 1.54e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 12348
VecScatterBegin      794 1.0 8.7307e-01155.9 0.00e+00 0.0 2.4e+03 2.2e+04 0.0e+00  1  0 82 10  0   1  0 84 10  0     0
VecScatterEnd        794 1.0 1.0487e+00 3.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
VecNormalize         850 1.0 1.0097e+00 1.5 2.78e+07 1.1 0.0e+00 0.0e+00 4.2e+02  2  0  0  0  1   2  0  0  0 45    80
KSPGMRESOrthog       794 1.0 3.9375e-01 1.3 2.73e+08 1.1 0.0e+00 0.0e+00 4.0e+02  1  1  0  0  1   1  1  0  0 42  1999
KSPSetUp              58 1.0 1.2827e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              56 1.0 1.4291e+01 1.0 2.31e+10 1.1 2.4e+03 2.2e+04 8.2e+02 31100 82 10  1  31100 84 10 87  4559
PCSetUp              112 1.0 7.7626e+00 1.0 1.65e+10 1.1 0.0e+00 0.0e+00 0.0e+00 16 72  0  0  0  16 72  0  0  0  6023
PCSetUpOnBlocks       56 1.0 2.5845e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  1  3  0  0  0   1  3  0  0  0  6461
PCApply              850 1.0 9.6987e+00 1.1 1.90e+10 1.1 0.0e+00 0.0e+00 0.0e+00 20 82  0  0  0  21 82  0  0  0  5540
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 1.38283e-06
Average time for zero size MPI_Send(): 9.53674e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:07:13 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           4.780e+01      1.00000   4.780e+01
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                2.391e+10      1.11464   2.250e+10  6.751e+10
Flops/sec:            5.002e+08      1.11464   4.708e+08  1.412e+09
MPI Messages:         1.030e+03      1.04410   1.006e+03  3.018e+03
MPI Message Lengths:  2.269e+08      1.37129   1.874e+05  5.656e+08
MPI Reductions:       5.771e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 4.7654e+01  99.7%  6.7514e+10 100.0%  2.934e+03  97.2%  1.874e+05      100.0%  9.810e+02   1.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult              824 1.0 3.5154e+00 1.0 3.24e+09 1.1 2.5e+03 2.2e+04 0.0e+00  7 14 82 10  0   7 14 84 10  0  2606
MatSolve             882 1.0 2.2474e+00 1.1 3.21e+09 1.1 0.0e+00 0.0e+00 0.0e+00  5 13  0  0  0   5 13  0  0  0  3993
MatLUFactorNum        58 1.0 8.0016e+00 1.0 1.71e+10 1.1 0.0e+00 0.0e+00 0.0e+00 16 72  0  0  0  16 72  0  0  0  6052
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      58 1.0 1.9374e-01 1.5 0.00e+00 0.0 4.4e+02 1.2e+06 5.8e+01  0  0 14 90  0   0  0 15 90  6     0
MatAssemblyEnd        58 1.0 1.4095e+00 1.1 0.00e+00 0.0 1.2e+01 5.5e+03 6.5e+01  3  0  0  0  0   3  0  0  0  7     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        58 1.0 1.3056e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot              824 1.0 3.6874e-01 1.4 1.42e+08 1.1 0.0e+00 0.0e+00 4.1e+02  1  1  0  0  1   1  1  0  0 42  1108
VecNorm              882 1.0 1.0677e+00 1.5 1.93e+07 1.1 0.0e+00 0.0e+00 4.4e+02  2  0  0  0  1   2  0  0  0 45    52
VecScale             880 1.0 4.1072e-03 1.1 9.61e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6757
VecCopy               58 1.0 1.5907e-03 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               998 1.0 2.7873e-02 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               56 1.0 1.3084e-03 1.6 1.22e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2700
VecMAXPY             880 1.0 3.7093e-02 1.1 1.59e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 12420
VecScatterBegin      824 1.0 8.7333e-01151.2 0.00e+00 0.0 2.5e+03 2.2e+04 0.0e+00  1  0 82 10  0   1  0 84 10  0     0
VecScatterEnd        824 1.0 1.0582e+00 3.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
VecNormalize         882 1.0 1.0724e+00 1.5 2.89e+07 1.1 0.0e+00 0.0e+00 4.4e+02  2  0  0  0  1   2  0  0  0 45    78
KSPGMRESOrthog       824 1.0 4.0216e-01 1.3 2.83e+08 1.1 0.0e+00 0.0e+00 4.1e+02  1  1  0  0  1   1  1  0  0 42  2033
KSPSetUp              60 1.0 1.2827e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              58 1.0 1.4764e+01 1.0 2.39e+10 1.1 2.5e+03 2.2e+04 8.5e+02 31100 82 10  1  31100 84 10 87  4573
PCSetUp              116 1.0 8.0176e+00 1.0 1.71e+10 1.1 0.0e+00 0.0e+00 0.0e+00 16 72  0  0  0  16 72  0  0  0  6040
PCSetUpOnBlocks       58 1.0 2.5845e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  1  2  0  0  0   1  2  0  0  0  6461
PCApply              882 1.0 1.0031e+01 1.1 1.97e+10 1.1 0.0e+00 0.0e+00 0.0e+00 20 83  0  0  0  20 83  0  0  0  5556
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 3.00407e-06
Average time for zero size MPI_Send(): 9.53674e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:07:15 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           4.959e+01      1.00001   4.959e+01
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                2.475e+10      1.11465   2.329e+10  6.988e+10
Flops/sec:            4.990e+08      1.11466   4.697e+08  1.409e+09
MPI Messages:         1.067e+03      1.04403   1.042e+03  3.126e+03
MPI Message Lengths:  2.347e+08      1.37127   1.872e+05  5.852e+08
MPI Reductions:       5.977e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 4.9434e+01  99.7%  6.9878e+10 100.0%  3.039e+03  97.2%  1.872e+05      100.0%  1.016e+03   1.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult              854 1.0 3.6196e+00 1.0 3.36e+09 1.1 2.6e+03 2.2e+04 0.0e+00  7 14 82 10  0   7 14 84 10  0  2623
MatSolve             914 1.0 2.3216e+00 1.1 3.33e+09 1.1 0.0e+00 0.0e+00 0.0e+00  5 13  0  0  0   5 13  0  0  0  4005
MatLUFactorNum        60 1.0 8.2575e+00 1.0 1.77e+10 1.1 0.0e+00 0.0e+00 0.0e+00 16 72  0  0  0  16 72  0  0  0  6067
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      60 1.0 2.0508e-01 1.6 0.00e+00 0.0 4.5e+02 1.2e+06 6.0e+01  0  0 14 90  0   0  0 15 90  6     0
MatAssemblyEnd        60 1.0 1.4487e+00 1.1 0.00e+00 0.0 1.2e+01 5.5e+03 6.7e+01  3  0  0  0  0   3  0  0  0  7     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        60 1.0 1.4076e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot              854 1.0 3.8136e-01 1.3 1.47e+08 1.1 0.0e+00 0.0e+00 4.3e+02  1  1  0  0  1   1  1  0  0 42  1111
VecNorm              914 1.0 1.0975e+00 1.5 2.00e+07 1.1 0.0e+00 0.0e+00 4.6e+02  2  0  0  0  1   2  0  0  0 45    53
VecScale             912 1.0 4.2989e-03 1.1 9.96e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6691
VecCopy               60 1.0 1.6448e-03 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              1034 1.0 2.8884e-02 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               58 1.0 1.3356e-03 1.5 1.27e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2739
VecMAXPY             912 1.0 3.8561e-02 1.1 1.65e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 12389
VecScatterBegin      854 1.0 8.7362e-01146.1 0.00e+00 0.0 2.6e+03 2.2e+04 0.0e+00  1  0 82 10  0   1  0 84 10  0     0
VecScatterEnd        854 1.0 1.0781e+00 3.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
VecNormalize         914 1.0 1.1025e+00 1.5 2.99e+07 1.1 0.0e+00 0.0e+00 4.6e+02  2  0  0  0  1   2  0  0  0 45    78
KSPGMRESOrthog       854 1.0 4.1615e-01 1.2 2.93e+08 1.1 0.0e+00 0.0e+00 4.3e+02  1  1  0  0  1   1  1  0  0 42  2037
KSPSetUp              62 1.0 1.2922e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              60 1.0 1.5237e+01 1.0 2.47e+10 1.1 2.6e+03 2.2e+04 8.8e+02 31100 82 10  1  31100 84 10 87  4586
PCSetUp              120 1.0 8.2734e+00 1.0 1.77e+10 1.1 0.0e+00 0.0e+00 0.0e+00 16 72  0  0  0  16 72  0  0  0  6055
PCSetUpOnBlocks       60 1.0 2.5846e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  1  2  0  0  0   1  2  0  0  0  6461
PCApply              914 1.0 1.0362e+01 1.1 2.05e+10 1.1 0.0e+00 0.0e+00 0.0e+00 20 83  0  0  0  20 83  0  0  0  5571
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 4.57764e-06
Average time for zero size MPI_Send(): 1.35104e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:07:16 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           5.111e+01      1.00000   5.111e+01
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                2.559e+10      1.11466   2.408e+10  7.224e+10
Flops/sec:            5.006e+08      1.11467   4.711e+08  1.413e+09
MPI Messages:         1.104e+03      1.04397   1.078e+03  3.234e+03
MPI Message Lengths:  2.426e+08      1.37126   1.870e+05  6.048e+08
MPI Reductions:       6.183e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 5.0952e+01  99.7%  7.2242e+10 100.0%  3.144e+03  97.2%  1.870e+05      100.0%  1.051e+03   1.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult              884 1.0 3.7190e+00 1.0 3.48e+09 1.1 2.7e+03 2.2e+04 0.0e+00  7 14 82 10  0   7 14 84 10  0  2642
MatSolve             946 1.0 2.3975e+00 1.1 3.45e+09 1.1 0.0e+00 0.0e+00 0.0e+00  5 13  0  0  0   5 13  0  0  0  4014
MatLUFactorNum        62 1.0 8.5056e+00 1.0 1.83e+10 1.1 0.0e+00 0.0e+00 0.0e+00 16 72  0  0  0  16 72  0  0  0  6086
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      62 1.0 2.1492e-01 1.6 0.00e+00 0.0 4.6e+02 1.2e+06 6.2e+01  0  0 14 90  0   0  0 15 90  6     0
MatAssemblyEnd        62 1.0 1.4898e+00 1.1 0.00e+00 0.0 1.2e+01 5.5e+03 6.9e+01  3  0  0  0  0   3  0  0  0  7     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        62 1.0 1.4510e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot              884 1.0 3.9263e-01 1.2 1.52e+08 1.1 0.0e+00 0.0e+00 4.4e+02  1  1  0  0  1   1  1  0  0 42  1118
VecNorm              946 1.0 1.1125e+00 1.5 2.07e+07 1.1 0.0e+00 0.0e+00 4.7e+02  2  0  0  0  1   2  0  0  0 45    54
VecScale             944 1.0 4.4620e-03 1.1 1.03e+07 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6672
VecCopy               62 1.0 1.6937e-03 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              1070 1.0 3.1120e-02 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               60 1.0 1.3695e-03 1.5 1.31e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2763
VecMAXPY             944 1.0 3.9907e-02 1.1 1.71e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 12398
VecScatterBegin      884 1.0 8.7406e-01142.2 0.00e+00 0.0 2.7e+03 2.2e+04 0.0e+00  1  0 82 10  0   1  0 84 10  0     0
VecScatterEnd        884 1.0 1.0956e+00 3.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
VecNormalize         946 1.0 1.1176e+00 1.5 3.10e+07 1.1 0.0e+00 0.0e+00 4.7e+02  2  0  0  0  1   2  0  0  0 45    80
KSPGMRESOrthog       884 1.0 4.3098e-01 1.2 3.04e+08 1.1 0.0e+00 0.0e+00 4.4e+02  1  1  0  0  1   1  1  0  0 42  2037
KSPSetUp              64 1.0 1.2922e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              62 1.0 1.5717e+01 1.0 2.56e+10 1.1 2.7e+03 2.2e+04 9.2e+02 31100 82 10  1  31100 84 10 87  4596
PCSetUp              124 1.0 8.5216e+00 1.0 1.83e+10 1.1 0.0e+00 0.0e+00 0.0e+00 16 72  0  0  0  16 72  0  0  0  6075
PCSetUpOnBlocks       62 1.0 2.5846e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0  6461
PCApply              946 1.0 1.0686e+01 1.0 2.12e+10 1.1 0.0e+00 0.0e+00 0.0e+00 20 83  0  0  0  20 83  0  0  0  5588
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 3.19481e-06
Average time for zero size MPI_Send(): 1.35104e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:07:18 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           5.261e+01      1.00000   5.261e+01
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                2.642e+10      1.11467   2.487e+10  7.461e+10
Flops/sec:            5.022e+08      1.11467   4.727e+08  1.418e+09
MPI Messages:         1.141e+03      1.04392   1.114e+03  3.343e+03
MPI Message Lengths:  2.505e+08      1.37124   1.868e+05  6.244e+08
MPI Reductions:       6.389e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 5.2442e+01  99.7%  7.4606e+10 100.0%  3.250e+03  97.2%  1.868e+05      100.0%  1.086e+03   1.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult              914 1.0 3.8158e+00 1.0 3.60e+09 1.1 2.7e+03 2.2e+04 0.0e+00  7 14 82 10  0   7 14 84 10  0  2663
MatSolve             978 1.0 2.4732e+00 1.1 3.56e+09 1.1 0.0e+00 0.0e+00 0.0e+00  5 13  0  0  0   5 13  0  0  0  4023
MatLUFactorNum        64 1.0 8.7842e+00 1.0 1.89e+10 1.1 0.0e+00 0.0e+00 0.0e+00 16 72  0  0  0  16 72  0  0  0  6083
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      64 1.0 2.2122e-01 1.6 0.00e+00 0.0 4.8e+02 1.2e+06 6.4e+01  0  0 14 90  0   0  0 15 90  6     0
MatAssemblyEnd        64 1.0 1.5574e+00 1.1 0.00e+00 0.0 1.2e+01 5.5e+03 7.1e+01  3  0  0  0  0   3  0  0  0  7     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        64 1.0 1.4878e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot              914 1.0 4.0474e-01 1.2 1.57e+08 1.1 0.0e+00 0.0e+00 4.6e+02  1  1  0  0  1   1  1  0  0 42  1122
VecNorm              978 1.0 1.1133e+00 1.4 2.14e+07 1.1 0.0e+00 0.0e+00 4.9e+02  2  0  0  0  1   2  0  0  0 45    55
VecScale             976 1.0 4.5872e-03 1.1 1.07e+07 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6710
VecCopy               64 1.0 1.7178e-03 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              1106 1.0 3.1960e-02 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               62 1.0 1.4265e-03 1.5 1.35e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2742
VecMAXPY             976 1.0 4.1107e-02 1.1 1.77e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 12450
VecScatterBegin      914 1.0 8.7434e-01137.8 0.00e+00 0.0 2.7e+03 2.2e+04 0.0e+00  1  0 82 10  0   1  0 84 10  0     0
VecScatterEnd        914 1.0 1.1068e+00 3.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
VecNormalize         978 1.0 1.1186e+00 1.4 3.20e+07 1.1 0.0e+00 0.0e+00 4.9e+02  2  0  0  0  1   2  0  0  0 45    83
KSPGMRESOrthog       914 1.0 4.4204e-01 1.2 3.14e+08 1.1 0.0e+00 0.0e+00 4.6e+02  1  1  0  0  1   1  1  0  0 42  2055
KSPSetUp              66 1.0 1.2922e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              64 1.0 1.6239e+01 1.0 2.64e+10 1.1 2.7e+03 2.2e+04 9.5e+02 31100 82 10  1  31100 84 10 87  4594
PCSetUp              128 1.0 8.8001e+00 1.0 1.89e+10 1.1 0.0e+00 0.0e+00 0.0e+00 16 72  0  0  0  16 72  0  0  0  6072
PCSetUpOnBlocks       64 1.0 2.5846e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0  6461
PCApply              978 1.0 1.1041e+01 1.0 2.19e+10 1.1 0.0e+00 0.0e+00 0.0e+00 21 83  0  0  0  21 83  0  0  0  5589
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 2.38419e-06
Average time for zero size MPI_Send(): 9.53674e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:07:19 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           5.392e+01      1.00000   5.392e+01
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                2.726e+10      1.11468   2.566e+10  7.697e+10
Flops/sec:            5.056e+08      1.11468   4.758e+08  1.428e+09
MPI Messages:         1.178e+03      1.04386   1.150e+03  3.452e+03
MPI Message Lengths:  2.583e+08      1.37123   1.866e+05  6.440e+08
MPI Reductions:       6.595e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 5.3748e+01  99.7%  7.6971e+10 100.0%  3.356e+03  97.2%  1.866e+05      100.0%  1.121e+03   1.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult              944 1.0 3.8999e+00 1.0 3.72e+09 1.1 2.8e+03 2.2e+04 0.0e+00  7 14 82 10  0   7 14 84 10  0  2691
MatSolve            1010 1.0 2.5494e+00 1.1 3.68e+09 1.1 0.0e+00 0.0e+00 0.0e+00  5 13  0  0  0   5 13  0  0  0  4030
MatLUFactorNum        66 1.0 9.0423e+00 1.0 1.95e+10 1.1 0.0e+00 0.0e+00 0.0e+00 16 72  0  0  0  16 72  0  0  0  6094
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      66 1.0 2.2735e-01 1.6 0.00e+00 0.0 5.0e+02 1.2e+06 6.6e+01  0  0 14 90  0   0  0 15 90  6     0
MatAssemblyEnd        66 1.0 1.6058e+00 1.1 0.00e+00 0.0 1.2e+01 5.5e+03 7.3e+01  3  0  0  0  0   3  0  0  0  7     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        66 1.0 1.5229e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot              944 1.0 4.1894e-01 1.2 1.62e+08 1.1 0.0e+00 0.0e+00 4.7e+02  1  1  0  0  1   1  1  0  0 42  1120
VecNorm             1010 1.0 1.1142e+00 1.4 2.21e+07 1.1 0.0e+00 0.0e+00 5.0e+02  2  0  0  0  1   2  0  0  0 45    57
VecScale            1008 1.0 4.7336e-03 1.1 1.10e+07 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6716
VecCopy               66 1.0 1.7407e-03 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              1142 1.0 3.3853e-02 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               64 1.0 1.4725e-03 1.5 1.40e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2742
VecMAXPY            1008 1.0 4.2193e-02 1.1 1.83e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 12533
VecScatterBegin      944 1.0 8.7467e-01134.3 0.00e+00 0.0 2.8e+03 2.2e+04 0.0e+00  1  0 82 10  0   1  0 84 10  0     0
VecScatterEnd        944 1.0 1.1073e+00 3.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
VecNormalize        1010 1.0 1.1195e+00 1.4 3.31e+07 1.1 0.0e+00 0.0e+00 5.0e+02  2  0  0  0  1   2  0  0  0 45    85
KSPGMRESOrthog       944 1.0 4.5731e-01 1.2 3.25e+08 1.1 0.0e+00 0.0e+00 4.7e+02  1  1  0  0  1   1  1  0  0 42  2052
KSPSetUp              68 1.0 1.2922e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              66 1.0 1.6668e+01 1.0 2.73e+10 1.1 2.8e+03 2.2e+04 9.8e+02 31100 82 10  1  31100 84 10 87  4618
PCSetUp              132 1.0 9.0583e+00 1.0 1.95e+10 1.1 0.0e+00 0.0e+00 0.0e+00 16 72  0  0  0  17 72  0  0  0  6083
PCSetUpOnBlocks       66 1.0 2.5846e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0  6461
PCApply             1010 1.0 1.1376e+01 1.0 2.26e+10 1.1 0.0e+00 0.0e+00 0.0e+00 21 83  0  0  0  21 83  0  0  0  5600
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 4.43459e-06
Average time for zero size MPI_Send(): 1.03315e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:07:21 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           5.554e+01      1.00000   5.554e+01
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                2.810e+10      1.11469   2.644e+10  7.933e+10
Flops/sec:            5.058e+08      1.11469   4.761e+08  1.428e+09
MPI Messages:         1.215e+03      1.04381   1.187e+03  3.560e+03
MPI Message Lengths:  2.662e+08      1.37122   1.864e+05  6.636e+08
MPI Reductions:       6.802e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 5.5368e+01  99.7%  7.9335e+10 100.0%  3.461e+03  97.2%  1.864e+05      100.0%  1.156e+03   1.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult              974 1.0 4.0053e+00 1.0 3.84e+09 1.1 2.9e+03 2.2e+04 0.0e+00  7 14 82 10  0   7 14 84 10  0  2703
MatSolve            1042 1.0 2.6427e+00 1.1 3.80e+09 1.1 0.0e+00 0.0e+00 0.0e+00  5 13  0  0  0   5 13  0  0  0  4011
MatLUFactorNum        68 1.0 9.3132e+00 1.0 2.01e+10 1.1 0.0e+00 0.0e+00 0.0e+00 16 72  0  0  0  17 72  0  0  0  6096
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      68 1.0 2.3467e-01 1.6 0.00e+00 0.0 5.1e+02 1.2e+06 6.8e+01  0  0 14 90  0   0  0 15 90  6     0
MatAssemblyEnd        68 1.0 1.6595e+00 1.1 0.00e+00 0.0 1.2e+01 5.5e+03 7.5e+01  3  0  0  0  0   3  0  0  0  6     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        68 1.0 1.5574e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot              974 1.0 4.4064e-01 1.3 1.68e+08 1.1 0.0e+00 0.0e+00 4.9e+02  1  1  0  0  1   1  1  0  0 42  1099
VecNorm             1042 1.0 1.1436e+00 1.4 2.28e+07 1.1 0.0e+00 0.0e+00 5.2e+02  2  0  0  0  1   2  0  0  0 45    57
VecScale            1040 1.0 4.8609e-03 1.1 1.14e+07 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6748
VecCopy               68 1.0 1.7707e-03 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              1178 1.0 3.4280e-02 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               66 1.0 1.5004e-03 1.5 1.44e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2775
VecMAXPY            1040 1.0 4.3499e-02 1.1 1.89e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 12549
VecScatterBegin      974 1.0 8.7498e-01129.6 0.00e+00 0.0 2.9e+03 2.2e+04 0.0e+00  1  0 82 10  0   1  0 84 10  0     0
VecScatterEnd        974 1.0 1.1134e+00 3.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
VecNormalize        1042 1.0 1.1491e+00 1.3 3.41e+07 1.1 0.0e+00 0.0e+00 5.2e+02  2  0  0  0  1   2  0  0  0 45    86
KSPGMRESOrthog       974 1.0 4.8009e-01 1.2 3.35e+08 1.1 0.0e+00 0.0e+00 4.9e+02  1  1  0  0  1   1  1  0  0 42  2018
KSPSetUp              70 1.0 1.3018e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              68 1.0 1.7178e+01 1.0 2.81e+10 1.1 2.9e+03 2.2e+04 1.0e+03 31100 82 10  1  31100 84 10 87  4618
PCSetUp              136 1.0 9.3292e+00 1.0 2.01e+10 1.1 0.0e+00 0.0e+00 0.0e+00 16 72  0  0  0  17 72  0  0  0  6086
PCSetUpOnBlocks       68 1.0 2.5846e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0  6461
PCApply             1042 1.0 1.1741e+01 1.0 2.33e+10 1.1 0.0e+00 0.0e+00 0.0e+00 21 83  0  0  0  21 83  0  0  0  5596
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 2.6226e-06
Average time for zero size MPI_Send(): 1.03315e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:07:22 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           5.719e+01      1.00000   5.719e+01
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                2.893e+10      1.11470   2.723e+10  8.170e+10
Flops/sec:            5.059e+08      1.11470   4.762e+08  1.429e+09
MPI Messages:         1.252e+03      1.04377   1.223e+03  3.668e+03
MPI Message Lengths:  2.740e+08      1.37121   1.862e+05  6.832e+08
MPI Reductions:       7.008e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 5.7008e+01  99.7%  8.1699e+10 100.0%  3.566e+03  97.2%  1.862e+05      100.0%  1.191e+03   1.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult             1004 1.0 4.1190e+00 1.0 3.95e+09 1.1 3.0e+03 2.2e+04 0.0e+00  7 14 82 10  0   7 14 84 10  0  2709
MatSolve            1074 1.0 2.7442e+00 1.1 3.91e+09 1.1 0.0e+00 0.0e+00 0.0e+00  5 13  0  0  0   5 13  0  0  0  3982
MatLUFactorNum        70 1.0 9.5973e+00 1.0 2.07e+10 1.1 0.0e+00 0.0e+00 0.0e+00 16 72  0  0  0  17 72  0  0  0  6090
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      70 1.0 2.4069e-01 1.6 0.00e+00 0.0 5.2e+02 1.2e+06 7.0e+01  0  0 14 90  0   0  0 15 90  6     0
MatAssemblyEnd        70 1.0 1.7100e+00 1.1 0.00e+00 0.0 1.2e+01 5.5e+03 7.7e+01  3  0  0  0  0   3  0  0  0  6     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        70 1.0 1.6455e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             1004 1.0 4.6691e-01 1.3 1.73e+08 1.1 0.0e+00 0.0e+00 5.0e+02  1  1  0  0  1   1  1  0  0 42  1070
VecNorm             1074 1.0 1.1451e+00 1.3 2.35e+07 1.1 0.0e+00 0.0e+00 5.4e+02  2  0  0  0  1   2  0  0  0 45    59
VecScale            1072 1.0 5.0364e-03 1.1 1.17e+07 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6713
VecCopy               70 1.0 1.8246e-03 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              1214 1.0 3.5261e-02 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               68 1.0 1.5733e-03 1.5 1.48e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2726
VecMAXPY            1072 1.0 4.4965e-02 1.1 1.95e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 12518
VecScatterBegin     1004 1.0 8.7533e-01124.6 0.00e+00 0.0 3.0e+03 2.2e+04 0.0e+00  1  0 82 10  0   1  0 84 10  0     0
VecScatterEnd       1004 1.0 1.1171e+00 3.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
VecNormalize        1074 1.0 1.1508e+00 1.3 3.52e+07 1.1 0.0e+00 0.0e+00 5.4e+02  2  0  0  0  1   2  0  0  0 45    88
KSPGMRESOrthog      1004 1.0 5.0976e-01 1.3 3.46e+08 1.1 0.0e+00 0.0e+00 5.0e+02  1  1  0  0  1   1  1  0  0 42  1960
KSPSetUp              72 1.0 1.3018e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              70 1.0 1.7723e+01 1.0 2.89e+10 1.1 3.0e+03 2.2e+04 1.0e+03 31100 82 10  1  31100 84 10 87  4610
PCSetUp              140 1.0 9.6133e+00 1.0 2.07e+10 1.1 0.0e+00 0.0e+00 0.0e+00 17 72  0  0  0  17 72  0  0  0  6080
PCSetUpOnBlocks       70 1.0 2.5846e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0  6461
PCApply             1074 1.0 1.2129e+01 1.0 2.40e+10 1.1 0.0e+00 0.0e+00 0.0e+00 21 83  0  0  0  21 83  0  0  0  5582
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 2.76566e-06
Average time for zero size MPI_Send(): 1.03315e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:07:24 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           5.873e+01      1.00000   5.873e+01
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                2.977e+10      1.11470   2.802e+10  8.406e+10
Flops/sec:            5.069e+08      1.11471   4.771e+08  1.431e+09
MPI Messages:         1.289e+03      1.04372   1.259e+03  3.777e+03
MPI Message Lengths:  2.819e+08      1.37120   1.861e+05  7.028e+08
MPI Reductions:       7.214e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 5.8542e+01  99.7%  8.4063e+10 100.0%  3.672e+03  97.2%  1.861e+05      100.0%  1.226e+03   1.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult             1034 1.0 4.2101e+00 1.0 4.07e+09 1.1 3.1e+03 2.2e+04 0.0e+00  7 14 82 10  0   7 14 84 10  0  2730
MatSolve            1106 1.0 2.8224e+00 1.1 4.03e+09 1.1 0.0e+00 0.0e+00 0.0e+00  5 13  0  0  0   5 13  0  0  0  3986
MatLUFactorNum        72 1.0 9.8551e+00 1.0 2.13e+10 1.1 0.0e+00 0.0e+00 0.0e+00 17 72  0  0  0  17 72  0  0  0  6100
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      72 1.0 2.4674e-01 1.6 0.00e+00 0.0 5.4e+02 1.2e+06 7.2e+01  0  0 14 90  0   0  0 15 90  6     0
MatAssemblyEnd        72 1.0 1.7561e+00 1.1 0.00e+00 0.0 1.2e+01 5.5e+03 7.9e+01  3  0  0  0  0   3  0  0  0  6     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        72 1.0 1.6838e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             1034 1.0 4.7440e-01 1.3 1.78e+08 1.1 0.0e+00 0.0e+00 5.2e+02  1  1  0  0  1   1  1  0  0 42  1085
VecNorm             1106 1.0 1.1826e+00 1.3 2.42e+07 1.1 0.0e+00 0.0e+00 5.5e+02  2  0  0  0  1   2  0  0  0 45    59
VecScale            1104 1.0 5.1825e-03 1.1 1.21e+07 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6718
VecCopy               72 1.0 1.8537e-03 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              1250 1.0 3.5594e-02 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               70 1.0 1.5953e-03 1.5 1.53e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2768
VecMAXPY            1104 1.0 4.6146e-02 1.1 2.01e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 12567
VecScatterBegin     1034 1.0 8.7558e-01121.4 0.00e+00 0.0 3.1e+03 2.2e+04 0.0e+00  1  0 82 10  0   1  0 84 10  0     0
VecScatterEnd       1034 1.0 1.1237e+00 3.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
VecNormalize        1106 1.0 1.1885e+00 1.3 3.62e+07 1.1 0.0e+00 0.0e+00 5.5e+02  2  0  0  0  1   2  0  0  0 45    88
KSPGMRESOrthog      1034 1.0 5.1847e-01 1.3 3.56e+08 1.1 0.0e+00 0.0e+00 5.2e+02  1  1  0  0  1   1  1  0  0 42  1985
KSPSetUp              74 1.0 1.3018e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              72 1.0 1.8175e+01 1.0 2.98e+10 1.1 3.1e+03 2.2e+04 1.1e+03 31100 82 10  1  31100 84 10 87  4625
PCSetUp              144 1.0 9.8711e+00 1.0 2.13e+10 1.1 0.0e+00 0.0e+00 0.0e+00 17 72  0  0  0  17 72  0  0  0  6090
PCSetUpOnBlocks       72 1.0 2.5846e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0  6461
PCApply             1106 1.0 1.2466e+01 1.0 2.47e+10 1.1 0.0e+00 0.0e+00 0.0e+00 21 83  0  0  0  21 83  0  0  0  5591
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 2.81334e-06
Average time for zero size MPI_Send(): 1.03315e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:07:25 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           6.029e+01      1.00000   6.029e+01
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                3.061e+10      1.11471   2.881e+10  8.643e+10
Flops/sec:            5.077e+08      1.11471   4.778e+08  1.434e+09
MPI Messages:         1.326e+03      1.04368   1.295e+03  3.886e+03
MPI Message Lengths:  2.898e+08      1.37119   1.859e+05  7.224e+08
MPI Reductions:       7.420e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 6.0096e+01  99.7%  8.6427e+10 100.0%  3.778e+03  97.2%  1.859e+05      100.0%  1.261e+03   1.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult             1064 1.0 4.2967e+00 1.0 4.19e+09 1.1 3.2e+03 2.2e+04 0.0e+00  7 14 82 10  0   7 14 85 10  0  2753
MatSolve            1138 1.0 2.9026e+00 1.1 4.15e+09 1.1 0.0e+00 0.0e+00 0.0e+00  5 13  0  0  0   5 13  0  0  0  3989
MatLUFactorNum        74 1.0 1.0125e+01 1.0 2.18e+10 1.1 0.0e+00 0.0e+00 0.0e+00 17 71  0  0  0  17 71  0  0  0  6102
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      74 1.0 2.5305e-01 1.6 0.00e+00 0.0 5.6e+02 1.2e+06 7.4e+01  0  0 14 90  0   0  0 15 90  6     0
MatAssemblyEnd        74 1.0 1.8077e+00 1.1 0.00e+00 0.0 1.2e+01 5.5e+03 8.1e+01  3  0  0  0  0   3  0  0  0  6     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        74 1.0 1.7255e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             1064 1.0 4.8218e-01 1.3 1.83e+08 1.1 0.0e+00 0.0e+00 5.3e+02  1  1  0  0  1   1  1  0  0 42  1099
VecNorm             1138 1.0 1.2164e+00 1.3 2.49e+07 1.1 0.0e+00 0.0e+00 5.7e+02  2  0  0  0  1   2  0  0  0 45    59
VecScale            1136 1.0 5.3194e-03 1.1 1.24e+07 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6735
VecCopy               74 1.0 1.9026e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              1286 1.0 3.6517e-02 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               72 1.0 1.6222e-03 1.5 1.57e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2800
VecMAXPY            1136 1.0 4.7399e-02 1.0 2.07e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 12594
VecScatterBegin     1064 1.0 8.7587e-01118.4 0.00e+00 0.0 3.2e+03 2.2e+04 0.0e+00  1  0 82 10  0   1  0 85 10  0     0
VecScatterEnd       1064 1.0 1.1242e+00 3.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
VecNormalize        1138 1.0 1.2224e+00 1.3 3.73e+07 1.1 0.0e+00 0.0e+00 5.7e+02  2  0  0  0  1   2  0  0  0 45    88
KSPGMRESOrthog      1064 1.0 5.2758e-01 1.3 3.67e+08 1.1 0.0e+00 0.0e+00 5.3e+02  1  1  0  0  1   1  1  0  0 42  2008
KSPSetUp              76 1.0 1.3018e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              74 1.0 1.8654e+01 1.0 3.06e+10 1.1 3.2e+03 2.2e+04 1.1e+03 31100 82 10  1  31100 85 10 87  4633
PCSetUp              148 1.0 1.0141e+01 1.0 2.18e+10 1.1 0.0e+00 0.0e+00 0.0e+00 17 71  0  0  0  17 71  0  0  0  6092
PCSetUpOnBlocks       74 1.0 2.5846e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0  6461
PCApply             1138 1.0 1.2818e+01 1.0 2.54e+10 1.1 0.0e+00 0.0e+00 0.0e+00 21 83  0  0  0  21 83  0  0  0  5593
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 1.38283e-06
Average time for zero size MPI_Send(): 6.35783e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:07:27 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           6.198e+01      1.00000   6.198e+01
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                3.145e+10      1.11472   2.960e+10  8.879e+10
Flops/sec:            5.074e+08      1.11472   4.776e+08  1.433e+09
MPI Messages:         1.363e+03      1.04364   1.331e+03  3.994e+03
MPI Message Lengths:  2.976e+08      1.37118   1.858e+05  7.420e+08
MPI Reductions:       7.626e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 6.1778e+01  99.7%  8.8791e+10 100.0%  3.883e+03  97.2%  1.858e+05      100.0%  1.296e+03   1.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult             1094 1.0 4.4017e+00 1.0 4.31e+09 1.1 3.3e+03 2.2e+04 0.0e+00  7 14 82 10  0   7 14 85 10  0  2763
MatSolve            1170 1.0 2.9922e+00 1.1 4.26e+09 1.1 0.0e+00 0.0e+00 0.0e+00  5 13  0  0  0   5 13  0  0  0  3978
MatLUFactorNum        76 1.0 1.0396e+01 1.0 2.24e+10 1.1 0.0e+00 0.0e+00 0.0e+00 17 71  0  0  0  17 71  0  0  0  6104
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      76 1.0 2.5927e-01 1.6 0.00e+00 0.0 5.7e+02 1.2e+06 7.6e+01  0  0 14 90  0   0  0 15 90  6     0
MatAssemblyEnd        76 1.0 1.8600e+00 1.1 0.00e+00 0.0 1.2e+01 5.5e+03 8.3e+01  3  0  0  0  0   3  0  0  0  6     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        76 1.0 1.7697e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             1094 1.0 5.0715e-01 1.4 1.89e+08 1.1 0.0e+00 0.0e+00 5.5e+02  1  1  0  0  1   1  1  0  0 42  1075
VecNorm             1170 1.0 1.2174e+00 1.2 2.55e+07 1.1 0.0e+00 0.0e+00 5.8e+02  2  0  0  0  1   2  0  0  0 45    61
VecScale            1168 1.0 5.4977e-03 1.1 1.28e+07 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6700
VecCopy               76 1.0 1.9536e-03 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              1322 1.0 3.9040e-02 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               74 1.0 1.6482e-03 1.5 1.62e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2832
VecMAXPY            1168 1.0 4.8801e-02 1.0 2.13e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 12581
VecScatterBegin     1094 1.0 8.7620e-01115.1 0.00e+00 0.0 3.3e+03 2.2e+04 0.0e+00  1  0 82 10  0   1  0 85 10  0     0
VecScatterEnd       1094 1.0 1.1324e+00 3.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
VecNormalize        1170 1.0 1.2236e+00 1.2 3.83e+07 1.1 0.0e+00 0.0e+00 5.8e+02  2  0  0  0  1   2  0  0  0 45    90
KSPGMRESOrthog      1094 1.0 5.5385e-01 1.3 3.77e+08 1.1 0.0e+00 0.0e+00 5.5e+02  1  1  0  0  1   1  1  0  0 42  1968
KSPSetUp              78 1.0 1.3113e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              76 1.0 1.9207e+01 1.0 3.14e+10 1.1 3.3e+03 2.2e+04 1.1e+03 31100 82 10  1  31100 85 10 87  4623
PCSetUp              152 1.0 1.0412e+01 1.0 2.24e+10 1.1 0.0e+00 0.0e+00 0.0e+00 17 71  0  0  0  17 71  0  0  0  6094
PCSetUpOnBlocks       76 1.0 2.5846e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0  6461
PCApply             1170 1.0 1.3179e+01 1.0 2.61e+10 1.1 0.0e+00 0.0e+00 0.0e+00 21 83  0  0  0  21 83  0  0  0  5591
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 2.19345e-06
Average time for zero size MPI_Send(): 1.35104e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:07:29 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           6.370e+01      1.00000   6.370e+01
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                3.228e+10      1.11472   3.039e+10  9.116e+10
Flops/sec:            5.068e+08      1.11473   4.770e+08  1.431e+09
MPI Messages:         1.400e+03      1.04361   1.368e+03  4.102e+03
MPI Message Lengths:  3.055e+08      1.37117   1.857e+05  7.616e+08
MPI Reductions:       7.832e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 6.3498e+01  99.7%  9.1156e+10 100.0%  3.988e+03  97.2%  1.857e+05      100.0%  1.331e+03   1.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult             1124 1.0 4.4998e+00 1.0 4.43e+09 1.1 3.4e+03 2.2e+04 0.0e+00  7 14 82 10  0   7 14 85 10  0  2777
MatSolve            1202 1.0 3.0764e+00 1.1 4.38e+09 1.1 0.0e+00 0.0e+00 0.0e+00  5 13  0  0  0   5 13  0  0  0  3975
MatLUFactorNum        78 1.0 1.0748e+01 1.0 2.30e+10 1.1 0.0e+00 0.0e+00 0.0e+00 17 71  0  0  0  17 71  0  0  0  6059
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      78 1.0 2.6539e-01 1.6 0.00e+00 0.0 5.8e+02 1.2e+06 7.8e+01  0  0 14 90  0   0  0 15 90  6     0
MatAssemblyEnd        78 1.0 1.9235e+00 1.1 0.00e+00 0.0 1.2e+01 5.5e+03 8.5e+01  3  0  0  0  0   3  0  0  0  6     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        78 1.0 1.8134e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             1124 1.0 5.2693e-01 1.4 1.94e+08 1.1 0.0e+00 0.0e+00 5.6e+02  1  1  0  0  1   1  1  0  0 42  1063
VecNorm             1202 1.0 1.3197e+00 1.3 2.62e+07 1.1 0.0e+00 0.0e+00 6.0e+02  2  0  0  0  1   2  0  0  0 45    57
VecScale            1200 1.0 5.6708e-03 1.1 1.31e+07 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6674
VecCopy               78 1.0 1.9815e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              1358 1.0 4.0143e-02 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               76 1.0 1.7142e-03 1.5 1.66e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2796
VecMAXPY            1200 1.0 5.0141e-02 1.0 2.18e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 12585
VecScatterBegin     1124 1.0 8.7651e-01112.2 0.00e+00 0.0 3.4e+03 2.2e+04 0.0e+00  1  0 82 10  0   1  0 85 10  0     0
VecScatterEnd       1124 1.0 1.1402e+00 3.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
VecNormalize        1202 1.0 1.3260e+00 1.3 3.94e+07 1.1 0.0e+00 0.0e+00 6.0e+02  2  0  0  0  1   2  0  0  0 45    86
KSPGMRESOrthog      1124 1.0 5.7495e-01 1.3 3.88e+08 1.1 0.0e+00 0.0e+00 5.6e+02  1  1  0  0  1   1  1  0  0 42  1948
KSPSetUp              80 1.0 1.3113e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              78 1.0 1.9755e+01 1.0 3.23e+10 1.1 3.4e+03 2.2e+04 1.2e+03 31100 82 10  1  31100 85 10 87  4614
PCSetUp              156 1.0 1.0764e+01 1.0 2.30e+10 1.1 0.0e+00 0.0e+00 0.0e+00 17 71  0  0  0  17 71  0  0  0  6050
PCSetUpOnBlocks       78 1.0 2.5847e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0  6461
PCApply             1202 1.0 1.3616e+01 1.0 2.68e+10 1.1 0.0e+00 0.0e+00 0.0e+00 21 83  0  0  0  21 83  0  0  0  5558
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 3.00407e-06
Average time for zero size MPI_Send(): 1.03315e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:07:30 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           6.543e+01      1.00000   6.543e+01
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                3.312e+10      1.11473   3.117e+10  9.352e+10
Flops/sec:            5.062e+08      1.11473   4.764e+08  1.429e+09
MPI Messages:         1.437e+03      1.04357   1.404e+03  4.211e+03
MPI Message Lengths:  3.134e+08      1.37117   1.855e+05  7.812e+08
MPI Reductions:       8.038e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 6.5224e+01  99.7%  9.3520e+10 100.0%  4.094e+03  97.2%  1.855e+05      100.0%  1.366e+03   1.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult             1154 1.0 4.6160e+00 1.0 4.54e+09 1.1 3.5e+03 2.2e+04 0.0e+00  7 14 82 10  0   7 14 85 10  0  2779
MatSolve            1234 1.0 3.1685e+00 1.1 4.50e+09 1.1 0.0e+00 0.0e+00 0.0e+00  5 13  0  0  0   5 13  0  0  0  3962
MatLUFactorNum        80 1.0 1.0999e+01 1.0 2.36e+10 1.1 0.0e+00 0.0e+00 0.0e+00 17 71  0  0  0  17 71  0  0  0  6073
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      80 1.0 2.7557e-01 1.6 0.00e+00 0.0 6.0e+02 1.2e+06 8.0e+01  0  0 14 90  0   0  0 15 90  6     0
MatAssemblyEnd        80 1.0 1.9821e+00 1.1 0.00e+00 0.0 1.2e+01 5.5e+03 8.7e+01  3  0  0  0  0   3  0  0  0  6     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        80 1.0 1.8498e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             1154 1.0 5.5471e-01 1.4 1.99e+08 1.1 0.0e+00 0.0e+00 5.8e+02  1  1  0  0  1   1  1  0  0 42  1037
VecNorm             1234 1.0 1.3208e+00 1.2 2.69e+07 1.1 0.0e+00 0.0e+00 6.2e+02  2  0  0  0  1   2  0  0  0 45    59
VecScale            1232 1.0 5.8548e-03 1.1 1.35e+07 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6636
VecCopy               80 1.0 2.0375e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              1394 1.0 4.1891e-02 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               78 1.0 1.7421e-03 1.5 1.70e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2824
VecMAXPY            1232 1.0 5.1684e-02 1.0 2.24e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 12538
VecScatterBegin     1154 1.0 8.7688e-01109.0 0.00e+00 0.0 3.5e+03 2.2e+04 0.0e+00  1  0 82 10  0   1  0 85 10  0     0
VecScatterEnd       1154 1.0 1.1532e+00 3.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
VecNormalize        1234 1.0 1.3273e+00 1.2 4.04e+07 1.1 0.0e+00 0.0e+00 6.2e+02  2  0  0  0  1   2  0  0  0 45    88
KSPGMRESOrthog      1154 1.0 6.0410e-01 1.4 3.98e+08 1.1 0.0e+00 0.0e+00 5.8e+02  1  1  0  0  1   1  1  0  0 42  1904
KSPSetUp              82 1.0 1.3208e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              80 1.0 2.0281e+01 1.0 3.31e+10 1.1 3.5e+03 2.2e+04 1.2e+03 31100 82 10  1  31100 85 10 87  4611
PCSetUp              160 1.0 1.1015e+01 1.0 2.36e+10 1.1 0.0e+00 0.0e+00 0.0e+00 17 71  0  0  0  17 71  0  0  0  6064
PCSetUpOnBlocks       80 1.0 2.5847e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0  6461
PCApply             1234 1.0 1.3960e+01 1.0 2.75e+10 1.1 0.0e+00 0.0e+00 0.0e+00 21 83  0  0  0  21 83  0  0  0  5564
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 1.19209e-07
Average time for MPI_Barrier(): 2.6226e-06
Average time for zero size MPI_Send(): 1.35104e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:07:32 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           6.675e+01      1.00000   6.675e+01
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                3.396e+10      1.11473   3.196e+10  9.588e+10
Flops/sec:            5.088e+08      1.11474   4.788e+08  1.437e+09
MPI Messages:         1.474e+03      1.04354   1.440e+03  4.320e+03
MPI Message Lengths:  3.212e+08      1.37116   1.854e+05  8.008e+08
MPI Reductions:       8.244e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 6.6533e+01  99.7%  9.5884e+10 100.0%  4.200e+03  97.2%  1.854e+05      100.0%  1.401e+03   1.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult             1184 1.0 4.7014e+00 1.0 4.66e+09 1.1 3.6e+03 2.2e+04 0.0e+00  7 14 82 10  0   7 14 85 10  0  2799
MatSolve            1266 1.0 3.2443e+00 1.1 4.61e+09 1.1 0.0e+00 0.0e+00 0.0e+00  5 13  0  0  0   5 13  0  0  0  3970
MatLUFactorNum        82 1.0 1.1245e+01 1.0 2.42e+10 1.1 0.0e+00 0.0e+00 0.0e+00 17 71  0  0  0  17 71  0  0  0  6088
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      82 1.0 2.8029e-01 1.6 0.00e+00 0.0 6.2e+02 1.2e+06 8.2e+01  0  0 14 90  0   0  0 15 90  6     0
MatAssemblyEnd        82 1.0 2.0261e+00 1.1 0.00e+00 0.0 1.2e+01 5.5e+03 8.9e+01  3  0  0  0  0   3  0  0  0  6     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        82 1.0 1.8971e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             1184 1.0 5.6088e-01 1.4 2.04e+08 1.1 0.0e+00 0.0e+00 5.9e+02  1  1  0  0  1   1  1  0  0 42  1053
VecNorm             1266 1.0 1.3482e+00 1.3 2.76e+07 1.1 0.0e+00 0.0e+00 6.3e+02  2  0  0  0  1   2  0  0  0 45    59
VecScale            1264 1.0 5.9721e-03 1.1 1.38e+07 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6675
VecCopy               82 1.0 2.0635e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              1430 1.0 4.2253e-02 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               80 1.0 1.7841e-03 1.5 1.75e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2828
VecMAXPY            1264 1.0 5.2804e-02 1.0 2.30e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 12595
VecScatterBegin     1184 1.0 8.7714e-01106.8 0.00e+00 0.0 3.6e+03 2.2e+04 0.0e+00  1  0 82 10  0   1  0 85 10  0     0
VecScatterEnd       1184 1.0 1.1568e+00 3.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
VecNormalize        1266 1.0 1.3552e+00 1.3 4.14e+07 1.1 0.0e+00 0.0e+00 6.3e+02  2  0  0  0  1   2  0  0  0 45    88
KSPGMRESOrthog      1184 1.0 6.1133e-01 1.4 4.09e+08 1.1 0.0e+00 0.0e+00 5.9e+02  1  1  0  0  1   1  1  0  0 42  1931
KSPSetUp              84 1.0 1.3208e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              82 1.0 2.0692e+01 1.0 3.40e+10 1.1 3.6e+03 2.2e+04 1.2e+03 31100 82 10  1  31100 85 10 87  4634
PCSetUp              164 1.0 1.1261e+01 1.0 2.42e+10 1.1 0.0e+00 0.0e+00 0.0e+00 17 71  0  0  0  17 71  0  0  0  6080
PCSetUpOnBlocks       82 1.0 2.5847e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0  6461
PCApply             1266 1.0 1.4283e+01 1.0 2.82e+10 1.1 0.0e+00 0.0e+00 0.0e+00 21 83  0  0  0  21 83  0  0  0  5578
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 2.57492e-06
Average time for zero size MPI_Send(): 9.53674e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:07:33 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           6.821e+01      1.00000   6.821e+01
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                3.480e+10      1.11474   3.275e+10  9.825e+10
Flops/sec:            5.101e+08      1.11474   4.801e+08  1.440e+09
MPI Messages:         1.511e+03      1.04351   1.476e+03  4.428e+03
MPI Message Lengths:  3.291e+08      1.37115   1.853e+05  8.205e+08
MPI Reductions:       8.450e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 6.7993e+01  99.7%  9.8248e+10 100.0%  4.305e+03  97.2%  1.853e+05      100.0%  1.436e+03   1.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult             1214 1.0 4.7814e+00 1.0 4.78e+09 1.1 3.6e+03 2.2e+04 0.0e+00  7 14 82 10  0   7 14 85 10  0  2822
MatSolve            1298 1.0 3.3192e+00 1.1 4.73e+09 1.1 0.0e+00 0.0e+00 0.0e+00  5 13  0  0  0   5 13  0  0  0  3978
MatLUFactorNum        84 1.0 1.1485e+01 1.0 2.48e+10 1.1 0.0e+00 0.0e+00 0.0e+00 17 71  0  0  0  17 71  0  0  0  6106
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      84 1.0 2.8658e-01 1.6 0.00e+00 0.0 6.3e+02 1.2e+06 8.4e+01  0  0 14 90  0   0  0 15 90  6     0
MatAssemblyEnd        84 1.0 2.0730e+00 1.1 0.00e+00 0.0 1.2e+01 5.5e+03 9.1e+01  3  0  0  0  0   3  0  0  0  6     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        84 1.0 1.9440e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             1214 1.0 5.6859e-01 1.4 2.10e+08 1.1 0.0e+00 0.0e+00 6.1e+02  1  1  0  0  1   1  1  0  0 42  1065
VecNorm             1298 1.0 1.3782e+00 1.3 2.83e+07 1.1 0.0e+00 0.0e+00 6.5e+02  2  0  0  0  1   2  0  0  0 45    59
VecScale            1296 1.0 6.1018e-03 1.1 1.42e+07 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6698
VecCopy               84 1.0 2.2075e-03 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              1466 1.0 4.3398e-02 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               82 1.0 1.8051e-03 1.5 1.79e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2865
VecMAXPY            1296 1.0 5.3892e-02 1.0 2.36e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 12657
VecScatterBegin     1214 1.0 8.7737e-01104.5 0.00e+00 0.0 3.6e+03 2.2e+04 0.0e+00  1  0 82 10  0   1  0 85 10  0     0
VecScatterEnd       1214 1.0 1.1573e+00 2.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
VecNormalize        1298 1.0 1.3851e+00 1.3 4.25e+07 1.1 0.0e+00 0.0e+00 6.5e+02  2  0  0  0  1   2  0  0  0 45    89
KSPGMRESOrthog      1214 1.0 6.2010e-01 1.4 4.19e+08 1.1 0.0e+00 0.0e+00 6.1e+02  1  1  0  0  1   1  1  0  0 42  1953
KSPSetUp              86 1.0 1.3304e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              84 1.0 2.1117e+01 1.0 3.48e+10 1.1 3.6e+03 2.2e+04 1.3e+03 31100 82 10  1  31100 85 10 87  4653
PCSetUp              168 1.0 1.1502e+01 1.0 2.48e+10 1.1 0.0e+00 0.0e+00 0.0e+00 17 71  0  0  0  17 71  0  0  0  6098
PCSetUpOnBlocks       84 1.0 2.5847e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0  6460
PCApply             1298 1.0 1.4599e+01 1.0 2.89e+10 1.1 0.0e+00 0.0e+00 0.0e+00 21 83  0  0  0  21 83  0  0  0  5594
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 2.38419e-06
Average time for zero size MPI_Send(): 1.03315e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:07:34 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           6.934e+01      1.00000   6.934e+01
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                3.563e+10      1.11475   3.354e+10  1.006e+11
Flops/sec:            5.139e+08      1.11475   4.837e+08  1.451e+09
MPI Messages:         1.548e+03      1.04348   1.512e+03  4.536e+03
MPI Message Lengths:  3.369e+08      1.37114   1.852e+05  8.401e+08
MPI Reductions:       8.656e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 6.9116e+01  99.7%  1.0061e+11 100.0%  4.410e+03  97.2%  1.852e+05      100.0%  1.471e+03   1.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult             1244 1.0 4.8615e+00 1.0 4.90e+09 1.1 3.7e+03 2.2e+04 0.0e+00  7 14 82 10  0   7 14 85 10  0  2844
MatSolve            1330 1.0 3.3931e+00 1.1 4.85e+09 1.1 0.0e+00 0.0e+00 0.0e+00  5 13  0  0  0   5 13  0  0  0  3988
MatLUFactorNum        86 1.0 1.1716e+01 1.0 2.54e+10 1.1 0.0e+00 0.0e+00 0.0e+00 17 71  0  0  0  17 71  0  0  0  6128
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      86 1.0 2.9268e-01 1.6 0.00e+00 0.0 6.4e+02 1.2e+06 8.6e+01  0  0 14 90  0   0  0 15 90  6     0
MatAssemblyEnd        86 1.0 2.1047e+00 1.1 0.00e+00 0.0 1.2e+01 5.5e+03 9.3e+01  3  0  0  0  0   3  0  0  0  6     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        86 1.0 1.9758e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             1244 1.0 5.7476e-01 1.4 2.15e+08 1.1 0.0e+00 0.0e+00 6.2e+02  1  1  0  0  1   1  1  0  0 42  1080
VecNorm             1330 1.0 1.3959e+00 1.3 2.90e+07 1.1 0.0e+00 0.0e+00 6.6e+02  2  0  0  0  1   2  0  0  0 45    60
VecScale            1328 1.0 6.2330e-03 1.1 1.45e+07 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6719
VecCopy               86 1.0 2.2504e-03 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              1502 1.0 4.4583e-02 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               84 1.0 1.8251e-03 1.4 1.83e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2903
VecMAXPY            1328 1.0 5.5000e-02 1.0 2.42e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 12711
VecScatterBegin     1244 1.0 8.7761e-01102.5 0.00e+00 0.0 3.7e+03 2.2e+04 0.0e+00  1  0 82 10  0   1  0 85 10  0     0
VecScatterEnd       1244 1.0 1.1578e+00 2.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
VecNormalize        1330 1.0 1.4029e+00 1.3 4.35e+07 1.1 0.0e+00 0.0e+00 6.6e+02  2  0  0  0  1   2  0  0  0 45    90
KSPGMRESOrthog      1244 1.0 6.2738e-01 1.4 4.30e+08 1.1 0.0e+00 0.0e+00 6.2e+02  1  1  0  0  1   1  1  0  0 42  1979
KSPSetUp              88 1.0 1.3304e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              86 1.0 2.1505e+01 1.0 3.56e+10 1.1 3.7e+03 2.2e+04 1.3e+03 31100 82 10  1  31100 85 10 87  4679
PCSetUp              172 1.0 1.1733e+01 1.0 2.54e+10 1.1 0.0e+00 0.0e+00 0.0e+00 17 71  0  0  0  17 71  0  0  0  6120
PCSetUpOnBlocks       86 1.0 2.5847e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0  6460
PCApply             1330 1.0 1.4905e+01 1.0 2.96e+10 1.1 0.0e+00 0.0e+00 0.0e+00 21 83  0  0  0  21 83  0  0  0  5613
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 2.38419e-06
Average time for zero size MPI_Send(): 9.53674e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:07:36 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           7.083e+01      1.00000   7.083e+01
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                3.647e+10      1.11475   3.433e+10  1.030e+11
Flops/sec:            5.149e+08      1.11475   4.846e+08  1.454e+09
MPI Messages:         1.585e+03      1.04345   1.548e+03  4.645e+03
MPI Message Lengths:  3.448e+08      1.37114   1.851e+05  8.597e+08
MPI Reductions:       8.862e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 7.0598e+01  99.7%  1.0298e+11 100.0%  4.516e+03  97.2%  1.851e+05      100.0%  1.506e+03   1.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult             1274 1.0 4.9542e+00 1.0 5.02e+09 1.1 3.8e+03 2.2e+04 0.0e+00  7 14 82 10  0   7 14 85 10  0  2858
MatSolve            1362 1.0 3.4775e+00 1.1 4.96e+09 1.1 0.0e+00 0.0e+00 0.0e+00  5 13  0  0  0   5 13  0  0  0  3984
MatLUFactorNum        88 1.0 1.1960e+01 1.0 2.60e+10 1.1 0.0e+00 0.0e+00 0.0e+00 17 71  0  0  0  17 71  0  0  0  6143
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      88 1.0 2.9897e-01 1.6 0.00e+00 0.0 6.6e+02 1.2e+06 8.8e+01  0  0 14 90  0   0  0 15 90  6     0
MatAssemblyEnd        88 1.0 2.1400e+00 1.1 0.00e+00 0.0 1.2e+01 5.5e+03 9.5e+01  3  0  0  0  0   3  0  0  0  6     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        88 1.0 2.0185e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             1274 1.0 5.9741e-01 1.5 2.20e+08 1.1 0.0e+00 0.0e+00 6.4e+02  1  1  0  0  1   1  1  0  0 42  1064
VecNorm             1362 1.0 1.5100e+00 1.3 2.97e+07 1.1 0.0e+00 0.0e+00 6.8e+02  2  0  0  0  1   2  0  0  0 45    57
VecScale            1360 1.0 6.3710e-03 1.1 1.48e+07 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6732
VecCopy               88 1.0 2.2743e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              1538 1.0 4.4984e-02 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               86 1.0 1.8761e-03 1.5 1.88e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2891
VecMAXPY            1360 1.0 5.6571e-02 1.0 2.48e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 12659
VecScatterBegin     1274 1.0 8.7787e-01100.1 0.00e+00 0.0 3.8e+03 2.2e+04 0.0e+00  1  0 82 10  0   1  0 85 10  0     0
VecScatterEnd       1274 1.0 1.1590e+00 2.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
VecNormalize        1362 1.0 1.5171e+00 1.3 4.46e+07 1.1 0.0e+00 0.0e+00 6.8e+02  2  0  0  0  1   2  0  0  0 45    85
KSPGMRESOrthog      1274 1.0 6.5126e-01 1.4 4.40e+08 1.1 0.0e+00 0.0e+00 6.4e+02  1  1  0  0  1   1  1  0  0 42  1952
KSPSetUp              90 1.0 1.3304e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              88 1.0 2.2029e+01 1.0 3.65e+10 1.1 3.8e+03 2.2e+04 1.3e+03 31100 82 10  1  31100 85 10 88  4675
PCSetUp              176 1.0 1.1976e+01 1.0 2.60e+10 1.1 0.0e+00 0.0e+00 0.0e+00 17 71  0  0  0  17 71  0  0  0  6135
PCSetUpOnBlocks       88 1.0 2.5847e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0  6460
PCApply             1362 1.0 1.5234e+01 1.0 3.03e+10 1.1 0.0e+00 0.0e+00 0.0e+00 21 83  0  0  0  21 83  0  0  0  5623
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 2.95639e-06
Average time for zero size MPI_Send(): 1.27157e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:07:37 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           7.229e+01      1.00000   7.229e+01
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                3.729e+10      1.11475   3.510e+10  1.053e+11
Flops/sec:            5.158e+08      1.11475   4.855e+08  1.457e+09
MPI Messages:         1.620e+03      1.04348   1.582e+03  4.748e+03
MPI Message Lengths:  3.526e+08      1.37114   1.852e+05  8.791e+08
MPI Reductions:       9.068e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 7.2055e+01  99.7%  1.0529e+11 100.0%  4.616e+03  97.2%  1.852e+05      100.0%  1.539e+03   1.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult             1302 1.0 5.0366e+00 1.0 5.13e+09 1.1 3.9e+03 2.2e+04 0.0e+00  7 14 82 10  0   7 14 85 10  0  2874
MatSolve            1392 1.0 3.5484e+00 1.1 5.07e+09 1.1 0.0e+00 0.0e+00 0.0e+00  5 13  0  0  0   5 13  0  0  0  3991
MatLUFactorNum        90 1.0 1.2200e+01 1.0 2.66e+10 1.1 0.0e+00 0.0e+00 0.0e+00 17 71  0  0  0  17 71  0  0  0  6159
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      90 1.0 3.0534e-01 1.6 0.00e+00 0.0 6.8e+02 1.2e+06 9.0e+01  0  0 14 90  0   0  0 15 90  6     0
MatAssemblyEnd        90 1.0 2.1744e+00 1.1 0.00e+00 0.0 1.2e+01 5.5e+03 9.7e+01  3  0  0  0  0   3  0  0  0  6     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        90 1.0 2.1060e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             1302 1.0 6.0331e-01 1.5 2.25e+08 1.1 0.0e+00 0.0e+00 6.5e+02  1  1  0  0  1   1  1  0  0 42  1076
VecNorm             1392 1.0 1.5549e+00 1.3 3.04e+07 1.1 0.0e+00 0.0e+00 7.0e+02  2  0  0  0  1   2  0  0  0 45    56
VecScale            1390 1.0 6.5043e-03 1.0 1.52e+07 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6740
VecCopy               90 1.0 2.2993e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              1572 1.0 4.5295e-02 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               88 1.0 1.8992e-03 1.5 1.92e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2923
VecMAXPY            1390 1.0 5.7566e-02 1.0 2.53e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 12701
VecScatterBegin     1302 1.0 8.7809e-0198.2 0.00e+00 0.0 3.9e+03 2.2e+04 0.0e+00  1  0 82 10  0   1  0 85 10  0     0
VecScatterEnd       1302 1.0 1.1644e+00 2.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
VecNormalize        1392 1.0 1.5622e+00 1.3 4.56e+07 1.1 0.0e+00 0.0e+00 7.0e+02  2  0  0  0  1   2  0  0  0 45    84
KSPGMRESOrthog      1302 1.0 6.5812e-01 1.4 4.49e+08 1.1 0.0e+00 0.0e+00 6.5e+02  1  1  0  0  1   1  1  0  0 42  1972
KSPSetUp              92 1.0 1.3304e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              90 1.0 2.2452e+01 1.0 3.73e+10 1.1 3.9e+03 2.2e+04 1.3e+03 31100 82 10  1  31100 85 10 88  4690
PCSetUp              180 1.0 1.2216e+01 1.0 2.66e+10 1.1 0.0e+00 0.0e+00 0.0e+00 17 71  0  0  0  17 71  0  0  0  6151
PCSetUpOnBlocks       90 1.0 2.5847e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0  6460
PCApply             1392 1.0 1.5545e+01 1.0 3.10e+10 1.1 0.0e+00 0.0e+00 0.0e+00 21 83  0  0  0  21 83  0  0  0  5638
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 2.6226e-06
Average time for zero size MPI_Send(): 1.03315e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:07:39 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           7.368e+01      1.00000   7.368e+01
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                3.813e+10      1.11475   3.589e+10  1.077e+11
Flops/sec:            5.175e+08      1.11475   4.870e+08  1.461e+09
MPI Messages:         1.657e+03      1.04345   1.619e+03  4.856e+03
MPI Message Lengths:  3.605e+08      1.37114   1.851e+05  8.987e+08
MPI Reductions:       9.274e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 7.3443e+01  99.7%  1.0766e+11 100.0%  4.721e+03  97.2%  1.851e+05      100.0%  1.574e+03   1.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult             1332 1.0 5.1205e+00 1.0 5.24e+09 1.1 4.0e+03 2.2e+04 0.0e+00  7 14 82 10  0   7 14 85 10  0  2892
MatSolve            1424 1.0 3.6244e+00 1.1 5.19e+09 1.1 0.0e+00 0.0e+00 0.0e+00  5 13  0  0  0   5 13  0  0  0  3997
MatLUFactorNum        92 1.0 1.2454e+01 1.0 2.72e+10 1.1 0.0e+00 0.0e+00 0.0e+00 17 71  0  0  0  17 71  0  0  0  6168
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      92 1.0 3.1039e-01 1.6 0.00e+00 0.0 6.9e+02 1.2e+06 9.2e+01  0  0 14 90  0   0  0 15 90  6     0
MatAssemblyEnd        92 1.0 2.2195e+00 1.1 0.00e+00 0.0 1.2e+01 5.5e+03 9.9e+01  3  0  0  0  0   3  0  0  0  6     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        92 1.0 2.1495e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             1332 1.0 6.1013e-01 1.5 2.30e+08 1.1 0.0e+00 0.0e+00 6.7e+02  1  1  0  0  1   1  1  0  0 42  1089
VecNorm             1424 1.0 1.5740e+00 1.3 3.11e+07 1.1 0.0e+00 0.0e+00 7.1e+02  2  0  0  0  1   2  0  0  0 45    57
VecScale            1422 1.0 6.6195e-03 1.0 1.55e+07 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6775
VecCopy               92 1.0 2.3403e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              1608 1.0 4.6007e-02 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               90 1.0 1.9193e-03 1.4 1.97e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2958
VecMAXPY            1422 1.0 5.8727e-02 1.0 2.59e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 12740
VecScatterBegin     1332 1.0 8.7834e-0196.2 0.00e+00 0.0 4.0e+03 2.2e+04 0.0e+00  1  0 82 10  0   1  0 85 10  0     0
VecScatterEnd       1332 1.0 1.1649e+00 2.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
VecNormalize        1424 1.0 1.5814e+00 1.3 4.66e+07 1.1 0.0e+00 0.0e+00 7.1e+02  2  0  0  0  1   2  0  0  0 45    85
KSPGMRESOrthog      1332 1.0 6.6608e-01 1.4 4.60e+08 1.1 0.0e+00 0.0e+00 6.7e+02  1  1  0  0  1   1  1  0  0 42  1994
KSPSetUp              94 1.0 1.3399e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              92 1.0 2.2870e+01 1.0 3.81e+10 1.1 4.0e+03 2.2e+04 1.4e+03 31100 82 10  1  31100 85 10 88  4707
PCSetUp              184 1.0 1.2471e+01 1.0 2.72e+10 1.1 0.0e+00 0.0e+00 0.0e+00 17 71  0  0  0  17 71  0  0  0  6160
PCSetUpOnBlocks       92 1.0 2.5847e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0  6460
PCApply             1424 1.0 1.5876e+01 1.0 3.18e+10 1.1 0.0e+00 0.0e+00 0.0e+00 21 83  0  0  0  21 83  0  0  0  5645
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 2.19345e-06
Average time for zero size MPI_Send(): 7.15256e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:07:40 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           7.530e+01      1.00000   7.530e+01
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                3.897e+10      1.11476   3.667e+10  1.100e+11
Flops/sec:            5.175e+08      1.11476   4.870e+08  1.461e+09
MPI Messages:         1.694e+03      1.04342   1.655e+03  4.964e+03
MPI Message Lengths:  3.683e+08      1.37113   1.850e+05  9.184e+08
MPI Reductions:       9.481e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 7.5057e+01  99.7%  1.1002e+11 100.0%  4.826e+03  97.2%  1.850e+05      100.0%  1.609e+03   1.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult             1362 1.0 5.2270e+00 1.0 5.36e+09 1.1 4.1e+03 2.2e+04 0.0e+00  7 14 82 10  0   7 14 85 10  0  2896
MatSolve            1456 1.0 3.7180e+00 1.1 5.31e+09 1.1 0.0e+00 0.0e+00 0.0e+00  5 13  0  0  0   5 13  0  0  0  3984
MatLUFactorNum        94 1.0 1.2754e+01 1.0 2.77e+10 1.1 0.0e+00 0.0e+00 0.0e+00 17 71  0  0  0  17 71  0  0  0  6154
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      94 1.0 3.1658e-01 1.6 0.00e+00 0.0 7.0e+02 1.2e+06 9.4e+01  0  0 14 90  0   0  0 15 90  6     0
MatAssemblyEnd        94 1.0 2.2719e+00 1.1 0.00e+00 0.0 1.2e+01 5.5e+03 1.0e+02  3  0  0  0  0   3  0  0  0  6     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        94 1.0 2.1910e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             1362 1.0 6.2693e-01 1.5 2.35e+08 1.1 0.0e+00 0.0e+00 6.8e+02  1  1  0  0  1   1  1  0  0 42  1084
VecNorm             1456 1.0 1.6074e+00 1.3 3.18e+07 1.1 0.0e+00 0.0e+00 7.3e+02  2  0  0  0  1   2  0  0  0 45    57
VecScale            1454 1.0 6.7732e-03 1.0 1.59e+07 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6770
VecCopy               94 1.0 2.3894e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              1644 1.0 4.6913e-02 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               92 1.0 1.9453e-03 1.4 2.01e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2983
VecMAXPY            1454 1.0 6.0161e-02 1.0 2.65e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 12720
VecScatterBegin     1362 1.0 8.7864e-0193.7 0.00e+00 0.0 4.1e+03 2.2e+04 0.0e+00  1  0 82 10  0   1  0 85 10  0     0
VecScatterEnd       1362 1.0 1.1695e+00 2.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
VecNormalize        1456 1.0 1.6150e+00 1.3 4.77e+07 1.1 0.0e+00 0.0e+00 7.3e+02  2  0  0  0  1   2  0  0  0 45    85
KSPGMRESOrthog      1362 1.0 6.8436e-01 1.4 4.70e+08 1.1 0.0e+00 0.0e+00 6.8e+02  1  1  0  0  1   1  1  0  0 42  1985
KSPSetUp              96 1.0 1.3399e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              94 1.0 2.3380e+01 1.0 3.90e+10 1.1 4.1e+03 2.2e+04 1.4e+03 31100 82 10  1  31100 85 10 88  4706
PCSetUp              188 1.0 1.2770e+01 1.0 2.77e+10 1.1 0.0e+00 0.0e+00 0.0e+00 17 71  0  0  0  17 71  0  0  0  6146
PCSetUpOnBlocks       94 1.0 2.5847e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0  6460
PCApply             1456 1.0 1.6270e+01 1.0 3.25e+10 1.1 0.0e+00 0.0e+00 0.0e+00 21 83  0  0  0  21 83  0  0  0  5632
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 3.00407e-06
Average time for zero size MPI_Send(): 9.53674e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:07:42 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           7.700e+01      1.00000   7.700e+01
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                3.980e+10      1.11476   3.746e+10  1.124e+11
Flops/sec:            5.169e+08      1.11476   4.865e+08  1.460e+09
MPI Messages:         1.731e+03      1.04340   1.691e+03  5.073e+03
MPI Message Lengths:  3.762e+08      1.37112   1.849e+05  9.380e+08
MPI Reductions:       9.687e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 7.6750e+01  99.7%  1.1239e+11 100.0%  4.932e+03  97.2%  1.849e+05      100.0%  1.644e+03   1.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult             1392 1.0 5.3338e+00 1.0 5.48e+09 1.1 4.2e+03 2.2e+04 0.0e+00  7 14 82 10  0   7 14 85 10  0  2901
MatSolve            1488 1.0 3.7957e+00 1.1 5.42e+09 1.1 0.0e+00 0.0e+00 0.0e+00  5 13  0  0  0   5 13  0  0  0  3988
MatLUFactorNum        96 1.0 1.3024e+01 1.0 2.83e+10 1.1 0.0e+00 0.0e+00 0.0e+00 17 71  0  0  0  17 71  0  0  0  6154
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      96 1.0 3.2279e-01 1.6 0.00e+00 0.0 7.2e+02 1.2e+06 9.6e+01  0  0 14 90  0   0  0 15 90  6     0
MatAssemblyEnd        96 1.0 2.3221e+00 1.1 0.00e+00 0.0 1.2e+01 5.5e+03 1.0e+02  3  0  0  0  0   3  0  0  0  6     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        96 1.0 2.2361e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             1392 1.0 6.3546e-01 1.4 2.40e+08 1.1 0.0e+00 0.0e+00 7.0e+02  1  1  0  0  1   1  1  0  0 42  1093
VecNorm             1488 1.0 1.6089e+00 1.3 3.25e+07 1.1 0.0e+00 0.0e+00 7.4e+02  2  0  0  0  1   2  0  0  0 45    58
VecScale            1486 1.0 6.9492e-03 1.1 1.62e+07 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6744
VecCopy               96 1.0 2.4395e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              1680 1.0 5.0030e-02 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               94 1.0 1.9753e-03 1.4 2.05e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  3002
VecMAXPY            1486 1.0 6.1590e-02 1.0 2.71e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 12701
VecScatterBegin     1392 1.0 8.7905e-0191.7 0.00e+00 0.0 4.2e+03 2.2e+04 0.0e+00  1  0 82 10  0   1  0 85 10  0     0
VecScatterEnd       1392 1.0 1.1903e+00 2.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
VecNormalize        1488 1.0 1.6167e+00 1.3 4.87e+07 1.1 0.0e+00 0.0e+00 7.4e+02  2  0  0  0  1   2  0  0  0 45    87
KSPGMRESOrthog      1392 1.0 6.9402e-01 1.4 4.81e+08 1.1 0.0e+00 0.0e+00 7.0e+02  1  1  0  0  1   1  1  0  0 42  2001
KSPSetUp              98 1.0 1.3399e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              96 1.0 2.3892e+01 1.0 3.98e+10 1.1 4.2e+03 2.2e+04 1.4e+03 31100 82 10  1  31100 85 10 88  4704
PCSetUp              192 1.0 1.3040e+01 1.0 2.83e+10 1.1 0.0e+00 0.0e+00 0.0e+00 17 71  0  0  0  17 71  0  0  0  6147
PCSetUpOnBlocks       96 1.0 2.5848e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  6460
PCApply             1488 1.0 1.6619e+01 1.0 3.32e+10 1.1 0.0e+00 0.0e+00 0.0e+00 21 83  0  0  0  21 83  0  0  0  5633
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 2.57492e-06
Average time for zero size MPI_Send(): 1.03315e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:07:44 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           7.869e+01      1.00000   7.869e+01
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                4.064e+10      1.11476   3.825e+10  1.148e+11
Flops/sec:            5.165e+08      1.11477   4.861e+08  1.458e+09
MPI Messages:         1.768e+03      1.04338   1.727e+03  5.182e+03
MPI Message Lengths:  3.841e+08      1.37112   1.848e+05  9.576e+08
MPI Reductions:       9.893e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 7.8428e+01  99.7%  1.1475e+11 100.0%  5.038e+03  97.2%  1.848e+05      100.0%  1.679e+03   1.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult             1422 1.0 5.4260e+00 1.0 5.60e+09 1.1 4.3e+03 2.2e+04 0.0e+00  7 14 82 10  0   7 14 85 10  0  2913
MatSolve            1520 1.0 3.8801e+00 1.1 5.54e+09 1.1 0.0e+00 0.0e+00 0.0e+00  5 13  0  0  0   5 13  0  0  0  3985
MatLUFactorNum        98 1.0 1.3323e+01 1.0 2.89e+10 1.1 0.0e+00 0.0e+00 0.0e+00 17 71  0  0  0  17 71  0  0  0  6142
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      98 1.0 3.2821e-01 1.6 0.00e+00 0.0 7.4e+02 1.2e+06 9.8e+01  0  0 14 90  0   0  0 15 90  6     0
MatAssemblyEnd        98 1.0 2.3721e+00 1.1 0.00e+00 0.0 1.2e+01 5.5e+03 1.0e+02  3  0  0  0  0   3  0  0  0  6     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        98 1.0 2.2808e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             1422 1.0 6.4377e-01 1.4 2.46e+08 1.1 0.0e+00 0.0e+00 7.1e+02  1  1  0  0  1   1  1  0  0 42  1102
VecNorm             1520 1.0 1.6635e+00 1.3 3.32e+07 1.1 0.0e+00 0.0e+00 7.6e+02  2  0  0  0  1   2  0  0  0 45    58
VecScale            1518 1.0 7.0901e-03 1.1 1.66e+07 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6752
VecCopy               98 1.0 2.4614e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              1716 1.0 5.0952e-02 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               96 1.0 2.0213e-03 1.4 2.10e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2996
VecMAXPY            1518 1.0 6.2844e-02 1.0 2.77e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 12719
VecScatterBegin     1422 1.0 8.7932e-0189.8 0.00e+00 0.0 4.3e+03 2.2e+04 0.0e+00  1  0 82 10  0   1  0 85 10  0     0
VecScatterEnd       1422 1.0 1.1911e+00 2.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
VecNormalize        1520 1.0 1.6715e+00 1.3 4.98e+07 1.1 0.0e+00 0.0e+00 7.6e+02  2  0  0  0  1   2  0  0  0 45    86
KSPGMRESOrthog      1422 1.0 7.0358e-01 1.4 4.91e+08 1.1 0.0e+00 0.0e+00 7.1e+02  1  1  0  0  1   1  1  0  0 42  2017
KSPSetUp             100 1.0 1.3399e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              98 1.0 2.4372e+01 1.0 4.06e+10 1.1 4.3e+03 2.2e+04 1.5e+03 31100 82 10  1  31100 85 10 88  4708
PCSetUp              196 1.0 1.3339e+01 1.0 2.89e+10 1.1 0.0e+00 0.0e+00 0.0e+00 17 71  0  0  0  17 71  0  0  0  6134
PCSetUpOnBlocks       98 1.0 2.5848e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  6460
PCApply             1520 1.0 1.7003e+01 1.0 3.39e+10 1.1 0.0e+00 0.0e+00 0.0e+00 21 83  0  0  0  21 83  0  0  0  5623
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 2.00272e-06
Average time for zero size MPI_Send(): 9.53674e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 3 processors, by yuan Mon Apr 17 10:07:46 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           8.054e+01      1.00000   8.054e+01
Objects:              4.900e+01      1.00000   4.900e+01
Flops:                4.148e+10      1.11477   3.904e+10  1.171e+11
Flops/sec:            5.150e+08      1.11477   4.847e+08  1.454e+09
MPI Messages:         1.805e+03      1.04335   1.763e+03  5.290e+03
MPI Message Lengths:  3.919e+08      1.37111   1.847e+05  9.772e+08
MPI Reductions:       1.010e+05      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 8.0278e+01  99.7%  1.1711e+11 100.0%  5.143e+03  97.2%  1.847e+05      100.0%  1.714e+03   1.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult             1452 1.0 5.5146e+00 1.0 5.72e+09 1.1 4.4e+03 2.2e+04 0.0e+00  7 14 82 10  0   7 14 85 10  0  2927
MatSolve            1552 1.0 3.9561e+00 1.1 5.66e+09 1.1 0.0e+00 0.0e+00 0.0e+00  5 13  0  0  0   5 13  0  0  0  3991
MatLUFactorNum       100 1.0 1.3607e+01 1.0 2.95e+10 1.1 0.0e+00 0.0e+00 0.0e+00 17 71  0  0  0  17 71  0  0  0  6136
MatILUFactorSym        2 1.0 1.5334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin     100 1.0 3.3517e-01 1.6 0.00e+00 0.0 7.5e+02 1.2e+06 1.0e+02  0  0 14 90  0   0  0 15 90  6     0
MatAssemblyEnd       100 1.0 2.4315e+00 1.1 0.00e+00 0.0 1.2e+01 5.5e+03 1.1e+02  3  0  0  0  0   3  0  0  0  6     0
MatGetRowIJ            2 1.0 9.5367e-07 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 1.5688e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries       100 1.0 2.3217e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             1452 1.0 6.5481e-01 1.4 2.51e+08 1.1 0.0e+00 0.0e+00 7.3e+02  1  1  0  0  1   1  1  0  0 42  1107
VecNorm             1552 1.0 1.7193e+00 1.4 3.39e+07 1.1 0.0e+00 0.0e+00 7.8e+02  2  0  0  0  1   2  0  0  0 45    57
VecScale            1550 1.0 7.2360e-03 1.1 1.69e+07 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6756
VecCopy              100 1.0 2.5103e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              1752 1.0 5.3750e-02 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               98 1.0 2.0435e-03 1.4 2.14e+06 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  3025
VecMAXPY            1550 1.0 6.3952e-02 1.0 2.83e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 12765
VecScatterBegin     1452 1.0 8.7965e-0188.0 0.00e+00 0.0 4.4e+03 2.2e+04 0.0e+00  1  0 82 10  0   1  0 85 10  0     0
VecScatterEnd       1452 1.0 1.1976e+00 2.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
VecNormalize        1552 1.0 1.7274e+00 1.4 5.08e+07 1.1 0.0e+00 0.0e+00 7.8e+02  2  0  0  0  1   2  0  0  0 45    85
KSPGMRESOrthog      1452 1.0 7.1571e-01 1.4 5.02e+08 1.1 0.0e+00 0.0e+00 7.3e+02  1  1  0  0  1   1  1  0  0 42  2025
KSPSetUp             102 1.0 1.3399e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve             100 1.0 2.4828e+01 1.0 4.15e+10 1.1 4.4e+03 2.2e+04 1.5e+03 31100 82 10  1  31100 85 10 88  4717
PCSetUp              200 1.0 1.3623e+01 1.0 2.95e+10 1.1 0.0e+00 0.0e+00 0.0e+00 17 71  0  0  0  17 71  0  0  0  6129
PCSetUpOnBlocks      100 1.0 2.5848e-01 1.1 5.90e+08 1.1 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  6460
PCApply             1552 1.0 1.7364e+01 1.0 3.46e+10 1.1 0.0e+00 0.0e+00 0.0e+00 21 83  0  0  0  21 83  0  0  0  5621
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              0            0     0.
              Vector    31              1         1656     0.
      Vector Scatter     1              0            0     0.
       Krylov Solver     3              0            0     0.
           Index Set     7              4        58568     0.
      Preconditioner     2              0            0     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 2.38419e-06
Average time for zero size MPI_Send(): 1.03315e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1
-----------------------------------------
Libraries compiled on Mon Apr  3 13:12:31 2017 on cml01 
Machine characteristics: Linux-4.4.0-71-generic-x86_64-with-Ubuntu-14.04-trusty
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lparmetis -lmetis -lspai -lsuperlu -lscalapack -lflapack -lfblas -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_f77 -lgfortran -lm -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

