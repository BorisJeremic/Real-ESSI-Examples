************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 2 processors, by yuan Sat Jun 24 22:53:58 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           2.586e+01      1.00000   2.586e+01
Objects:              1.160e+02      1.00000   1.160e+02
Flops:                5.171e+08      1.01164   5.141e+08  1.028e+09
Flops/sec:            1.999e+07      1.01163   1.988e+07  3.976e+07
MPI Messages:         1.316e+03      1.00420   1.313e+03  2.626e+03
MPI Message Lengths:  2.470e+06      1.00001   1.881e+03  4.939e+06
MPI Reductions:       2.284e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 2.5848e+01  99.9%  1.0283e+09 100.0%  2.606e+03  99.2%  1.881e+03      100.0%  2.574e+03  11.3% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult             2526 1.0 5.8465e-02 1.0 1.70e+08 1.0 2.5e+03 1.5e+03 0.0e+00  0 33 96 78  0   0 33 97 78  0  5785
MatSolve            2548 1.0 6.5337e-02 1.0 1.67e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 32  0  0  0   0 32  0  0  0  5072
MatLUFactorNum        22 1.0 9.3610e-03 1.0 1.49e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0  3186
MatILUFactorSym        4 1.0 9.4485e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      22 1.0 5.1714e-01806.6 0.00e+00 0.0 6.6e+01 1.6e+04 2.2e+01  1  0  3 22  0   1  0  3 22  1     0
MatAssemblyEnd        22 1.0 2.7251e-03 1.0 0.00e+00 0.0 8.0e+00 3.8e+02 3.6e+01  0  0  0  0  0   0  0  0  0  1     0
MatGetRowIJ            4 1.0 5.0068e-06 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         4 1.0 5.2691e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        22 1.0 1.7052e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             2464 1.0 1.6469e-02 1.1 7.62e+07 1.0 0.0e+00 0.0e+00 1.2e+03  0 15  0  0  5   0 15  0  0 48  9204
VecNorm             2548 1.0 6.0878e-03 1.1 5.09e+06 1.0 0.0e+00 0.0e+00 1.3e+03  0  1  0  0  6   0  1  0  0 49  1664
VecScale            2548 1.0 6.7663e-04 1.1 2.55e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  7486
VecCopy               84 1.0 6.4850e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              2658 1.0 1.0972e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY              146 1.0 6.0081e-05 1.2 2.92e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  9662
VecMAXPY            2548 1.0 9.3503e-03 1.0 8.11e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0 16  0  0  0   0 16  0  0  0 17264
VecScatterBegin     2526 1.0 1.2801e-03 1.0 0.00e+00 0.0 2.5e+03 1.5e+03 0.0e+00  0  0 96 78  0   0  0 97 78  0     0
VecScatterEnd       2526 1.0 1.5337e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize        2548 1.0 6.9406e-03 1.1 7.64e+06 1.0 0.0e+00 0.0e+00 1.3e+03  0  1  0  0  6   0  1  0  0 49  2189
KSPGMRESOrthog      2464 1.0 2.5613e-02 1.1 1.52e+08 1.0 0.0e+00 0.0e+00 1.2e+03  0 29  0  0  5   0 29  0  0 48 11838
KSPSetUp              26 1.0 8.7976e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              22 1.0 1.7162e-01 1.0 5.17e+08 1.0 2.5e+03 1.5e+03 2.5e+03  1100 96 78 11   1100 97 78 97  5992
PCSetUp               44 1.0 1.0500e-02 1.0 1.49e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0  2840
PCSetUpOnBlocks       22 1.0 2.7280e-03 1.0 2.71e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  1988
PCApply             2548 1.0 7.7894e-02 1.0 1.79e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 35  0  0  0   0 35  0  0  0  4568
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     8              0            0     0.
              Vector    80              2         3312     0.
      Vector Scatter     2              0            0     0.
       Krylov Solver     6              0            0     0.
           Index Set    14              8        15440     0.
      Preconditioner     4              0            0     0.
              Viewer     2              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 5.72205e-07
Average time for zero size MPI_Send(): 4.76837e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1 --with-mumps=1 --download-mumps=1 --with-ptscotch=1 --download-ptscotch=1
-----------------------------------------
Libraries compiled on Sat Jun 24 17:54:26 2017 on cml01 
Machine characteristics: Linux-4.8.0-56-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include/openmpi/opal/mca/event/libevent2021/libevent -I/usr/lib/openmpi/include/openmpi/opal/mca/event/libevent2021/libevent/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lparmetis -lmetis -lscalapack -lsuperlu -lspai -lflapack -lfblas -lhwloc -lptesmumps -lptscotch -lptscotcherr -lscotch -lscotcherr -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lgfortran -lm -Wl,-rpath,/usr/lib/openmpi/lib -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -lrt -lm -lz -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -Wl,-rpath,/usr/lib/openmpi/lib -lmpi -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 2 processors, by yuan Sat Jun 24 22:53:59 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           2.730e+01      1.00000   2.730e+01
Objects:              1.160e+02      1.00000   1.160e+02
Flops:                5.344e+08      1.01162   5.314e+08  1.063e+09
Flops/sec:            1.957e+07      1.01162   1.946e+07  3.892e+07
MPI Messages:         1.362e+03      1.00442   1.359e+03  2.718e+03
MPI Message Lengths:  2.585e+06      1.00001   1.902e+03  5.170e+06
MPI Reductions:       2.495e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 2.7286e+01  99.9%  1.0627e+09 100.0%  2.696e+03  99.2%  1.902e+03      100.0%  2.662e+03  10.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult             2610 1.0 6.0452e-02 1.0 1.76e+08 1.0 2.6e+03 1.5e+03 0.0e+00  0 33 96 77  0   0 33 97 77  0  5781
MatSolve            2634 1.0 6.7548e-02 1.0 1.72e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 32  0  0  0   0 32  0  0  0  5072
MatLUFactorNum        24 1.0 1.0174e-02 1.0 1.63e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0  3198
MatILUFactorSym        4 1.0 9.4485e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      24 1.0 5.1722e-01721.2 0.00e+00 0.0 7.2e+01 1.6e+04 2.4e+01  1  0  3 23  0   1  0  3 23  1     0
MatAssemblyEnd        24 1.0 2.9352e-03 1.0 0.00e+00 0.0 8.0e+00 3.8e+02 3.8e+01  0  0  0  0  0   0  0  0  0  1     0
MatGetRowIJ            4 1.0 5.0068e-06 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         4 1.0 5.2691e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        24 1.0 1.7583e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             2546 1.0 1.7003e-02 1.1 7.83e+07 1.0 0.0e+00 0.0e+00 1.3e+03  0 15  0  0  5   0 15  0  0 48  9164
VecNorm             2634 1.0 6.2802e-03 1.1 5.26e+06 1.0 0.0e+00 0.0e+00 1.3e+03  0  1  0  0  5   0  1  0  0 49  1668
VecScale            2634 1.0 6.9737e-04 1.1 2.63e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  7509
VecCopy               88 1.0 6.8665e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              2750 1.0 1.1783e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY              152 1.0 6.2943e-05 1.2 3.04e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  9602
VecMAXPY            2634 1.0 9.6128e-03 1.0 8.34e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0 16  0  0  0   0 16  0  0  0 17266
VecScatterBegin     2610 1.0 1.3180e-03 1.0 0.00e+00 0.0 2.6e+03 1.5e+03 0.0e+00  0  0 96 77  0   0  0 97 77  0     0
VecScatterEnd       2610 1.0 1.5843e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize        2634 1.0 7.1557e-03 1.1 7.89e+06 1.0 0.0e+00 0.0e+00 1.3e+03  0  1  0  0  5   0  1  0  0 49  2195
KSPGMRESOrthog      2546 1.0 2.6400e-02 1.1 1.57e+08 1.0 0.0e+00 0.0e+00 1.3e+03  0 29  0  0  5   0 29  0  0 48 11805
KSPSetUp              28 1.0 8.7976e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              24 1.0 1.7784e-01 1.0 5.34e+08 1.0 2.6e+03 1.5e+03 2.6e+03  1100 96 77 10   1100 97 77 97  5976
PCSetUp               48 1.0 1.1318e-02 1.0 1.63e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0  2875
PCSetUpOnBlocks       24 1.0 2.7289e-03 1.0 2.71e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  1987
PCApply             2634 1.0 8.1131e-02 1.0 1.86e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 35  0  0  0   0 35  0  0  0  4557
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     8              0            0     0.
              Vector    80              2         3312     0.
      Vector Scatter     2              0            0     0.
       Krylov Solver     6              0            0     0.
           Index Set    14              8        15440     0.
      Preconditioner     4              0            0     0.
              Viewer     2              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 4.29153e-07
Average time for zero size MPI_Send(): 9.53674e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1 --with-mumps=1 --download-mumps=1 --with-ptscotch=1 --download-ptscotch=1
-----------------------------------------
Libraries compiled on Sat Jun 24 17:54:26 2017 on cml01 
Machine characteristics: Linux-4.8.0-56-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include/openmpi/opal/mca/event/libevent2021/libevent -I/usr/lib/openmpi/include/openmpi/opal/mca/event/libevent2021/libevent/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lparmetis -lmetis -lscalapack -lsuperlu -lspai -lflapack -lfblas -lhwloc -lptesmumps -lptscotch -lptscotcherr -lscotch -lscotcherr -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lgfortran -lm -Wl,-rpath,/usr/lib/openmpi/lib -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -lrt -lm -lz -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -Wl,-rpath,/usr/lib/openmpi/lib -lmpi -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 2 processors, by yuan Sat Jun 24 22:54:00 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           2.874e+01      1.00000   2.874e+01
Objects:              1.160e+02      1.00000   1.160e+02
Flops:                5.577e+08      1.01161   5.545e+08  1.109e+09
Flops/sec:            1.941e+07      1.01162   1.930e+07  3.859e+07
MPI Messages:         1.422e+03      1.00459   1.419e+03  2.838e+03
MPI Message Lengths:  2.722e+06      1.00001   1.918e+03  5.444e+06
MPI Reductions:       2.709e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 2.8717e+01  99.9%  1.1090e+09 100.0%  2.814e+03  99.2%  1.918e+03      100.0%  2.778e+03  10.3% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult             2722 1.0 6.2968e-02 1.0 1.83e+08 1.0 2.7e+03 1.5e+03 0.0e+00  0 33 96 76  0   0 33 97 76  0  5789
MatSolve            2748 1.0 7.0379e-02 1.0 1.80e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 32  0  0  0   0 32  0  0  0  5079
MatLUFactorNum        26 1.0 1.0986e-02 1.0 1.76e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0  3208
MatILUFactorSym        4 1.0 9.4485e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      26 1.0 5.1727e-01668.2 0.00e+00 0.0 7.8e+01 1.6e+04 2.6e+01  1  0  3 24  0   1  0  3 24  1     0
MatAssemblyEnd        26 1.0 3.1571e-03 1.0 0.00e+00 0.0 8.0e+00 3.8e+02 4.0e+01  0  0  0  0  0   0  0  0  0  1     0
MatGetRowIJ            4 1.0 5.0068e-06 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         4 1.0 5.2691e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        26 1.0 1.8313e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             2656 1.0 1.7583e-02 1.1 8.15e+07 1.0 0.0e+00 0.0e+00 1.3e+03  0 15  0  0  5   0 15  0  0 48  9219
VecNorm             2748 1.0 6.5181e-03 1.1 5.49e+06 1.0 0.0e+00 0.0e+00 1.4e+03  0  1  0  0  5   0  1  0  0 49  1676
VecScale            2748 1.0 7.2479e-04 1.1 2.75e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  7537
VecCopy               92 1.0 7.2956e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              2870 1.0 1.2658e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY              158 1.0 6.5088e-05 1.2 3.16e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  9652
VecMAXPY            2748 1.0 9.9924e-03 1.0 8.68e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0 16  0  0  0   0 16  0  0  0 17283
VecScatterBegin     2722 1.0 1.3857e-03 1.0 0.00e+00 0.0 2.7e+03 1.5e+03 0.0e+00  0  0 96 76  0   0  0 97 76  0     0
VecScatterEnd       2722 1.0 1.6448e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize        2748 1.0 7.4368e-03 1.1 8.24e+06 1.0 0.0e+00 0.0e+00 1.4e+03  0  1  0  0  5   0  1  0  0 49  2204
KSPGMRESOrthog      2656 1.0 2.7357e-02 1.1 1.63e+08 1.0 0.0e+00 0.0e+00 1.3e+03  0 29  0  0  5   0 29  0  0 48 11852
KSPSetUp              30 1.0 8.7976e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              26 1.0 1.8568e-01 1.0 5.58e+08 1.0 2.7e+03 1.5e+03 2.7e+03  1100 96 76 10   1100 97 76 97  5972
PCSetUp               52 1.0 1.2137e-02 1.0 1.76e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0  2904
PCSetUpOnBlocks       26 1.0 2.7289e-03 1.0 2.71e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1987
PCApply             2748 1.0 8.5038e-02 1.0 1.95e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 35  0  0  0   0 35  0  0  0  4554
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     8              0            0     0.
              Vector    80              2         3312     0.
      Vector Scatter     2              0            0     0.
       Krylov Solver     6              0            0     0.
           Index Set    14              8        15440     0.
      Preconditioner     4              0            0     0.
              Viewer     2              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 3.8147e-07
Average time for zero size MPI_Send(): 4.76837e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1 --with-mumps=1 --download-mumps=1 --with-ptscotch=1 --download-ptscotch=1
-----------------------------------------
Libraries compiled on Sat Jun 24 17:54:26 2017 on cml01 
Machine characteristics: Linux-4.8.0-56-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include/openmpi/opal/mca/event/libevent2021/libevent -I/usr/lib/openmpi/include/openmpi/opal/mca/event/libevent2021/libevent/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lparmetis -lmetis -lscalapack -lsuperlu -lspai -lflapack -lfblas -lhwloc -lptesmumps -lptscotch -lptscotcherr -lscotch -lscotcherr -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lgfortran -lm -Wl,-rpath,/usr/lib/openmpi/lib -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -lrt -lm -lz -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -Wl,-rpath,/usr/lib/openmpi/lib -lmpi -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 2 processors, by yuan Sat Jun 24 22:54:02 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           3.016e+01      1.00000   3.016e+01
Objects:              1.160e+02      1.00000   1.160e+02
Flops:                5.819e+08      1.01161   5.786e+08  1.157e+09
Flops/sec:            1.929e+07      1.01161   1.918e+07  3.836e+07
MPI Messages:         1.485e+03      1.00474   1.482e+03  2.963e+03
MPI Message Lengths:  2.862e+06      1.00001   1.932e+03  5.723e+06
MPI Reductions:       2.924e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 3.0144e+01  99.9%  1.1572e+09 100.0%  2.937e+03  99.1%  1.932e+03      100.0%  2.898e+03   9.9% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult             2838 1.0 6.5581e-02 1.0 1.91e+08 1.0 2.8e+03 1.5e+03 0.0e+00  0 33 96 76  0   0 33 97 76  0  5795
MatSolve            2866 1.0 7.3342e-02 1.0 1.88e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 32  0  0  0   0 32  0  0  0  5083
MatLUFactorNum        28 1.0 1.1826e-02 1.0 1.90e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0  3210
MatILUFactorSym        4 1.0 9.4485e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      28 1.0 5.1731e-01626.9 0.00e+00 0.0 8.4e+01 1.6e+04 2.8e+01  1  0  3 24  0   1  0  3 24  1     0
MatAssemblyEnd        28 1.0 3.3491e-03 1.0 0.00e+00 0.0 8.0e+00 3.8e+02 4.2e+01  0  0  0  0  0   0  0  0  0  1     0
MatGetRowIJ            4 1.0 5.0068e-06 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         4 1.0 5.2691e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        28 1.0 1.8723e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             2770 1.0 1.8214e-02 1.1 8.48e+07 1.0 0.0e+00 0.0e+00 1.4e+03  0 15  0  0  5   0 15  0  0 48  9267
VecNorm             2866 1.0 6.8073e-03 1.1 5.73e+06 1.0 0.0e+00 0.0e+00 1.4e+03  0  1  0  0  5   0  1  0  0 49  1674
VecScale            2866 1.0 7.5769e-04 1.1 2.86e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  7520
VecCopy               96 1.0 7.6771e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              2994 1.0 1.3523e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY              164 1.0 6.6996e-05 1.2 3.28e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  9733
VecMAXPY            2866 1.0 1.0407e-02 1.0 9.04e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0 16  0  0  0   0 16  0  0  0 17281
VecScatterBegin     2838 1.0 1.4489e-03 1.0 0.00e+00 0.0 2.8e+03 1.5e+03 0.0e+00  0  0 96 76  0   0  0 97 76  0     0
VecScatterEnd       2838 1.0 1.7140e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize        2866 1.0 7.7636e-03 1.1 8.59e+06 1.0 0.0e+00 0.0e+00 1.4e+03  0  1  0  0  5   0  1  0  0 49  2202
KSPGMRESOrthog      2770 1.0 2.8390e-02 1.1 1.70e+08 1.0 0.0e+00 0.0e+00 1.4e+03  0 29  0  0  5   0 29  0  0 48 11893
KSPSetUp              32 1.0 8.8930e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              28 1.0 1.9381e-01 1.0 5.82e+08 1.0 2.8e+03 1.5e+03 2.8e+03  1100 96 76 10   1100 97 76 97  5971
PCSetUp               56 1.0 1.2982e-02 1.0 1.90e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0  2924
PCSetUpOnBlocks       28 1.0 2.7289e-03 1.0 2.71e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1987
PCApply             2866 1.0 8.9099e-02 1.0 2.04e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 35  0  0  0   0 35  0  0  0  4549
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     8              0            0     0.
              Vector    80              2         3312     0.
      Vector Scatter     2              0            0     0.
       Krylov Solver     6              0            0     0.
           Index Set    14              8        15440     0.
      Preconditioner     4              0            0     0.
              Viewer     2              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 3.8147e-07
Average time for zero size MPI_Send(): 4.76837e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1 --with-mumps=1 --download-mumps=1 --with-ptscotch=1 --download-ptscotch=1
-----------------------------------------
Libraries compiled on Sat Jun 24 17:54:26 2017 on cml01 
Machine characteristics: Linux-4.8.0-56-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include/openmpi/opal/mca/event/libevent2021/libevent -I/usr/lib/openmpi/include/openmpi/opal/mca/event/libevent2021/libevent/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lparmetis -lmetis -lscalapack -lsuperlu -lspai -lflapack -lfblas -lhwloc -lptesmumps -lptscotch -lptscotcherr -lscotch -lscotcherr -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lgfortran -lm -Wl,-rpath,/usr/lib/openmpi/lib -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -lrt -lm -lz -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -Wl,-rpath,/usr/lib/openmpi/lib -lmpi -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 2 processors, by yuan Sat Jun 24 22:54:03 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           3.163e+01      1.00000   3.163e+01
Objects:              1.160e+02      1.00000   1.160e+02
Flops:                6.066e+08      1.01160   6.032e+08  1.206e+09
Flops/sec:            1.918e+07      1.01160   1.907e+07  3.813e+07
MPI Messages:         1.548e+03      1.00487   1.545e+03  3.090e+03
MPI Message Lengths:  3.003e+06      1.00001   1.944e+03  6.006e+06
MPI Reductions:       3.138e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 3.1612e+01  99.9%  1.2063e+09 100.0%  3.062e+03  99.1%  1.944e+03      100.0%  3.020e+03   9.6% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult             2956 1.0 6.8415e-02 1.0 1.99e+08 1.0 3.0e+03 1.5e+03 0.0e+00  0 33 96 75  0   0 33 97 75  0  5786
MatSolve            2986 1.0 7.6597e-02 1.0 1.95e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 32  0  0  0   0 32  0  0  0  5070
MatLUFactorNum        30 1.0 1.2825e-02 1.0 2.04e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0  3171
MatILUFactorSym        4 1.0 9.4485e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      30 1.0 5.1740e-01559.7 0.00e+00 0.0 9.0e+01 1.7e+04 3.0e+01  1  0  3 25  0   1  0  3 25  1     0
MatAssemblyEnd        30 1.0 3.6111e-03 1.0 0.00e+00 0.0 8.0e+00 3.8e+02 4.4e+01  0  0  0  0  0   0  0  0  0  1     0
MatGetRowIJ            4 1.0 5.0068e-06 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         4 1.0 5.2691e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        30 1.0 2.0261e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             2886 1.0 1.9252e-02 1.2 8.83e+07 1.0 0.0e+00 0.0e+00 1.4e+03  0 15  0  0  5   0 15  0  0 48  9127
VecNorm             2986 1.0 7.1588e-03 1.1 5.97e+06 1.0 0.0e+00 0.0e+00 1.5e+03  0  1  0  0  5   0  1  0  0 49  1658
VecScale            2986 1.0 7.9632e-04 1.1 2.98e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  7455
VecCopy              100 1.0 8.5115e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              3120 1.0 1.5585e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY              170 1.0 6.8903e-05 1.1 3.40e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  9810
VecMAXPY            2986 1.0 1.0846e-02 1.0 9.41e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0 16  0  0  0   0 16  0  0  0 17264
VecScatterBegin     2956 1.0 1.5070e-03 1.0 0.00e+00 0.0 3.0e+03 1.5e+03 0.0e+00  0  0 96 75  0   0  0 97 75  0     0
VecScatterEnd       2956 1.0 1.8151e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize        2986 1.0 8.1687e-03 1.1 8.95e+06 1.0 0.0e+00 0.0e+00 1.5e+03  0  1  0  0  5   0  1  0  0 49  2180
KSPGMRESOrthog      2886 1.0 2.9847e-02 1.1 1.77e+08 1.0 0.0e+00 0.0e+00 1.4e+03  0 29  0  0  5   0 29  0  0 48 11776
KSPSetUp              34 1.0 8.9884e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              30 1.0 2.0279e-01 1.0 6.07e+08 1.0 3.0e+03 1.5e+03 2.9e+03  1100 96 75  9   1100 97 75 97  5949
PCSetUp               60 1.0 1.3989e-02 1.0 2.04e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0  2907
PCSetUpOnBlocks       30 1.0 2.7299e-03 1.0 2.71e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1986
PCApply             2986 1.0 9.3742e-02 1.0 2.13e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 35  0  0  0   0 35  0  0  0  4519
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     8              0            0     0.
              Vector    80              2         3312     0.
      Vector Scatter     2              0            0     0.
       Krylov Solver     6              0            0     0.
           Index Set    14              8        15440     0.
      Preconditioner     4              0            0     0.
              Viewer     2              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 6.19888e-07
Average time for zero size MPI_Send(): 4.76837e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1 --with-mumps=1 --download-mumps=1 --with-ptscotch=1 --download-ptscotch=1
-----------------------------------------
Libraries compiled on Sat Jun 24 17:54:26 2017 on cml01 
Machine characteristics: Linux-4.8.0-56-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include/openmpi/opal/mca/event/libevent2021/libevent -I/usr/lib/openmpi/include/openmpi/opal/mca/event/libevent2021/libevent/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lparmetis -lmetis -lscalapack -lsuperlu -lspai -lflapack -lfblas -lhwloc -lptesmumps -lptscotch -lptscotcherr -lscotch -lscotcherr -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lgfortran -lm -Wl,-rpath,/usr/lib/openmpi/lib -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -lrt -lm -lz -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -Wl,-rpath,/usr/lib/openmpi/lib -lmpi -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 2 processors, by yuan Sat Jun 24 22:54:05 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           3.312e+01      1.00000   3.312e+01
Objects:              1.160e+02      1.00000   1.160e+02
Flops:                6.348e+08      1.01159   6.312e+08  1.262e+09
Flops/sec:            1.917e+07      1.01159   1.906e+07  3.811e+07
MPI Messages:         1.622e+03      1.00496   1.618e+03  3.236e+03
MPI Message Lengths:  3.160e+06      1.00001   1.953e+03  6.319e+06
MPI Reductions:       3.355e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 3.3099e+01  99.9%  1.2624e+09 100.0%  3.206e+03  99.1%  1.953e+03      100.0%  3.161e+03   9.4% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult             3094 1.0 7.1663e-02 1.0 2.08e+08 1.0 3.1e+03 1.5e+03 0.0e+00  0 33 96 75  0   0 33 97 75  0  5781
MatSolve            3126 1.0 8.0328e-02 1.0 2.05e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 32  0  0  0   0 32  0  0  0  5062
MatLUFactorNum        32 1.0 1.3689e-02 1.0 2.17e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0  3169
MatILUFactorSym        4 1.0 9.4485e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      32 1.0 5.1744e-01530.0 0.00e+00 0.0 9.6e+01 1.7e+04 3.2e+01  1  0  3 25  0   1  0  3 25  1     0
MatAssemblyEnd        32 1.0 3.8002e-03 1.0 0.00e+00 0.0 8.0e+00 3.8e+02 4.6e+01  0  0  0  0  0   0  0  0  0  1     0
MatGetRowIJ            4 1.0 5.0068e-06 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         4 1.0 5.2691e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        32 1.0 2.0630e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             3020 1.0 2.0267e-02 1.2 9.21e+07 1.0 0.0e+00 0.0e+00 1.5e+03  0 15  0  0  5   0 15  0  0 48  9046
VecNorm             3126 1.0 7.5562e-03 1.1 6.25e+06 1.0 0.0e+00 0.0e+00 1.6e+03  0  1  0  0  5   0  1  0  0 49  1645
VecScale            3126 1.0 8.3566e-04 1.1 3.12e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  7437
VecCopy              106 1.0 9.0122e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              3268 1.0 1.6456e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY              180 1.0 7.2002e-05 1.1 3.60e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  9940
VecMAXPY            3126 1.0 1.1339e-02 1.0 9.82e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0 15  0  0  0   0 15  0  0  0 17231
VecScatterBegin     3094 1.0 1.5862e-03 1.0 0.00e+00 0.0 3.1e+03 1.5e+03 0.0e+00  0  0 96 75  0   0  0 97 75  0     0
VecScatterEnd       3094 1.0 1.9085e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize        3126 1.0 8.6126e-03 1.1 9.37e+06 1.0 0.0e+00 0.0e+00 1.6e+03  0  1  0  0  5   0  1  0  0 49  2165
KSPGMRESOrthog      3020 1.0 3.1321e-02 1.1 1.84e+08 1.0 0.0e+00 0.0e+00 1.5e+03  0 29  0  0  5   0 29  0  0 48 11708
KSPSetUp              36 1.0 8.9884e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              32 1.0 2.1256e-01 1.0 6.35e+08 1.0 3.1e+03 1.5e+03 3.1e+03  1100 96 75  9   1100 97 75 97  5939
PCSetUp               64 1.0 1.4858e-02 1.0 2.17e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0  2919
PCSetUpOnBlocks       32 1.0 2.7308e-03 1.0 2.71e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1986
PCApply             3126 1.0 9.8640e-02 1.0 2.24e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 35  0  0  0   0 35  0  0  0  4507
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     8              0            0     0.
              Vector    80              2         3312     0.
      Vector Scatter     2              0            0     0.
       Krylov Solver     6              0            0     0.
           Index Set    14              8        15440     0.
      Preconditioner     4              0            0     0.
              Viewer     2              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 6.19888e-07
Average time for zero size MPI_Send(): 9.53674e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1 --with-mumps=1 --download-mumps=1 --with-ptscotch=1 --download-ptscotch=1
-----------------------------------------
Libraries compiled on Sat Jun 24 17:54:26 2017 on cml01 
Machine characteristics: Linux-4.8.0-56-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include/openmpi/opal/mca/event/libevent2021/libevent -I/usr/lib/openmpi/include/openmpi/opal/mca/event/libevent2021/libevent/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lparmetis -lmetis -lscalapack -lsuperlu -lspai -lflapack -lfblas -lhwloc -lptesmumps -lptscotch -lptscotcherr -lscotch -lscotcherr -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lgfortran -lm -Wl,-rpath,/usr/lib/openmpi/lib -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -lrt -lm -lz -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -Wl,-rpath,/usr/lib/openmpi/lib -lmpi -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 2 processors, by yuan Sat Jun 24 22:54:06 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           3.456e+01      1.00000   3.456e+01
Objects:              1.160e+02      1.00000   1.160e+02
Flops:                6.590e+08      1.01159   6.553e+08  1.311e+09
Flops/sec:            1.907e+07      1.01159   1.896e+07  3.792e+07
MPI Messages:         1.684e+03      1.00507   1.680e+03  3.360e+03
MPI Message Lengths:  3.300e+06      1.00001   1.964e+03  6.599e+06
MPI Reductions:       3.570e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 3.4531e+01  99.9%  1.3105e+09 100.0%  3.328e+03  99.0%  1.964e+03      100.0%  3.281e+03   9.2% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult             3210 1.0 7.4288e-02 1.0 2.16e+08 1.0 3.2e+03 1.5e+03 0.0e+00  0 33 96 74  0   0 33 96 74  0  5786
MatSolve            3244 1.0 8.3279e-02 1.0 2.12e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 32  0  0  0   0 32  0  0  0  5067
MatLUFactorNum        34 1.0 1.4496e-02 1.0 2.31e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  3179
MatILUFactorSym        4 1.0 9.4485e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      34 1.0 5.1750e-01499.0 0.00e+00 0.0 1.0e+02 1.7e+04 3.4e+01  1  0  3 26  0   1  0  3 26  1     0
MatAssemblyEnd        34 1.0 4.0140e-03 1.0 0.00e+00 0.0 8.0e+00 3.8e+02 4.8e+01  0  0  0  0  0   0  0  0  0  1     0
MatGetRowIJ            4 1.0 5.0068e-06 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         4 1.0 5.2691e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        34 1.0 2.0940e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             3134 1.0 2.0897e-02 1.2 9.55e+07 1.0 0.0e+00 0.0e+00 1.6e+03  0 15  0  0  4   0 15  0  0 48  9094
VecNorm             3244 1.0 7.8015e-03 1.1 6.48e+06 1.0 0.0e+00 0.0e+00 1.6e+03  0  1  0  0  5   0  1  0  0 49  1653
VecScale            3244 1.0 8.6665e-04 1.1 3.24e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  7441
VecCopy              110 1.0 9.3460e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              3392 1.0 1.7312e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY              186 1.0 7.5102e-05 1.1 3.72e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  9847
VecMAXPY            3244 1.0 1.1752e-02 1.0 1.02e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 15  0  0  0   0 15  0  0  0 17235
VecScatterBegin     3210 1.0 1.6606e-03 1.0 0.00e+00 0.0 3.2e+03 1.5e+03 0.0e+00  0  0 96 74  0   0  0 96 74  0     0
VecScatterEnd       3210 1.0 1.9848e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize        3244 1.0 8.8973e-03 1.1 9.72e+06 1.0 0.0e+00 0.0e+00 1.6e+03  0  1  0  0  5   0  1  0  0 49  2175
KSPGMRESOrthog      3134 1.0 3.2361e-02 1.1 1.91e+08 1.0 0.0e+00 0.0e+00 1.6e+03  0 29  0  0  4   0 29  0  0 48 11746
KSPSetUp              38 1.0 8.9884e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              34 1.0 2.2067e-01 1.0 6.59e+08 1.0 3.2e+03 1.5e+03 3.2e+03  1100 96 74  9   1100 96 74 97  5939
PCSetUp               68 1.0 1.5670e-02 1.0 2.31e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  2941
PCSetUpOnBlocks       34 1.0 2.7308e-03 1.0 2.71e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1986
PCApply             3244 1.0 1.0265e-01 1.0 2.33e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 35  0  0  0   0 35  0  0  0  4507
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     8              0            0     0.
              Vector    80              2         3312     0.
      Vector Scatter     2              0            0     0.
       Krylov Solver     6              0            0     0.
           Index Set    14              8        15440     0.
      Preconditioner     4              0            0     0.
              Viewer     2              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 4.29153e-07
Average time for zero size MPI_Send(): 4.76837e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1 --with-mumps=1 --download-mumps=1 --with-ptscotch=1 --download-ptscotch=1
-----------------------------------------
Libraries compiled on Sat Jun 24 17:54:26 2017 on cml01 
Machine characteristics: Linux-4.8.0-56-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include/openmpi/opal/mca/event/libevent2021/libevent -I/usr/lib/openmpi/include/openmpi/opal/mca/event/libevent2021/libevent/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lparmetis -lmetis -lscalapack -lsuperlu -lspai -lflapack -lfblas -lhwloc -lptesmumps -lptscotch -lptscotcherr -lscotch -lscotcherr -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lgfortran -lm -Wl,-rpath,/usr/lib/openmpi/lib -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -lrt -lm -lz -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -Wl,-rpath,/usr/lib/openmpi/lib -lmpi -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 2 processors, by yuan Sat Jun 24 22:54:08 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           3.598e+01      1.00000   3.598e+01
Objects:              1.160e+02      1.00000   1.160e+02
Flops:                6.833e+08      1.01158   6.794e+08  1.359e+09
Flops/sec:            1.899e+07      1.01158   1.888e+07  3.776e+07
MPI Messages:         1.747e+03      1.00518   1.742e+03  3.485e+03
MPI Message Lengths:  3.439e+06      1.00001   1.974e+03  6.879e+06
MPI Reductions:       3.784e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 3.5953e+01  99.9%  1.3587e+09 100.0%  3.451e+03  99.0%  1.974e+03      100.0%  3.401e+03   9.0% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult             3326 1.0 7.6921e-02 1.0 2.24e+08 1.0 3.3e+03 1.5e+03 0.0e+00  0 33 95 74  0   0 33 96 74  0  5790
MatSolve            3362 1.0 8.6245e-02 1.0 2.20e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 32  0  0  0   0 32  0  0  0  5070
MatLUFactorNum        36 1.0 1.5303e-02 1.0 2.44e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  3189
MatILUFactorSym        4 1.0 9.4485e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      36 1.0 5.1754e-01476.1 0.00e+00 0.0 1.1e+02 1.7e+04 3.6e+01  1  0  3 26  0   1  0  3 26  1     0
MatAssemblyEnd        36 1.0 4.2040e-03 1.0 0.00e+00 0.0 8.0e+00 3.8e+02 5.0e+01  0  0  0  0  0   0  0  0  0  1     0
MatGetRowIJ            4 1.0 5.0068e-06 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         4 1.0 5.2691e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        36 1.0 2.1212e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             3248 1.0 2.1576e-02 1.2 9.89e+07 1.0 0.0e+00 0.0e+00 1.6e+03  0 14  0  0  4   0 14  0  0 48  9119
VecNorm             3362 1.0 8.0850e-03 1.1 6.72e+06 1.0 0.0e+00 0.0e+00 1.7e+03  0  1  0  0  4   0  1  0  0 49  1653
VecScale            3362 1.0 9.0408e-04 1.1 3.36e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  7393
VecCopy              114 1.0 9.6560e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              3516 1.0 1.8098e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY              192 1.0 7.8201e-05 1.2 3.84e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  9762
VecMAXPY            3362 1.0 1.2197e-02 1.0 1.05e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 15  0  0  0   0 15  0  0  0 17194
VecScatterBegin     3326 1.0 1.7161e-03 1.0 0.00e+00 0.0 3.3e+03 1.5e+03 0.0e+00  0  0 95 74  0   0  0 96 74  0     0
VecScatterEnd       3326 1.0 2.0552e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize        3362 1.0 9.2244e-03 1.1 1.01e+07 1.0 0.0e+00 0.0e+00 1.7e+03  0  1  0  0  4   0  1  0  0 49  2174
KSPGMRESOrthog      3248 1.0 3.3436e-02 1.1 1.98e+08 1.0 0.0e+00 0.0e+00 1.6e+03  0 29  0  0  4   0 29  0  0 48 11770
KSPSetUp              40 1.0 8.9884e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              36 1.0 2.2881e-01 1.0 6.83e+08 1.0 3.3e+03 1.5e+03 3.3e+03  1100 95 74  9   1100 96 74 97  5938
PCSetUp               72 1.0 1.6482e-02 1.0 2.44e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  2961
PCSetUpOnBlocks       36 1.0 2.7308e-03 1.0 2.71e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1986
PCApply             3362 1.0 1.0667e-01 1.0 2.42e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 35  0  0  0   0 35  0  0  0  4506
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     8              0            0     0.
              Vector    80              2         3312     0.
      Vector Scatter     2              0            0     0.
       Krylov Solver     6              0            0     0.
           Index Set    14              8        15440     0.
      Preconditioner     4              0            0     0.
              Viewer     2              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 4.29153e-07
Average time for zero size MPI_Send(): 1.07288e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1 --with-mumps=1 --download-mumps=1 --with-ptscotch=1 --download-ptscotch=1
-----------------------------------------
Libraries compiled on Sat Jun 24 17:54:26 2017 on cml01 
Machine characteristics: Linux-4.8.0-56-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include/openmpi/opal/mca/event/libevent2021/libevent -I/usr/lib/openmpi/include/openmpi/opal/mca/event/libevent2021/libevent/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lparmetis -lmetis -lscalapack -lsuperlu -lspai -lflapack -lfblas -lhwloc -lptesmumps -lptscotch -lptscotcherr -lscotch -lscotcherr -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lgfortran -lm -Wl,-rpath,/usr/lib/openmpi/lib -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -lrt -lm -lz -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -Wl,-rpath,/usr/lib/openmpi/lib -lmpi -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 2 processors, by yuan Sat Jun 24 22:54:09 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           3.741e+01      1.00000   3.741e+01
Objects:              1.160e+02      1.00000   1.160e+02
Flops:                7.065e+08      1.01158   7.025e+08  1.405e+09
Flops/sec:            1.889e+07      1.01158   1.878e+07  3.756e+07
MPI Messages:         1.808e+03      1.00528   1.803e+03  3.606e+03
MPI Message Lengths:  3.576e+06      1.00001   1.984e+03  7.152e+06
MPI Reductions:       3.999e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 3.7378e+01  99.9%  1.4050e+09 100.0%  3.570e+03  99.0%  1.984e+03      100.0%  3.517e+03   8.8% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult             3438 1.0 7.9464e-02 1.0 2.32e+08 1.0 3.4e+03 1.5e+03 0.0e+00  0 33 95 73  0   0 33 96 73  0  5793
MatSolve            3476 1.0 8.9162e-02 1.0 2.28e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 32  0  0  0   0 32  0  0  0  5071
MatLUFactorNum        38 1.0 1.6117e-02 1.0 2.58e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  3196
MatILUFactorSym        4 1.0 9.4485e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      38 1.0 5.1759e-01454.5 0.00e+00 0.0 1.1e+02 1.7e+04 3.8e+01  1  0  3 27  0   1  0  3 27  1     0
MatAssemblyEnd        38 1.0 4.3969e-03 1.0 0.00e+00 0.0 8.0e+00 3.8e+02 5.2e+01  0  0  0  0  0   0  0  0  0  1     0
MatGetRowIJ            4 1.0 5.0068e-06 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         4 1.0 5.2691e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        38 1.0 2.1522e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             3358 1.0 2.2190e-02 1.1 1.02e+08 1.0 0.0e+00 0.0e+00 1.7e+03  0 14  0  0  4   0 14  0  0 48  9149
VecNorm             3476 1.0 8.3239e-03 1.1 6.95e+06 1.0 0.0e+00 0.0e+00 1.7e+03  0  1  0  0  4   0  1  0  0 49  1660
VecScale            3476 1.0 9.3222e-04 1.1 3.47e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  7413
VecCopy              118 1.0 1.0061e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              3636 1.0 1.8928e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY              198 1.0 8.1062e-05 1.2 3.96e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  9712
VecMAXPY            3476 1.0 1.2572e-02 1.0 1.09e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 15  0  0  0   0 15  0  0  0 17215
VecScatterBegin     3438 1.0 1.7738e-03 1.0 0.00e+00 0.0 3.4e+03 1.5e+03 0.0e+00  0  0 95 73  0   0  0 96 73  0     0
VecScatterEnd       3438 1.0 2.1198e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize        3476 1.0 9.5060e-03 1.1 1.04e+07 1.0 0.0e+00 0.0e+00 1.7e+03  0  1  0  0  4   0  1  0  0 49  2181
KSPGMRESOrthog      3358 1.0 3.4426e-02 1.1 2.04e+08 1.0 0.0e+00 0.0e+00 1.7e+03  0 29  0  0  4   0 29  0  0 48 11796
KSPSetUp              42 1.0 8.9884e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              38 1.0 2.3668e-01 1.0 7.07e+08 1.0 3.4e+03 1.5e+03 3.4e+03  1100 95 73  9   1100 96 73 97  5936
PCSetUp               76 1.0 1.7301e-02 1.0 2.58e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  2977
PCSetUpOnBlocks       38 1.0 2.7308e-03 1.0 2.71e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1986
PCApply             3476 1.0 1.1065e-01 1.0 2.51e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 35  0  0  0   0 35  0  0  0  4503
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     8              0            0     0.
              Vector    80              2         3312     0.
      Vector Scatter     2              0            0     0.
       Krylov Solver     6              0            0     0.
           Index Set    14              8        15440     0.
      Preconditioner     4              0            0     0.
              Viewer     2              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 2.81334e-06
Average time for zero size MPI_Send(): 1.07288e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1 --with-mumps=1 --download-mumps=1 --with-ptscotch=1 --download-ptscotch=1
-----------------------------------------
Libraries compiled on Sat Jun 24 17:54:26 2017 on cml01 
Machine characteristics: Linux-4.8.0-56-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include/openmpi/opal/mca/event/libevent2021/libevent -I/usr/lib/openmpi/include/openmpi/opal/mca/event/libevent2021/libevent/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lparmetis -lmetis -lscalapack -lsuperlu -lspai -lflapack -lfblas -lhwloc -lptesmumps -lptscotch -lptscotcherr -lscotch -lscotcherr -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lgfortran -lm -Wl,-rpath,/usr/lib/openmpi/lib -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -lrt -lm -lz -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -Wl,-rpath,/usr/lib/openmpi/lib -lmpi -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 2 processors, by yuan Sat Jun 24 22:54:11 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           3.883e+01      1.00000   3.883e+01
Objects:              1.160e+02      1.00000   1.160e+02
Flops:                7.284e+08      1.01157   7.242e+08  1.448e+09
Flops/sec:            1.876e+07      1.01157   1.865e+07  3.730e+07
MPI Messages:         1.865e+03      1.00539   1.860e+03  3.720e+03
MPI Message Lengths:  3.708e+06      1.00001   1.994e+03  7.417e+06
MPI Reductions:       4.212e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 3.8805e+01  99.9%  1.4484e+09 100.0%  3.682e+03  99.0%  1.994e+03      100.0%  3.627e+03   8.6% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult             3544 1.0 8.1902e-02 1.0 2.39e+08 1.0 3.5e+03 1.5e+03 0.0e+00  0 33 95 73  0   0 33 96 73  0  5794
MatSolve            3584 1.0 9.1895e-02 1.0 2.35e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 32  0  0  0   0 32  0  0  0  5073
MatLUFactorNum        40 1.0 1.6973e-02 1.0 2.71e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  3195
MatILUFactorSym        4 1.0 9.4485e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      40 1.0 5.1763e-01435.1 0.00e+00 0.0 1.2e+02 1.7e+04 4.0e+01  1  0  3 27  0   1  0  3 27  1     0
MatAssemblyEnd        40 1.0 4.5869e-03 1.0 0.00e+00 0.0 8.0e+00 3.8e+02 5.4e+01  0  0  0  0  0   0  0  0  0  1     0
MatGetRowIJ            4 1.0 5.0068e-06 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         4 1.0 5.2691e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        40 1.0 2.1942e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             3462 1.0 2.2796e-02 1.1 1.05e+08 1.0 0.0e+00 0.0e+00 1.7e+03  0 14  0  0  4   0 14  0  0 48  9156
VecNorm             3584 1.0 8.6207e-03 1.1 7.16e+06 1.0 0.0e+00 0.0e+00 1.8e+03  0  1  0  0  4   0  1  0  0 49  1653
VecScale            3584 1.0 9.6202e-04 1.1 3.58e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  7406
VecCopy              122 1.0 1.1039e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              3750 1.0 1.9910e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY              204 1.0 8.3923e-05 1.1 4.08e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  9665
VecMAXPY            3584 1.0 1.2915e-02 1.0 1.12e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 15  0  0  0   0 15  0  0  0 17232
VecScatterBegin     3544 1.0 1.8256e-03 1.0 0.00e+00 0.0 3.5e+03 1.5e+03 0.0e+00  0  0 95 73  0   0  0 96 73  0     0
VecScatterEnd       3544 1.0 2.1844e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize        3584 1.0 9.8436e-03 1.1 1.07e+07 1.0 0.0e+00 0.0e+00 1.8e+03  0  1  0  0  4   0  1  0  0 49  2171
KSPGMRESOrthog      3462 1.0 3.5372e-02 1.1 2.10e+08 1.0 0.0e+00 0.0e+00 1.7e+03  0 29  0  0  4   0 29  0  0 48 11803
KSPSetUp              44 1.0 8.9884e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              40 1.0 2.4423e-01 1.0 7.28e+08 1.0 3.5e+03 1.5e+03 3.5e+03  1100 95 73  8   1100 96 73 97  5931
PCSetUp               80 1.0 1.8162e-02 1.0 2.71e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  2986
PCSetUpOnBlocks       40 1.0 2.7318e-03 1.0 2.71e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1985
PCApply             3584 1.0 1.1450e-01 1.0 2.59e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 36  0  0  0   0 36  0  0  0  4497
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     8              0            0     0.
              Vector    80              2         3312     0.
      Vector Scatter     2              0            0     0.
       Krylov Solver     6              0            0     0.
           Index Set    14              8        15440     0.
      Preconditioner     4              0            0     0.
              Viewer     2              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 4.29153e-07
Average time for zero size MPI_Send(): 5.96046e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1 --with-mumps=1 --download-mumps=1 --with-ptscotch=1 --download-ptscotch=1
-----------------------------------------
Libraries compiled on Sat Jun 24 17:54:26 2017 on cml01 
Machine characteristics: Linux-4.8.0-56-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include/openmpi/opal/mca/event/libevent2021/libevent -I/usr/lib/openmpi/include/openmpi/opal/mca/event/libevent2021/libevent/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lparmetis -lmetis -lscalapack -lsuperlu -lspai -lflapack -lfblas -lhwloc -lptesmumps -lptscotch -lptscotcherr -lscotch -lscotcherr -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lgfortran -lm -Wl,-rpath,/usr/lib/openmpi/lib -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -lrt -lm -lz -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -Wl,-rpath,/usr/lib/openmpi/lib -lmpi -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 2 processors, by yuan Sat Jun 24 22:54:12 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           4.032e+01      1.00000   4.032e+01
Objects:              1.160e+02      1.00000   1.160e+02
Flops:                7.521e+08      1.01156   7.478e+08  1.496e+09
Flops/sec:            1.865e+07      1.01156   1.855e+07  3.709e+07
MPI Messages:         1.926e+03      1.00548   1.921e+03  3.842e+03
MPI Message Lengths:  3.847e+06      1.00001   2.002e+03  7.693e+06
MPI Reductions:       4.427e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 4.0289e+01  99.9%  1.4956e+09 100.0%  3.802e+03  99.0%  2.002e+03      100.0%  3.745e+03   8.5% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult             3658 1.0 8.4534e-02 1.0 2.46e+08 1.0 3.7e+03 1.5e+03 0.0e+00  0 33 95 73  0   0 33 96 73  0  5795
MatSolve            3700 1.0 9.4861e-02 1.0 2.42e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 32  0  0  0   0 32  0  0  0  5073
MatLUFactorNum        42 1.0 1.7833e-02 1.0 2.85e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  3193
MatILUFactorSym        4 1.0 9.4485e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      42 1.0 5.1769e-01412.6 0.00e+00 0.0 1.3e+02 1.7e+04 4.2e+01  1  0  3 27  0   1  0  3 27  1     0
MatAssemblyEnd        42 1.0 4.8120e-03 1.0 0.00e+00 0.0 8.0e+00 3.8e+02 5.6e+01  0  0  0  0  0   0  0  0  0  1     0
MatGetRowIJ            4 1.0 5.0068e-06 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         4 1.0 5.2691e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        42 1.0 2.2740e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             3574 1.0 2.3504e-02 1.1 1.08e+08 1.0 0.0e+00 0.0e+00 1.8e+03  0 14  0  0  4   0 14  0  0 48  9156
VecNorm             3700 1.0 8.9157e-03 1.1 7.39e+06 1.0 0.0e+00 0.0e+00 1.8e+03  0  1  0  0  4   0  1  0  0 49  1650
VecScale            3700 1.0 9.9206e-04 1.1 3.70e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  7414
VecCopy              126 1.0 1.1420e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              3872 1.0 2.0754e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY              210 1.0 8.6069e-05 1.1 4.20e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  9701
VecMAXPY            3700 1.0 1.3304e-02 1.0 1.15e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 15  0  0  0   0 15  0  0  0 17249
VecScatterBegin     3658 1.0 1.8852e-03 1.0 0.00e+00 0.0 3.7e+03 1.5e+03 0.0e+00  0  0 95 73  0   0  0 96 73  0     0
VecScatterEnd       3658 1.0 2.2552e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize        3700 1.0 1.0184e-02 1.1 1.11e+07 1.0 0.0e+00 0.0e+00 1.8e+03  0  1  0  0  4   0  1  0  0 49  2167
KSPGMRESOrthog      3574 1.0 3.6468e-02 1.1 2.16e+08 1.0 0.0e+00 0.0e+00 1.8e+03  0 29  0  0  4   0 29  0  0 48 11804
KSPSetUp              46 1.0 9.0837e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              42 1.0 2.5238e-01 1.0 7.52e+08 1.0 3.7e+03 1.5e+03 3.6e+03  1100 95 73  8   1100 96 73 97  5926
PCSetUp               84 1.0 1.9028e-02 1.0 2.85e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  2992
PCSetUpOnBlocks       42 1.0 2.7318e-03 1.0 2.71e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1985
PCApply             3700 1.0 1.1859e-01 1.0 2.68e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 36  0  0  0   0 36  0  0  0  4492
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     8              0            0     0.
              Vector    80              2         3312     0.
      Vector Scatter     2              0            0     0.
       Krylov Solver     6              0            0     0.
           Index Set    14              8        15440     0.
      Preconditioner     4              0            0     0.
              Viewer     2              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 3.8147e-07
Average time for zero size MPI_Send(): 4.76837e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1 --with-mumps=1 --download-mumps=1 --with-ptscotch=1 --download-ptscotch=1
-----------------------------------------
Libraries compiled on Sat Jun 24 17:54:26 2017 on cml01 
Machine characteristics: Linux-4.8.0-56-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include/openmpi/opal/mca/event/libevent2021/libevent -I/usr/lib/openmpi/include/openmpi/opal/mca/event/libevent2021/libevent/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lparmetis -lmetis -lscalapack -lsuperlu -lspai -lflapack -lfblas -lhwloc -lptesmumps -lptscotch -lptscotcherr -lscotch -lscotcherr -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lgfortran -lm -Wl,-rpath,/usr/lib/openmpi/lib -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -lrt -lm -lz -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -Wl,-rpath,/usr/lib/openmpi/lib -lmpi -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 2 processors, by yuan Sat Jun 24 22:54:13 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           4.174e+01      1.00000   4.174e+01
Objects:              1.160e+02      1.00000   1.160e+02
Flops:                7.754e+08      1.01156   7.709e+08  1.542e+09
Flops/sec:            1.857e+07      1.01156   1.847e+07  3.694e+07
MPI Messages:         1.987e+03      1.00557   1.982e+03  3.963e+03
MPI Message Lengths:  3.983e+06      1.00001   2.010e+03  7.967e+06
MPI Reductions:       4.641e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 4.1710e+01  99.9%  1.5419e+09 100.0%  3.921e+03  98.9%  2.010e+03      100.0%  3.861e+03   8.3% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult             3770 1.0 8.7046e-02 1.0 2.54e+08 1.0 3.8e+03 1.5e+03 0.0e+00  0 33 95 72  0   0 33 96 72  0  5800
MatSolve            3814 1.0 9.7719e-02 1.0 2.50e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 32  0  0  0   0 32  0  0  0  5077
MatLUFactorNum        44 1.0 1.8696e-02 1.0 2.99e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  3190
MatILUFactorSym        4 1.0 9.4485e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      44 1.0 5.1773e-01397.1 0.00e+00 0.0 1.3e+02 1.7e+04 4.4e+01  1  0  3 28  0   1  0  3 28  1     0
MatAssemblyEnd        44 1.0 5.0030e-03 1.0 0.00e+00 0.0 8.0e+00 3.8e+02 5.8e+01  0  0  0  0  0   0  0  0  0  2     0
MatGetRowIJ            4 1.0 5.0068e-06 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         4 1.0 5.2691e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        44 1.0 2.3050e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             3684 1.0 2.4055e-02 1.1 1.11e+08 1.0 0.0e+00 0.0e+00 1.8e+03  0 14  0  0  4   0 14  0  0 48  9208
VecNorm             3814 1.0 9.2008e-03 1.1 7.62e+06 1.0 0.0e+00 0.0e+00 1.9e+03  0  1  0  0  4   0  1  0  0 49  1648
VecScale            3814 1.0 1.0271e-03 1.1 3.81e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  7382
VecCopy              130 1.0 1.1802e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              3992 1.0 2.1455e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY              216 1.0 8.8930e-05 1.2 4.32e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  9657
VecMAXPY            3814 1.0 1.3683e-02 1.0 1.19e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 15  0  0  0   0 15  0  0  0 17262
VecScatterBegin     3770 1.0 1.9486e-03 1.0 0.00e+00 0.0 3.8e+03 1.5e+03 0.0e+00  0  0 95 72  0   0  0 96 72  0     0
VecScatterEnd       3770 1.0 2.3172e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize        3814 1.0 1.0511e-02 1.1 1.14e+07 1.0 0.0e+00 0.0e+00 1.9e+03  0  1  0  0  4   0  1  0  0 49  2164
KSPGMRESOrthog      3684 1.0 3.7395e-02 1.1 2.23e+08 1.0 0.0e+00 0.0e+00 1.8e+03  0 29  0  0  4   0 29  0  0 48 11848
KSPSetUp              48 1.0 9.0837e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              44 1.0 2.6020e-01 1.0 7.75e+08 1.0 3.8e+03 1.5e+03 3.7e+03  1100 95 72  8   1100 96 72 97  5926
PCSetUp               88 1.0 1.9895e-02 1.0 2.99e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  2998
PCSetUpOnBlocks       44 1.0 2.7318e-03 1.0 2.71e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1985
PCApply             3814 1.0 1.2256e-01 1.0 2.77e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 36  0  0  0   0 36  0  0  0  4490
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     8              0            0     0.
              Vector    80              2         3312     0.
      Vector Scatter     2              0            0     0.
       Krylov Solver     6              0            0     0.
           Index Set    14              8        15440     0.
      Preconditioner     4              0            0     0.
              Viewer     2              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 6.19888e-07
Average time for zero size MPI_Send(): 1.07288e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1 --with-mumps=1 --download-mumps=1 --with-ptscotch=1 --download-ptscotch=1
-----------------------------------------
Libraries compiled on Sat Jun 24 17:54:26 2017 on cml01 
Machine characteristics: Linux-4.8.0-56-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include/openmpi/opal/mca/event/libevent2021/libevent -I/usr/lib/openmpi/include/openmpi/opal/mca/event/libevent2021/libevent/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lparmetis -lmetis -lscalapack -lsuperlu -lspai -lflapack -lfblas -lhwloc -lptesmumps -lptscotch -lptscotcherr -lscotch -lscotcherr -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lgfortran -lm -Wl,-rpath,/usr/lib/openmpi/lib -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -lrt -lm -lz -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -Wl,-rpath,/usr/lib/openmpi/lib -lmpi -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 2 processors, by yuan Sat Jun 24 22:54:15 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           4.317e+01      1.00000   4.317e+01
Objects:              1.160e+02      1.00000   1.160e+02
Flops:                7.991e+08      1.01155   7.945e+08  1.589e+09
Flops/sec:            1.851e+07      1.01155   1.840e+07  3.681e+07
MPI Messages:         2.048e+03      1.00565   2.043e+03  4.086e+03
MPI Message Lengths:  4.122e+06      1.00001   2.018e+03  8.243e+06
MPI Reductions:       4.855e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 4.3136e+01  99.9%  1.5891e+09 100.0%  4.042e+03  98.9%  2.018e+03      100.0%  3.979e+03   8.2% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult             3884 1.0 8.9631e-02 1.0 2.62e+08 1.0 3.9e+03 1.5e+03 0.0e+00  0 33 95 72  0   0 33 96 72  0  5803
MatSolve            3930 1.0 1.0066e-01 1.0 2.57e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 32  0  0  0   0 32  0  0  0  5078
MatLUFactorNum        46 1.0 1.9565e-02 1.0 3.12e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  3187
MatILUFactorSym        4 1.0 9.4485e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      46 1.0 5.1778e-01382.8 0.00e+00 0.0 1.4e+02 1.7e+04 4.6e+01  1  0  3 28  0   1  0  3 28  1     0
MatAssemblyEnd        46 1.0 5.1918e-03 1.0 0.00e+00 0.0 8.0e+00 3.8e+02 6.0e+01  0  0  0  0  0   0  0  0  0  2     0
MatGetRowIJ            4 1.0 5.0068e-06 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         4 1.0 5.2691e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        46 1.0 2.3510e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             3796 1.0 2.4697e-02 1.1 1.15e+08 1.0 0.0e+00 0.0e+00 1.9e+03  0 14  0  0  4   0 14  0  0 48  9231
VecNorm             3930 1.0 9.5289e-03 1.1 7.85e+06 1.0 0.0e+00 0.0e+00 2.0e+03  0  1  0  0  4   0  1  0  0 49  1640
VecScale            3930 1.0 1.0641e-03 1.1 3.93e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  7342
VecCopy              134 1.0 1.2207e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              4114 1.0 2.2237e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY              222 1.0 9.1791e-05 1.2 4.44e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  9616
VecMAXPY            3930 1.0 1.4109e-02 1.0 1.22e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 15  0  0  0   0 15  0  0  0 17232
VecScatterBegin     3884 1.0 2.0168e-03 1.0 0.00e+00 0.0 3.9e+03 1.5e+03 0.0e+00  0  0 95 72  0   0  0 96 72  0     0
VecScatterEnd       3884 1.0 2.3870e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize        3930 1.0 1.0885e-02 1.1 1.18e+07 1.0 0.0e+00 0.0e+00 2.0e+03  0  1  0  0  4   0  1  0  0 49  2153
KSPGMRESOrthog      3796 1.0 3.8431e-02 1.1 2.29e+08 1.0 0.0e+00 0.0e+00 1.9e+03  0 29  0  0  4   0 29  0  0 48 11866
KSPSetUp              50 1.0 9.0837e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              46 1.0 2.6830e-01 1.0 7.99e+08 1.0 3.9e+03 1.5e+03 3.9e+03  1100 95 72  8   1100 96 72 97  5923
PCSetUp               92 1.0 2.0769e-02 1.0 3.12e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  3002
PCSetUpOnBlocks       46 1.0 2.7330e-03 1.0 2.71e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1984
PCApply             3930 1.0 1.2662e-01 1.0 2.86e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 36  0  0  0   0 36  0  0  0  4487
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     8              0            0     0.
              Vector    80              2         3312     0.
      Vector Scatter     2              0            0     0.
       Krylov Solver     6              0            0     0.
           Index Set    14              8        15440     0.
      Preconditioner     4              0            0     0.
              Viewer     2              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 3.8147e-07
Average time for zero size MPI_Send(): 9.53674e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1 --with-mumps=1 --download-mumps=1 --with-ptscotch=1 --download-ptscotch=1
-----------------------------------------
Libraries compiled on Sat Jun 24 17:54:26 2017 on cml01 
Machine characteristics: Linux-4.8.0-56-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include/openmpi/opal/mca/event/libevent2021/libevent -I/usr/lib/openmpi/include/openmpi/opal/mca/event/libevent2021/libevent/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lparmetis -lmetis -lscalapack -lsuperlu -lspai -lflapack -lfblas -lhwloc -lptesmumps -lptscotch -lptscotcherr -lscotch -lscotcherr -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lgfortran -lm -Wl,-rpath,/usr/lib/openmpi/lib -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -lrt -lm -lz -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -Wl,-rpath,/usr/lib/openmpi/lib -lmpi -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 2 processors, by yuan Sat Jun 24 22:54:16 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           4.461e+01      1.00000   4.461e+01
Objects:              1.160e+02      1.00000   1.160e+02
Flops:                8.233e+08      1.01155   8.186e+08  1.637e+09
Flops/sec:            1.846e+07      1.01155   1.835e+07  3.671e+07
MPI Messages:         2.111e+03      1.00572   2.105e+03  4.210e+03
MPI Message Lengths:  4.262e+06      1.00001   2.024e+03  8.523e+06
MPI Reductions:       5.070e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 4.4568e+01  99.9%  1.6372e+09 100.0%  4.164e+03  98.9%  2.024e+03      100.0%  4.099e+03   8.1% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult             4000 1.0 9.2295e-02 1.0 2.69e+08 1.0 4.0e+03 1.5e+03 0.0e+00  0 33 95 72  0   0 33 96 72  0  5803
MatSolve            4048 1.0 1.0367e-01 1.0 2.65e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 32  0  0  0   0 32  0  0  0  5079
MatLUFactorNum        48 1.0 2.0379e-02 1.0 3.26e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  3193
MatILUFactorSym        4 1.0 9.4485e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      48 1.0 5.1783e-01367.3 0.00e+00 0.0 1.4e+02 1.7e+04 4.8e+01  1  0  3 28  0   1  0  3 28  1     0
MatAssemblyEnd        48 1.0 5.4047e-03 1.0 0.00e+00 0.0 8.0e+00 3.8e+02 6.2e+01  0  0  0  0  0   0  0  0  0  2     0
MatGetRowIJ            4 1.0 5.0068e-06 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         4 1.0 5.2691e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        48 1.0 2.4061e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             3910 1.0 2.5391e-02 1.1 1.18e+08 1.0 0.0e+00 0.0e+00 2.0e+03  0 14  0  0  4   0 14  0  0 48  9243
VecNorm             4048 1.0 9.7795e-03 1.1 8.09e+06 1.0 0.0e+00 0.0e+00 2.0e+03  0  1  0  0  4   0  1  0  0 49  1646
VecScale            4048 1.0 1.0948e-03 1.1 4.04e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  7350
VecCopy              138 1.0 1.2612e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              4238 1.0 2.3139e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY              228 1.0 9.3937e-05 1.2 4.56e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  9650
VecMAXPY            4048 1.0 1.4513e-02 1.0 1.26e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 15  0  0  0   0 15  0  0  0 17246
VecScatterBegin     4000 1.0 2.0828e-03 1.0 0.00e+00 0.0 4.0e+03 1.5e+03 0.0e+00  0  0 95 72  0   0  0 96 72  0     0
VecScatterEnd       4000 1.0 2.4605e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize        4048 1.0 1.1175e-02 1.1 1.21e+07 1.0 0.0e+00 0.0e+00 2.0e+03  0  1  0  0  4   0  1  0  0 49  2160
KSPGMRESOrthog      3910 1.0 3.9530e-02 1.1 2.36e+08 1.0 0.0e+00 0.0e+00 2.0e+03  0 29  0  0  4   0 29  0  0 48 11875
KSPSetUp              52 1.0 9.0837e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              48 1.0 2.7649e-01 1.0 8.23e+08 1.0 4.0e+03 1.5e+03 4.0e+03  1100 95 72  8   1100 96 72 97  5921
PCSetUp               96 1.0 2.1588e-02 1.0 3.26e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  3014
PCSetUpOnBlocks       48 1.0 2.7330e-03 1.0 2.71e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1984
PCApply             4048 1.0 1.3071e-01 1.0 2.95e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 36  0  0  0   0 36  0  0  0  4484
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     8              0            0     0.
              Vector    80              2         3312     0.
      Vector Scatter     2              0            0     0.
       Krylov Solver     6              0            0     0.
           Index Set    14              8        15440     0.
      Preconditioner     4              0            0     0.
              Viewer     2              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 4.29153e-07
Average time for zero size MPI_Send(): 1.07288e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1 --with-mumps=1 --download-mumps=1 --with-ptscotch=1 --download-ptscotch=1
-----------------------------------------
Libraries compiled on Sat Jun 24 17:54:26 2017 on cml01 
Machine characteristics: Linux-4.8.0-56-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include/openmpi/opal/mca/event/libevent2021/libevent -I/usr/lib/openmpi/include/openmpi/opal/mca/event/libevent2021/libevent/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lparmetis -lmetis -lscalapack -lsuperlu -lspai -lflapack -lfblas -lhwloc -lptesmumps -lptscotch -lptscotcherr -lscotch -lscotcherr -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lgfortran -lm -Wl,-rpath,/usr/lib/openmpi/lib -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -lrt -lm -lz -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -Wl,-rpath,/usr/lib/openmpi/lib -lmpi -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 2 processors, by yuan Sat Jun 24 22:54:18 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           4.606e+01      1.00000   4.606e+01
Objects:              1.160e+02      1.00000   1.160e+02
Flops:                8.471e+08      1.01155   8.422e+08  1.684e+09
Flops/sec:            1.839e+07      1.01155   1.828e+07  3.657e+07
MPI Messages:         2.172e+03      1.00579   2.166e+03  4.332e+03
MPI Message Lengths:  4.400e+06      1.00001   2.031e+03  8.800e+06
MPI Reductions:       5.284e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 4.6027e+01  99.9%  1.6844e+09 100.0%  4.284e+03  98.9%  2.031e+03      100.0%  4.217e+03   8.0% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult             4114 1.0 9.4993e-02 1.0 2.77e+08 1.0 4.1e+03 1.5e+03 0.0e+00  0 33 95 71  0   0 33 96 71  0  5799
MatSolve            4164 1.0 1.0678e-01 1.0 2.73e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 32  0  0  0   0 32  0  0  0  5072
MatLUFactorNum        50 1.0 2.1247e-02 1.0 3.39e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  3190
MatILUFactorSym        4 1.0 9.4485e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      50 1.0 5.1788e-01353.3 0.00e+00 0.0 1.5e+02 1.7e+04 5.0e+01  1  0  3 29  0   1  0  4 29  1     0
MatAssemblyEnd        50 1.0 5.6057e-03 1.0 0.00e+00 0.0 8.0e+00 3.8e+02 6.4e+01  0  0  0  0  0   0  0  0  0  2     0
MatGetRowIJ            4 1.0 5.0068e-06 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         4 1.0 5.2691e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        50 1.0 2.4853e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             4022 1.0 2.6279e-02 1.1 1.21e+08 1.0 0.0e+00 0.0e+00 2.0e+03  0 14  0  0  4   0 14  0  0 48  9177
VecNorm             4164 1.0 1.0091e-02 1.1 8.32e+06 1.0 0.0e+00 0.0e+00 2.1e+03  0  1  0  0  4   0  1  0  0 49  1641
VecScale            4164 1.0 1.1284e-03 1.1 4.16e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  7336
VecCopy              142 1.0 1.3018e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              4360 1.0 2.4002e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY              234 1.0 9.5844e-05 1.2 4.68e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  9707
VecMAXPY            4164 1.0 1.4929e-02 1.0 1.29e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 15  0  0  0   0 15  0  0  0 17230
VecScatterBegin     4114 1.0 2.1403e-03 1.0 0.00e+00 0.0 4.1e+03 1.5e+03 0.0e+00  0  0 95 71  0   0  0 96 71  0     0
VecScatterEnd       4114 1.0 2.5346e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize        4164 1.0 1.1526e-02 1.1 1.25e+07 1.0 0.0e+00 0.0e+00 2.1e+03  0  1  0  0  4   0  1  0  0 49  2155
KSPGMRESOrthog      4022 1.0 4.0812e-02 1.1 2.42e+08 1.0 0.0e+00 0.0e+00 2.0e+03  0 29  0  0  4   0 29  0  0 48 11820
KSPSetUp              54 1.0 9.0837e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              50 1.0 2.8483e-01 1.0 8.47e+08 1.0 4.1e+03 1.5e+03 4.1e+03  1100 95 71  8   1100 96 71 97  5914
PCSetUp              100 1.0 2.2460e-02 1.0 3.39e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  3018
PCSetUpOnBlocks       50 1.0 2.7339e-03 1.0 2.71e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1983
PCApply             4164 1.0 1.3497e-01 1.0 3.04e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 36  0  0  0   0 36  0  0  0  4475
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     8              0            0     0.
              Vector    80              2         3312     0.
      Vector Scatter     2              0            0     0.
       Krylov Solver     6              0            0     0.
           Index Set    14              8        15440     0.
      Preconditioner     4              0            0     0.
              Viewer     2              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 4.29153e-07
Average time for zero size MPI_Send(): 1.07288e-06
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1 --with-mumps=1 --download-mumps=1 --with-ptscotch=1 --download-ptscotch=1
-----------------------------------------
Libraries compiled on Sat Jun 24 17:54:26 2017 on cml01 
Machine characteristics: Linux-4.8.0-56-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include/openmpi/opal/mca/event/libevent2021/libevent -I/usr/lib/openmpi/include/openmpi/opal/mca/event/libevent2021/libevent/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lparmetis -lmetis -lscalapack -lsuperlu -lspai -lflapack -lfblas -lhwloc -lptesmumps -lptscotch -lptscotcherr -lscotch -lscotcherr -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lgfortran -lm -Wl,-rpath,/usr/lib/openmpi/lib -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -lrt -lm -lz -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -Wl,-rpath,/usr/lib/openmpi/lib -lmpi -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 2 processors, by yuan Sat Jun 24 22:54:19 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           4.753e+01      1.00000   4.753e+01
Objects:              1.160e+02      1.00000   1.160e+02
Flops:                8.713e+08      1.01154   8.663e+08  1.733e+09
Flops/sec:            1.833e+07      1.01154   1.823e+07  3.645e+07
MPI Messages:         2.235e+03      1.00585   2.228e+03  4.457e+03
MPI Message Lengths:  4.540e+06      1.00001   2.037e+03  9.079e+06
MPI Reductions:       5.499e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 4.7491e+01  99.9%  1.7326e+09 100.0%  4.407e+03  98.9%  2.037e+03      100.0%  4.337e+03   7.9% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult             4230 1.0 9.7615e-02 1.0 2.85e+08 1.0 4.2e+03 1.5e+03 0.0e+00  0 33 95 71  0   0 33 96 71  0  5803
MatSolve            4282 1.0 1.0976e-01 1.0 2.80e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 32  0  0  0   0 32  0  0  0  5074
MatLUFactorNum        52 1.0 2.2066e-02 1.0 3.53e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  3194
MatILUFactorSym        4 1.0 9.4485e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      52 1.0 5.1793e-01341.2 0.00e+00 0.0 1.6e+02 1.7e+04 5.2e+01  1  0  4 29  0   1  0  4 29  1     0
MatAssemblyEnd        52 1.0 5.8038e-03 1.0 0.00e+00 0.0 8.0e+00 3.8e+02 6.6e+01  0  0  0  0  0   0  0  0  0  2     0
MatGetRowIJ            4 1.0 5.0068e-06 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         4 1.0 5.2691e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        52 1.0 2.5344e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             4136 1.0 2.6908e-02 1.1 1.25e+08 1.0 0.0e+00 0.0e+00 2.1e+03  0 14  0  0  4   0 14  0  0 48  9212
VecNorm             4282 1.0 1.0331e-02 1.1 8.56e+06 1.0 0.0e+00 0.0e+00 2.1e+03  0  1  0  0  4   0  1  0  0 49  1648
VecScale            4282 1.0 1.1604e-03 1.1 4.28e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  7336
VecCopy              146 1.0 1.3423e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              4484 1.0 2.4829e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY              240 1.0 9.7752e-05 1.1 4.80e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  9762
VecMAXPY            4282 1.0 1.5334e-02 1.0 1.33e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 15  0  0  0   0 15  0  0  0 17242
VecScatterBegin     4230 1.0 2.2056e-03 1.0 0.00e+00 0.0 4.2e+03 1.5e+03 0.0e+00  0  0 95 71  0   0  0 96 71  0     0
VecScatterEnd       4230 1.0 2.6040e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize        4282 1.0 1.1812e-02 1.1 1.28e+07 1.0 0.0e+00 0.0e+00 2.1e+03  0  1  0  0  4   0  1  0  0 49  2162
KSPGMRESOrthog      4136 1.0 4.1839e-02 1.1 2.49e+08 1.0 0.0e+00 0.0e+00 2.1e+03  0 29  0  0  4   0 29  0  0 48 11850
KSPSetUp              56 1.0 9.0837e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              52 1.0 2.9296e-01 1.0 8.71e+08 1.0 4.2e+03 1.5e+03 4.2e+03  1100 95 71  8   1100 96 71 97  5914
PCSetUp              104 1.0 2.3286e-02 1.0 3.53e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  3027
PCSetUpOnBlocks       52 1.0 2.7351e-03 1.0 2.71e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1982
PCApply             4282 1.0 1.3903e-01 1.0 3.13e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 36  0  0  0   0 36  0  0  0  4474
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     8              0            0     0.
              Vector    80              2         3312     0.
      Vector Scatter     2              0            0     0.
       Krylov Solver     6              0            0     0.
           Index Set    14              8        15440     0.
      Preconditioner     4              0            0     0.
              Viewer     2              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 3.8147e-07
Average time for zero size MPI_Send(): 9.53674e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1 --with-mumps=1 --download-mumps=1 --with-ptscotch=1 --download-ptscotch=1
-----------------------------------------
Libraries compiled on Sat Jun 24 17:54:26 2017 on cml01 
Machine characteristics: Linux-4.8.0-56-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include/openmpi/opal/mca/event/libevent2021/libevent -I/usr/lib/openmpi/include/openmpi/opal/mca/event/libevent2021/libevent/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lparmetis -lmetis -lscalapack -lsuperlu -lspai -lflapack -lfblas -lhwloc -lptesmumps -lptscotch -lptscotcherr -lscotch -lscotcherr -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lgfortran -lm -Wl,-rpath,/usr/lib/openmpi/lib -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -lrt -lm -lz -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -Wl,-rpath,/usr/lib/openmpi/lib -lmpi -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 2 processors, by yuan Sat Jun 24 22:54:21 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           4.896e+01      1.00000   4.896e+01
Objects:              1.160e+02      1.00000   1.160e+02
Flops:                8.955e+08      1.01154   8.904e+08  1.781e+09
Flops/sec:            1.829e+07      1.01154   1.819e+07  3.637e+07
MPI Messages:         2.298e+03      1.00591   2.291e+03  4.582e+03
MPI Message Lengths:  4.679e+06      1.00001   2.043e+03  9.359e+06
MPI Reductions:       5.713e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 4.8920e+01  99.9%  1.7808e+09 100.0%  4.530e+03  98.9%  2.043e+03      100.0%  4.457e+03   7.8% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult             4346 1.0 1.0028e-01 1.0 2.93e+08 1.0 4.3e+03 1.5e+03 0.0e+00  0 33 95 71  0   0 33 96 71  0  5803
MatSolve            4400 1.0 1.1283e-01 1.0 2.88e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 32  0  0  0   0 32  0  0  0  5072
MatLUFactorNum        54 1.0 2.3040e-02 1.0 3.67e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  3177
MatILUFactorSym        4 1.0 9.4485e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      54 1.0 5.1798e-01328.5 0.00e+00 0.0 1.6e+02 1.7e+04 5.4e+01  1  0  4 29  0   1  0  4 29  1     0
MatAssemblyEnd        54 1.0 6.0227e-03 1.0 0.00e+00 0.0 8.0e+00 3.8e+02 6.8e+01  0  0  0  0  0   0  0  0  0  2     0
MatGetRowIJ            4 1.0 5.0068e-06 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         4 1.0 5.2691e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        54 1.0 2.5723e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             4250 1.0 2.7693e-02 1.1 1.28e+08 1.0 0.0e+00 0.0e+00 2.1e+03  0 14  0  0  4   0 14  0  0 48  9193
VecNorm             4400 1.0 1.0738e-02 1.2 8.79e+06 1.0 0.0e+00 0.0e+00 2.2e+03  0  1  0  0  4   0  1  0  0 49  1629
VecScale            4400 1.0 1.1892e-03 1.1 4.40e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  7355
VecCopy              150 1.0 1.3828e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              4608 1.0 2.5651e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY              246 1.0 1.0085e-04 1.1 4.92e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  9698
VecMAXPY            4400 1.0 1.5742e-02 1.0 1.36e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 15  0  0  0   0 15  0  0  0 17250
VecScatterBegin     4346 1.0 2.2736e-03 1.0 0.00e+00 0.0 4.3e+03 1.5e+03 0.0e+00  0  0 95 71  0   0  0 96 71  0     0
VecScatterEnd       4346 1.0 2.6751e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize        4400 1.0 1.2258e-02 1.1 1.32e+07 1.0 0.0e+00 0.0e+00 2.2e+03  0  1  0  0  4   0  1  0  0 49  2141
KSPGMRESOrthog      4250 1.0 4.3031e-02 1.1 2.56e+08 1.0 0.0e+00 0.0e+00 2.1e+03  0 29  0  0  4   0 29  0  0 48 11834
KSPSetUp              58 1.0 9.0837e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              54 1.0 3.0140e-01 1.0 8.96e+08 1.0 4.3e+03 1.5e+03 4.3e+03  1100 95 71  8   1100 96 71 97  5908
PCSetUp              108 1.0 2.4266e-02 1.0 3.67e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  3017
PCSetUpOnBlocks       54 1.0 2.7361e-03 1.0 2.71e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1982
PCApply             4400 1.0 1.4336e-01 1.0 3.22e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 36  0  0  0   0 36  0  0  0  4465
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     8              0            0     0.
              Vector    80              2         3312     0.
      Vector Scatter     2              0            0     0.
       Krylov Solver     6              0            0     0.
           Index Set    14              8        15440     0.
      Preconditioner     4              0            0     0.
              Viewer     2              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 3.8147e-07
Average time for zero size MPI_Send(): 4.76837e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1 --with-mumps=1 --download-mumps=1 --with-ptscotch=1 --download-ptscotch=1
-----------------------------------------
Libraries compiled on Sat Jun 24 17:54:26 2017 on cml01 
Machine characteristics: Linux-4.8.0-56-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include/openmpi/opal/mca/event/libevent2021/libevent -I/usr/lib/openmpi/include/openmpi/opal/mca/event/libevent2021/libevent/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lparmetis -lmetis -lscalapack -lsuperlu -lspai -lflapack -lfblas -lhwloc -lptesmumps -lptscotch -lptscotcherr -lscotch -lscotcherr -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lgfortran -lm -Wl,-rpath,/usr/lib/openmpi/lib -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -lrt -lm -lz -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -Wl,-rpath,/usr/lib/openmpi/lib -lmpi -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 2 processors, by yuan Sat Jun 24 22:54:22 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           5.050e+01      1.00000   5.050e+01
Objects:              1.160e+02      1.00000   1.160e+02
Flops:                9.192e+08      1.01154   9.140e+08  1.828e+09
Flops/sec:            1.820e+07      1.01154   1.810e+07  3.620e+07
MPI Messages:         2.359e+03      1.00597   2.352e+03  4.704e+03
MPI Message Lengths:  4.818e+06      1.00001   2.048e+03  9.636e+06
MPI Reductions:       5.928e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 5.0453e+01  99.9%  1.8280e+09 100.0%  4.650e+03  98.9%  2.048e+03      100.0%  4.575e+03   7.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult             4460 1.0 1.0302e-01 1.0 3.00e+08 1.0 4.5e+03 1.5e+03 0.0e+00  0 33 95 71  0   0 33 96 71  0  5797
MatSolve            4516 1.0 1.1602e-01 1.0 2.96e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 32  0  0  0   0 32  0  0  0  5063
MatLUFactorNum        56 1.0 2.4016e-02 1.0 3.80e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  3161
MatILUFactorSym        4 1.0 9.4485e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      56 1.0 5.1804e-01314.4 0.00e+00 0.0 1.7e+02 1.7e+04 5.6e+01  1  0  4 29  0   1  0  4 29  1     0
MatAssemblyEnd        56 1.0 6.2597e-03 1.0 0.00e+00 0.0 8.0e+00 3.8e+02 7.0e+01  0  0  0  0  0   0  0  0  0  2     0
MatGetRowIJ            4 1.0 5.0068e-06 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         4 1.0 5.2691e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        56 1.0 2.6655e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             4362 1.0 2.8661e-02 1.2 1.31e+08 1.0 0.0e+00 0.0e+00 2.2e+03  0 14  0  0  4   0 14  0  0 48  9109
VecNorm             4516 1.0 1.1143e-02 1.2 9.02e+06 1.0 0.0e+00 0.0e+00 2.3e+03  0  1  0  0  4   0  1  0  0 49  1611
VecScale            4516 1.0 1.2202e-03 1.1 4.51e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  7357
VecCopy              154 1.0 1.4234e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              4730 1.0 2.6593e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY              252 1.0 1.0419e-04 1.2 5.03e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  9617
VecMAXPY            4516 1.0 1.6165e-02 1.0 1.40e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 15  0  0  0   0 15  0  0  0 17227
VecScatterBegin     4460 1.0 2.3274e-03 1.0 0.00e+00 0.0 4.5e+03 1.5e+03 0.0e+00  0  0 95 71  0   0  0 96 71  0     0
VecScatterEnd       4460 1.0 2.7518e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize        4516 1.0 1.2699e-02 1.1 1.35e+07 1.0 0.0e+00 0.0e+00 2.3e+03  0  1  0  0  4   0  1  0  0 49  2121
KSPGMRESOrthog      4362 1.0 4.4384e-02 1.1 2.62e+08 1.0 0.0e+00 0.0e+00 2.2e+03  0 29  0  0  4   0 29  0  0 48 11765
KSPSetUp              60 1.0 9.1791e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              56 1.0 3.0994e-01 1.0 9.19e+08 1.0 4.5e+03 1.5e+03 4.4e+03  1100 95 71  7   1100 96 71 97  5898
PCSetUp              112 1.0 2.5247e-02 1.0 3.80e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  3007
PCSetUpOnBlocks       56 1.0 2.7361e-03 1.0 2.71e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1982
PCApply             4516 1.0 1.4778e-01 1.0 3.31e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 36  0  0  0   0 36  0  0  0  4452
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     8              0            0     0.
              Vector    80              2         3312     0.
      Vector Scatter     2              0            0     0.
       Krylov Solver     6              0            0     0.
           Index Set    14              8        15440     0.
      Preconditioner     4              0            0     0.
              Viewer     2              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 3.8147e-07
Average time for zero size MPI_Send(): 9.53674e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1 --with-mumps=1 --download-mumps=1 --with-ptscotch=1 --download-ptscotch=1
-----------------------------------------
Libraries compiled on Sat Jun 24 17:54:26 2017 on cml01 
Machine characteristics: Linux-4.8.0-56-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include/openmpi/opal/mca/event/libevent2021/libevent -I/usr/lib/openmpi/include/openmpi/opal/mca/event/libevent2021/libevent/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lparmetis -lmetis -lscalapack -lsuperlu -lspai -lflapack -lfblas -lhwloc -lptesmumps -lptscotch -lptscotcherr -lscotch -lscotcherr -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lgfortran -lm -Wl,-rpath,/usr/lib/openmpi/lib -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -lrt -lm -lz -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -Wl,-rpath,/usr/lib/openmpi/lib -lmpi -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 2 processors, by yuan Sat Jun 24 22:54:24 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           5.197e+01      1.00000   5.197e+01
Objects:              1.160e+02      1.00000   1.160e+02
Flops:                9.420e+08      1.01153   9.366e+08  1.873e+09
Flops/sec:            1.813e+07      1.01153   1.802e+07  3.605e+07
MPI Messages:         2.418e+03      1.00603   2.411e+03  4.822e+03
MPI Message Lengths:  4.953e+06      1.00001   2.054e+03  9.906e+06
MPI Reductions:       6.142e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 5.1925e+01  99.9%  1.8733e+09 100.0%  4.766e+03  98.8%  2.054e+03      100.0%  4.689e+03   7.6% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult             4570 1.0 1.0550e-01 1.0 3.08e+08 1.0 4.6e+03 1.5e+03 0.0e+00  0 33 95 70  0   0 33 96 70  0  5800
MatSolve            4628 1.0 1.1884e-01 1.0 3.03e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 32  0  0  0   0 32  0  0  0  5065
MatLUFactorNum        58 1.0 2.4832e-02 1.0 3.94e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  3166
MatILUFactorSym        4 1.0 9.4485e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      58 1.0 5.1809e-01303.9 0.00e+00 0.0 1.7e+02 1.7e+04 5.8e+01  1  0  4 29  0   1  0  4 29  1     0
MatAssemblyEnd        58 1.0 6.4507e-03 1.0 0.00e+00 0.0 8.0e+00 3.8e+02 7.2e+01  0  0  0  0  0   0  0  0  0  2     0
MatGetRowIJ            4 1.0 5.0068e-06 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         4 1.0 5.2691e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        58 1.0 2.7316e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             4470 1.0 2.9256e-02 1.1 1.34e+08 1.0 0.0e+00 0.0e+00 2.2e+03  0 14  0  0  4   0 14  0  0 48  9131
VecNorm             4628 1.0 1.1408e-02 1.2 9.25e+06 1.0 0.0e+00 0.0e+00 2.3e+03  0  1  0  0  4   0  1  0  0 49  1613
VecScale            4628 1.0 1.2479e-03 1.1 4.62e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  7373
VecCopy              158 1.0 1.4615e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              4848 1.0 2.7471e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY              258 1.0 1.0705e-04 1.2 5.15e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  9583
VecMAXPY            4628 1.0 1.6542e-02 1.0 1.43e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 15  0  0  0   0 15  0  0  0 17228
VecScatterBegin     4570 1.0 2.3925e-03 1.0 0.00e+00 0.0 4.6e+03 1.5e+03 0.0e+00  0  0 95 70  0   0  0 96 70  0     0
VecScatterEnd       4570 1.0 2.8183e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize        4628 1.0 1.3000e-02 1.1 1.39e+07 1.0 0.0e+00 0.0e+00 2.3e+03  0  1  0  0  4   0  1  0  0 49  2123
KSPGMRESOrthog      4470 1.0 4.5346e-02 1.1 2.69e+08 1.0 0.0e+00 0.0e+00 2.2e+03  0 29  0  0  4   0 29  0  0 48 11784
KSPSetUp              62 1.0 9.1791e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              58 1.0 3.1773e-01 1.0 9.42e+08 1.0 4.6e+03 1.5e+03 4.5e+03  1100 95 70  7   1100 96 70 97  5896
PCSetUp              116 1.0 2.6069e-02 1.0 3.94e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  3016
PCSetUpOnBlocks       58 1.0 2.7373e-03 1.0 2.71e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1981
PCApply             4628 1.0 1.5167e-01 1.0 3.40e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 36  0  0  0   0 36  0  0  0  4451
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     8              0            0     0.
              Vector    80              2         3312     0.
      Vector Scatter     2              0            0     0.
       Krylov Solver     6              0            0     0.
           Index Set    14              8        15440     0.
      Preconditioner     4              0            0     0.
              Viewer     2              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 5.72205e-07
Average time for zero size MPI_Send(): 9.53674e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1 --with-mumps=1 --download-mumps=1 --with-ptscotch=1 --download-ptscotch=1
-----------------------------------------
Libraries compiled on Sat Jun 24 17:54:26 2017 on cml01 
Machine characteristics: Linux-4.8.0-56-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include/openmpi/opal/mca/event/libevent2021/libevent -I/usr/lib/openmpi/include/openmpi/opal/mca/event/libevent2021/libevent/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lparmetis -lmetis -lscalapack -lsuperlu -lspai -lflapack -lfblas -lhwloc -lptesmumps -lptscotch -lptscotcherr -lscotch -lscotcherr -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lgfortran -lm -Wl,-rpath,/usr/lib/openmpi/lib -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -lrt -lm -lz -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -Wl,-rpath,/usr/lib/openmpi/lib -lmpi -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

Unknown Name on a build named cml01 with 2 processors, by yuan Sat Jun 24 22:54:25 2017
Using Petsc Release Version 3.7.3, Jul, 24, 2016 

                         Max       Max/Min        Avg      Total 
Time (sec):           5.341e+01      1.00000   5.341e+01
Objects:              1.160e+02      1.00000   1.160e+02
Flops:                9.648e+08      1.01153   9.593e+08  1.919e+09
Flops/sec:            1.806e+07      1.01153   1.796e+07  3.592e+07
MPI Messages:         2.478e+03      1.00609   2.470e+03  4.941e+03
MPI Message Lengths:  5.088e+06      1.00001   2.060e+03  1.018e+07
MPI Reductions:       6.356e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 5.3364e+01  99.9%  1.9186e+09 100.0%  4.883e+03  98.8%  2.060e+03      100.0%  4.803e+03   7.6% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult             4680 1.0 1.0804e-01 1.0 3.15e+08 1.0 4.7e+03 1.5e+03 0.0e+00  0 33 95 70  0   0 33 96 70  0  5801
MatSolve            4740 1.0 1.2181e-01 1.0 3.10e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 32  0  0  0   0 32  0  0  0  5061
MatLUFactorNum        60 1.0 2.5647e-02 1.0 4.07e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  3171
MatILUFactorSym        4 1.0 9.4485e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      60 1.0 5.1814e-01295.3 0.00e+00 0.0 1.8e+02 1.7e+04 6.0e+01  0  0  4 30  0   0  0  4 30  1     0
MatAssemblyEnd        60 1.0 6.6395e-03 1.0 0.00e+00 0.0 8.0e+00 3.8e+02 7.4e+01  0  0  0  0  0   0  0  0  0  2     0
MatGetRowIJ            4 1.0 5.0068e-06 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         4 1.0 5.2691e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries        60 1.0 2.7764e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot             4578 1.0 3.0065e-02 1.2 1.37e+08 1.0 0.0e+00 0.0e+00 2.3e+03  0 14  0  0  4   0 14  0  0 48  9088
VecNorm             4740 1.0 1.1676e-02 1.2 9.47e+06 1.0 0.0e+00 0.0e+00 2.4e+03  0  1  0  0  4   0  1  0  0 49  1614
VecScale            4740 1.0 1.2736e-03 1.1 4.74e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  7399
VecCopy              162 1.0 1.5020e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              4966 1.0 2.8379e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY              264 1.0 1.0824e-04 1.1 5.27e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  9697
VecMAXPY            4740 1.0 1.6941e-02 1.0 1.46e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 15  0  0  0   0 15  0  0  0 17207
VecScatterBegin     4680 1.0 2.4552e-03 1.0 0.00e+00 0.0 4.7e+03 1.5e+03 0.0e+00  0  0 95 70  0   0  0 96 70  0     0
VecScatterEnd       4680 1.0 2.8856e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize        4740 1.0 1.3306e-02 1.1 1.42e+07 1.0 0.0e+00 0.0e+00 2.4e+03  0  1  0  0  4   0  1  0  0 49  2125
KSPGMRESOrthog      4578 1.0 4.6510e-02 1.1 2.75e+08 1.0 0.0e+00 0.0e+00 2.3e+03  0 28  0  0  4   0 28  0  0 48 11751
KSPSetUp              64 1.0 9.1791e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              60 1.0 3.2566e-01 1.0 9.65e+08 1.0 4.7e+03 1.5e+03 4.7e+03  1100 95 70  7   1100 96 70 97  5891
PCSetUp              120 1.0 2.6889e-02 1.0 4.07e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  3025
PCSetUpOnBlocks       60 1.0 2.7385e-03 1.0 2.71e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1980
PCApply             4740 1.0 1.5571e-01 1.0 3.48e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0 36  0  0  0   0 36  0  0  0  4447
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     8              0            0     0.
              Vector    80              2         3312     0.
      Vector Scatter     2              0            0     0.
       Krylov Solver     6              0            0     0.
           Index Set    14              8        15440     0.
      Preconditioner     4              0            0     0.
              Viewer     2              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
Average time for MPI_Barrier(): 3.8147e-07
Average time for zero size MPI_Send(): 4.76837e-07
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_ARCH=build --with-x=0 --with-debugging=0 COPTFLAGS="-O3 -march=native -mtune=native" CXXOPTFLAGS="-O3 -march=native -mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native" --with-shared-libraries=0 --with-cxx=mpic++ --with-cc=mpicc --with-fc=mpif77 --with-blacs=1 --download-blacs=yes --with-scalapack=1 --download-scalapack=yes --with-spai=1 --download-spai=yes --with-hypre=0 --download-hypre=no --with-plapack=1 --download-plapack=1 --with-superlu_dist=1 --download-superlu_dist=yes --with-superlu=1 --download-superlu=yes --with-spooles=1 --download-spooles=yes --with-metis=1 --download-metis=yes --with-parmetis=1 --download-parmetis=yes --download-fblaslapack=1 --with-mumps=1 --download-mumps=1 --with-ptscotch=1 --download-ptscotch=1
-----------------------------------------
Libraries compiled on Sat Jun 24 17:54:26 2017 on cml01 
Machine characteristics: Linux-4.8.0-56-generic-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /home/yuan/Desktop/RealESSI_Dependencies/petsc
Using PETSc arch: build
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fvisibility=hidden -O3 -march=native -mtune=native  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif77  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3 -march=native -mtune=native   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/include -I/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/include -I/usr/lib/openmpi/include/openmpi/opal/mca/event/libevent2021/libevent -I/usr/lib/openmpi/include/openmpi/opal/mca/event/libevent2021/libevent/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif77
Using libraries: -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lpetsc -Wl,-rpath,/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -L/home/yuan/Desktop/RealESSI_Dependencies/petsc/build/lib -lsuperlu_dist -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lparmetis -lmetis -lscalapack -lsuperlu -lspai -lflapack -lfblas -lhwloc -lptesmumps -lptscotch -lptscotcherr -lscotch -lscotcherr -lssl -lcrypto -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lgfortran -lm -Wl,-rpath,/usr/lib/openmpi/lib -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -lrt -lm -lz -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -ldl -Wl,-rpath,/usr/lib/openmpi/lib -lmpi -lgcc_s -lpthread -ldl 
-----------------------------------------

